<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张慕晖的博客</title>
  
  <subtitle>LUX ET VERITAS</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhanghuimeng.github.io/"/>
  <updated>2020-09-07T20:48:59.000Z</updated>
  <id>https://zhanghuimeng.github.io/</id>
  
  <author>
    <name>张慕晖</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>THUMT代码详解（4）：infer阶段模型和数据流</title>
    <link href="https://zhanghuimeng.github.io/post/thumt-code-summary-4/"/>
    <id>https://zhanghuimeng.github.io/post/thumt-code-summary-4/</id>
    <published>2020-09-07T20:48:59.000Z</published>
    <updated>2020-09-07T20:48:59.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="/post/thumt-code-summary-1">简介篇地址</a></p><a id="more"></a><p>本篇将分为两个部分：一部分讲evaluation部分的模型和数据流，另一部分讲inference部分的模型和数据流。两者本质上是差不多的。</p><h2>evaluation</h2><p>evaluation的入口处也在<a href="https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/bin/trainer.py" target="_blank" rel="noopener">trainer.py</a>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> step % params.eval_steps == <span class="number">0</span>:</span><br><span class="line">    utils.evaluate(model, sorted_key, eval_dataset,</span><br><span class="line">                    params.output, references, params)</span><br></pre></td></tr></table></figure><p>这里的<code>sorted_key</code>和<code>eval_dataset</code>和<code>references</code>就是<a href="/post/thumt-code-summary-2">THUMT代码详解（2）：数据处理</a>中提到的：</p><ul><li><code>sorted_key</code>：用于把排序过的数据复原</li><li><code>eval_dataset</code>：一个dataset，由<code>source</code>和<code>source_mask</code>组成</li><li><code>references</code>：分词后的target端的句子，用于计算bleu值</li></ul><p>然后就是<a href="https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/utils/evaluation.py" target="_blank" rel="noopener">evaluation.py</a>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, sorted_key, dataset, base_dir, references, params)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> references:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 各种文件的地址，目前没什么用</span></span><br><span class="line">    base_dir = base_dir.rstrip(<span class="string">"/"</span>)</span><br><span class="line">    save_path = os.path.join(base_dir, <span class="string">"eval"</span>)</span><br><span class="line">    record_name = os.path.join(save_path, <span class="string">"record"</span>)</span><br><span class="line">    log_name = os.path.join(save_path, <span class="string">"log"</span>)</span><br><span class="line">    max_to_keep = params.keep_top_checkpoint_max</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建目录和文件，还是没什么用</span></span><br><span class="line">    <span class="keyword">if</span> dist.get_rank() == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Create directory and copy files</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(save_path):</span><br><span class="line">            print(<span class="string">"Making dir: %s"</span> % save_path)</span><br><span class="line">            os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">            params_pattern = os.path.join(base_dir, <span class="string">"*.json"</span>)</span><br><span class="line">            params_files = glob.glob(params_pattern)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> params_files:</span><br><span class="line">                new_name = name.replace(base_dir, save_path)</span><br><span class="line">                shutil.copy(name, new_name)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Do validation here</span></span><br><span class="line">    global_step = get_global_step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> dist.get_rank() == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Validating model at step %d"</span> % global_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用内部函数_evaluate_model，进行实际的evaluate工作</span></span><br><span class="line">    score = _evaluate_model(model, sorted_key, dataset, references, params)</span><br><span class="line">    .....</span><br></pre></td></tr></table></figure><p>调用<code>_evaluate_model</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_evaluate_model</span><span class="params">(model, sorted_key, dataset, references, params)</span>:</span></span><br><span class="line">    <span class="comment"># Create model</span></span><br><span class="line">    <span class="comment"># 在验证阶段不进行back-propagation</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 调整模型的模式</span></span><br><span class="line">        model.eval()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到dataset的iterator</span></span><br><span class="line">        iterator = iter(dataset)</span><br><span class="line">        counter = <span class="number">0</span></span><br><span class="line">        pad_max = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Buffers for synchronization</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line">        size = torch.zeros([dist.get_world_size()]).long()</span><br><span class="line">        t_list = [torch.empty([params.decode_batch_size, pad_max]).long()</span><br><span class="line">                  <span class="keyword">for</span> _ <span class="keyword">in</span> range(dist.get_world_size())]</span><br><span class="line">        results = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                features = next(iterator)</span><br><span class="line">                <span class="comment"># 这部分中继续对features进行处理，这个函数之前也见过了，就是把string转换成id</span></span><br><span class="line">                <span class="comment"># 所以就不细讲了</span></span><br><span class="line">                features = lookup(features, <span class="string">"infer"</span>, params)</span><br><span class="line">                batch_size = features[<span class="string">"source"</span>].shape[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                features = &#123;</span><br><span class="line">                    <span class="string">"source"</span>: torch.ones([<span class="number">1</span>, <span class="number">1</span>]).long(),</span><br><span class="line">                    <span class="string">"source_mask"</span>: torch.ones([<span class="number">1</span>, <span class="number">1</span>]).float()</span><br><span class="line">                &#125;</span><br><span class="line">                batch_size = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            t = time.time()</span><br><span class="line">            counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Decode</span></span><br><span class="line">            <span class="comment"># 调用beam_search进行实际解码工作</span></span><br><span class="line">            seqs, _ = beam_search([model], features, params)</span><br><span class="line">            ......</span><br></pre></td></tr></table></figure><p><code>beam_search</code>位于<a href="https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/utils/inference.py" target="_blank" rel="noopener">inference.py</a>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">beam_search</span><span class="params">(models, features, params)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(models, (list, tuple)):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"'models' must be a list or tuple"</span>)</span><br><span class="line"></span><br><span class="line">    beam_size = params.beam_size</span><br><span class="line">    top_beams = params.top_beams</span><br><span class="line">    alpha = params.decode_alpha</span><br><span class="line">    decode_ratio = params.decode_ratio</span><br><span class="line">    decode_length = params.decode_length</span><br><span class="line"></span><br><span class="line">    pad_id = params.lookup[<span class="string">"target"</span>][params.pad.encode(<span class="string">"utf-8"</span>)]</span><br><span class="line">    bos_id = params.lookup[<span class="string">"target"</span>][params.bos.encode(<span class="string">"utf-8"</span>)]</span><br><span class="line">    eos_id = params.lookup[<span class="string">"target"</span>][params.eos.encode(<span class="string">"utf-8"</span>)]</span><br><span class="line"></span><br><span class="line">    min_val = <span class="number">-1e9</span></span><br><span class="line">    shape = features[<span class="string">"source"</span>].shape</span><br><span class="line">    device = features[<span class="string">"source"</span>].device</span><br><span class="line">    batch_size = shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute initial state if necessary</span></span><br><span class="line">    states = []</span><br><span class="line">    funcs = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对每个model，都计算出encode之后的state：</span></span><br><span class="line">    <span class="comment"># &#123;</span></span><br><span class="line">    <span class="comment">#     "encoder_output": [batch, length_s, hidden],</span></span><br><span class="line">    <span class="comment">#     "enc_attn_bias": [batch, 1, 1, length_s],</span></span><br><span class="line">    <span class="comment">#     "decoder": ...</span></span><br><span class="line">    <span class="comment"># &#125;</span></span><br><span class="line">    <span class="comment"># encode函数之前已经讲了很多了，这里和训练阶段没有区别，所以就不讲了==</span></span><br><span class="line">    <span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">        state = model.empty_state(batch_size, device)</span><br><span class="line">        states.append(model.encode(features, state))</span><br><span class="line">        funcs.append(model.decode)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># For source sequence length</span></span><br><span class="line">    <span class="comment"># 计算每个句子译码时的最长长度</span></span><br><span class="line">    max_length = features[<span class="string">"source_mask"</span>].sum(<span class="number">1</span>) * decode_ratio</span><br><span class="line">    max_length = max_length.long() + decode_length</span><br><span class="line">    max_step = max_length.max()</span><br><span class="line">    <span class="comment"># [batch, beam_size]</span></span><br><span class="line">    <span class="comment"># 把长度扩展beam_size倍</span></span><br><span class="line">    max_length = torch.unsqueeze(max_length, <span class="number">1</span>).repeat([<span class="number">1</span>, beam_size])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Expand the inputs</span></span><br><span class="line">    <span class="comment"># [batch, length] =&gt; [batch * beam_size, length]</span></span><br><span class="line">    <span class="comment"># [batch, 1, length]</span></span><br><span class="line">    features[<span class="string">"source"</span>] = torch.unsqueeze(features[<span class="string">"source"</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># [batch, beam_size, length]</span></span><br><span class="line">    features[<span class="string">"source"</span>] = features[<span class="string">"source"</span>].repeat([<span class="number">1</span>, beam_size, <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># [batch * beam_size, length]</span></span><br><span class="line">    features[<span class="string">"source"</span>] = torch.reshape(features[<span class="string">"source"</span>],</span><br><span class="line">                                       [batch_size * beam_size, seq_length])</span><br><span class="line">    features[<span class="string">"source_mask"</span>] = torch.unsqueeze(features[<span class="string">"source_mask"</span>], <span class="number">1</span>)</span><br><span class="line">    features[<span class="string">"source_mask"</span>] = features[<span class="string">"source_mask"</span>].repeat([<span class="number">1</span>, beam_size, <span class="number">1</span>])</span><br><span class="line">    features[<span class="string">"source_mask"</span>] = torch.reshape(features[<span class="string">"source_mask"</span>],</span><br><span class="line">                                       [batch_size * beam_size, seq_length])</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure><p>这里使用了大量的<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.repeat" target="_blank" rel="noopener">torch.repeat</a>函数，其主要目的代码里已经写得很清楚了，就是把每个句子扩展<code>beam_size</code>倍，用于在beam search中使用。唯一值得注意的是，这里是整体重复而不是interleave的。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">beam_search</span><span class="params">(models, features, params)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    <span class="comment"># 把decode函数和features传过去了</span></span><br><span class="line">    <span class="comment"># 等到用到的时候再说</span></span><br><span class="line">    decoding_fn = _get_inference_fn(funcs, features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把每个states里的每个tensor都重复beam遍（增加第二维）</span></span><br><span class="line">    <span class="comment"># 主要作用在encoder_output和enc_attn_bias上</span></span><br><span class="line">    <span class="comment"># 因为感觉和主体内容没什么关系所以不讲了</span></span><br><span class="line">    <span class="comment"># 感兴趣的话请自行阅读nest.py</span></span><br><span class="line">    <span class="comment"># states[0]["encoder_output"]: [batch, beam, length_s, hidden]</span></span><br><span class="line">    <span class="comment"># states[0]["enc_attn_bias"]: [batch, beam, 1, 1, length_s]</span></span><br><span class="line">    states = map_structure(</span><br><span class="line">        <span class="keyword">lambda</span> x: _tile_to_beam_size(x, beam_size),</span><br><span class="line">        states)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initial beam search state</span></span><br><span class="line">    <span class="comment"># 创建一个维度是[batch_size, beam_size, 1]的矩阵，填充上&lt;bos&gt;</span></span><br><span class="line">    <span class="comment"># 用途是 TODO</span></span><br><span class="line">    init_seqs = torch.full([batch_size, beam_size, <span class="number">1</span>], bos_id, device=device)</span><br><span class="line">    init_seqs = init_seqs.long()</span><br><span class="line">    <span class="comment"># 创建一个维度是[batch_size, beam_size]的矩阵，第一列是0，其他列是无穷小</span></span><br><span class="line">    <span class="comment"># 用途是 TODO</span></span><br><span class="line">    init_log_probs = init_seqs.new_tensor(</span><br><span class="line">        [[<span class="number">0.</span>] + [min_val] * (beam_size - <span class="number">1</span>)], dtype=torch.float32)</span><br><span class="line">    init_log_probs = init_log_probs.repeat([batch_size, <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 创建一个维度是[batch_size, beam_size]的矩阵，全部为0</span></span><br><span class="line">    <span class="comment"># 用途是 TODO</span></span><br><span class="line">    <span class="comment"># score和log_probs的区别似乎是，score有一个length penalty</span></span><br><span class="line">    init_scores = torch.zeros_like(init_log_probs)</span><br><span class="line">    <span class="comment"># 创建结束状态矩阵</span></span><br><span class="line">    <span class="comment"># 为每个句子维护beam_size个分最高的已完成的句子</span></span><br><span class="line">    <span class="comment"># 用途是 TODO （我猜，inputs是未完成的句子，finish是已完成的句子）</span></span><br><span class="line">    fin_seqs = torch.zeros([batch_size, beam_size, <span class="number">1</span>], dtype=torch.int64,</span><br><span class="line">                           device=device)</span><br><span class="line">    fin_scores = torch.full([batch_size, beam_size], min_val,</span><br><span class="line">                            dtype=torch.float32, device=device)</span><br><span class="line">    fin_flags = torch.zeros([batch_size, beam_size], dtype=torch.bool,</span><br><span class="line">                            device=device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建BeamSearchState，把刚才的状态矩阵都放进去</span></span><br><span class="line">    <span class="comment"># 以及state TODO</span></span><br><span class="line">    state = BeamSearchState(</span><br><span class="line">        inputs=(init_seqs, init_log_probs, init_scores),</span><br><span class="line">        state=states,</span><br><span class="line">        finish=(fin_flags, fin_seqs, fin_scores),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接下来进入beam search每个step的循环</span></span><br><span class="line">    <span class="comment"># 循环结束的条件是，尚未结束的beam的最高可能得分低于目前已有的最低得分</span></span><br><span class="line">    <span class="keyword">for</span> time <span class="keyword">in</span> range(max_step):</span><br><span class="line">        state = _beam_search_step(time, decoding_fn, state, batch_size,</span><br><span class="line">                                  beam_size, alpha, pad_id, eos_id, max_length)</span><br><span class="line">        max_penalty = ((<span class="number">5.0</span> + max_step) / <span class="number">6.0</span>) ** alpha</span><br><span class="line">        best_alive_score = torch.max(state.inputs[<span class="number">1</span>][:, <span class="number">0</span>] / max_penalty)</span><br><span class="line">        worst_finished_score = torch.min(state.finish[<span class="number">2</span>])</span><br><span class="line">        cond = torch.gt(worst_finished_score, best_alive_score)</span><br><span class="line">        is_finished = bool(cond)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> is_finished:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    ......</span><br></pre></td></tr></table></figure><p>然后我们来看<code>_beam_search_step</code>这个函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># time：当前时间步</span></span><br><span class="line"><span class="comment"># func：用于解码的函数，实际上就是Transformer.decode</span></span><br><span class="line"><span class="comment"># state：包括inputs、state和finish三个属性</span></span><br><span class="line"><span class="comment"># ......</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_beam_search_step</span><span class="params">(time, func, state, batch_size, beam_size, alpha,</span></span></span><br><span class="line"><span class="function"><span class="params">                      pad_id, eos_id, max_length, inf=<span class="number">-1e9</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Compute log probabilities</span></span><br><span class="line">    <span class="comment"># seqs: [batch, beam, time + 1]</span></span><br><span class="line">    <span class="comment"># log_probs: [batch, beam]</span></span><br><span class="line">    seqs, log_probs = state.inputs[:<span class="number">2</span>]</span><br><span class="line">    <span class="comment"># 合并前两维</span></span><br><span class="line">    <span class="comment"># flat_seqs: [batch * beam, time + 1]</span></span><br><span class="line">    flat_seqs = _merge_first_two_dims(seqs)</span><br><span class="line">    <span class="comment"># flat_state[0]["encoder_output"]: [batch * beam, length_s, hidden]</span></span><br><span class="line">    <span class="comment"># flat_state[0]["enc_attn_bias"]: [batch * beam, 1, 1, length_s]</span></span><br><span class="line">    <span class="comment"># flat_state[0]["decoder"]["layer_0"]["k"]: [batch * beam, time, hidden]</span></span><br><span class="line">    flat_state = map_structure(<span class="keyword">lambda</span> x: _merge_first_two_dims(x), state.state)</span><br><span class="line">    step_log_probs, next_state = func(flat_seqs, flat_state)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure><p>这时再把<code>_get_inference_fn</code>拿出来看看：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_inference_fn</span><span class="params">(model_fns, features)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inference_fn</span><span class="params">(inputs, state)</span>:</span></span><br><span class="line">        <span class="comment"># 这里的features是beam_search里已经按beam重复过的source和source_mask</span></span><br><span class="line">        <span class="comment"># target显然是beam search中已经decode出的sequence</span></span><br><span class="line">        <span class="comment"># target_mask全1（因为这个mask只在Transformer.forward中计算loss时有用）</span></span><br><span class="line">        <span class="comment"># target, target_mask: [batch * beam, time + 1]</span></span><br><span class="line">        local_features = &#123;</span><br><span class="line">            <span class="string">"source"</span>: features[<span class="string">"source"</span>],</span><br><span class="line">            <span class="string">"source_mask"</span>: features[<span class="string">"source_mask"</span>],</span><br><span class="line">            <span class="string">"target"</span>: inputs,</span><br><span class="line">            <span class="string">"target_mask"</span>: torch.ones(*inputs.shape).float().cuda()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outputs = []</span><br><span class="line">        next_state = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (model_fn, model_state) <span class="keyword">in</span> zip(model_fns, state):</span><br><span class="line">            <span class="comment"># 把state输入给Transformer.decode，得到logits和新的state</span></span><br><span class="line">            <span class="comment"># state["encoder_output"]: [batch * beam, length_s, hidden]</span></span><br><span class="line">            <span class="comment"># state["enc_attn_bias"]: [batch * beam, 1, 1, length_s]</span></span><br><span class="line">            <span class="comment"># state["decoder"]["layer_0"]["k"]: [batch * beam, time, hidden]</span></span><br><span class="line">            <span class="keyword">if</span> model_state:</span><br><span class="line">                <span class="comment"># logits: [batch * beam, tvoc_size]</span></span><br><span class="line">                <span class="comment"># new_state["encoder_output"]: [batch * beam, length_s, hidden]（不变）</span></span><br><span class="line">                <span class="comment"># new_state["enc_attn_bias"]: [batch * beam, 1, 1, length_s]（不变）</span></span><br><span class="line">                <span class="comment"># new_state["decoder"]["layer_0"]["k"]: [batch * beam, time + 1, hidden]</span></span><br><span class="line">                logits, new_state = model_fn(local_features, model_state)</span><br><span class="line">                ......</span><br></pre></td></tr></table></figure><p>这里调用的<code>Transformer.decode</code>和训练阶段的是有一些区别的。简单来说，就是attention的key只留下了当前的一个token，而query和value用的是整个句子。这是因为infer阶段只需要在一个位置上进行预测。因为这部分太多了（如果细讲，还是需要把decoder再过一遍），所以这部分放到附录中。</p><p>TODO （描述可能不够清晰）</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_inference_fn</span><span class="params">(model_fns, features)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inference_fn</span><span class="params">(inputs, state)</span>:</span></span><br><span class="line">                ......</span><br><span class="line">                <span class="comment"># 对最后一维做softmax，然后做log，得到log_prob</span></span><br><span class="line">                <span class="comment"># outputs: [batch * beam, tvoc_size]</span></span><br><span class="line">                outputs.append(torch.nn.functional.log_softmax(logits,</span><br><span class="line">                                                               dim=<span class="number">-1</span>))</span><br><span class="line">                next_state.append(new_state)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                logits = model_fn(local_features)</span><br><span class="line">                outputs.append(torch.nn.functional.log_softmax(logits,</span><br><span class="line">                                                               dim=<span class="number">-1</span>))</span><br><span class="line">                next_state.append(&#123;&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Ensemble</span></span><br><span class="line">        <span class="comment"># 对所有模型输出的log_prob取个平均值</span></span><br><span class="line">        <span class="comment"># log_prob: [batch * beam, tvoc_size]</span></span><br><span class="line">        log_prob = sum(outputs) / float(len(outputs))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> log_prob.float(), next_state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inference_fn</span><br></pre></td></tr></table></figure><p>在decode完之后，回到<code>_beam_search_step</code>，进行下一步的处理：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_beam_search_step</span><span class="params">(time, func, state, batch_size, beam_size, alpha,</span></span></span><br><span class="line"><span class="function"><span class="params">                      pad_id, eos_id, max_length, inf=<span class="number">-1e9</span>)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    <span class="comment"># step_log_probs: [batch, beam, tvoc_size]</span></span><br><span class="line">    step_log_probs = _split_first_two_dims(step_log_probs, batch_size,</span><br><span class="line">                                           beam_size)</span><br><span class="line">    <span class="comment"># 把state中的tensor的前两维展开（虽然下一次计算之前又会折叠回去）</span></span><br><span class="line">    <span class="comment"># next_state[0]["encoder_output"]: [batch, beam, length_s, hidden]</span></span><br><span class="line">    <span class="comment"># next_state[0]["enc_attn_bias"]: [batch, beam, 1, 1, length_s]</span></span><br><span class="line">    <span class="comment"># next_state[0]["decoder"]["layer_0"]["k"]: [batch, beam, time + 1, hidden]</span></span><br><span class="line">    next_state = map_structure(</span><br><span class="line">        <span class="keyword">lambda</span> x: _split_first_two_dims(x, batch_size, beam_size), next_state)</span><br><span class="line">    <span class="comment"># 加法broadcast，维度[batch, beam, 1] + 维度[batch, beam, tvoc_size]</span></span><br><span class="line">    <span class="comment"># 结果维度为[batch, beam, tvoc_size]，相当于每个batch的每个beam在每个词上继续延伸都有一个概率</span></span><br><span class="line">    curr_log_probs = torch.unsqueeze(log_probs, <span class="number">2</span>) + step_log_probs</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply length penalty</span></span><br><span class="line">    <span class="comment"># 当前decode出的sequence长度是time+1，batch内的所有sequence长度都是一样的</span></span><br><span class="line">    <span class="comment"># （除了那些已经结束的，但是反正不用管）</span></span><br><span class="line">    <span class="comment"># TODO</span></span><br><span class="line">    <span class="comment"># 对log prob施加长度惩罚，得到scores</span></span><br><span class="line">    length_penalty = ((<span class="number">5.0</span> + float(time + <span class="number">1</span>)) / <span class="number">6.0</span>) ** alpha</span><br><span class="line">    <span class="comment"># curr_scores: [batch, beam, tvoc_size]</span></span><br><span class="line">    curr_scores = curr_log_probs / length_penalty</span><br><span class="line">    <span class="comment"># vocab_size = tvoc_size</span></span><br><span class="line">    vocab_size = curr_scores.shape[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Select top-k candidates</span></span><br><span class="line">    <span class="comment"># 从每个句子的所有beam的每个可能的下一个词（共beam*vocab个）中找出最好的2*beam个</span></span><br><span class="line">    <span class="comment"># 作为下一轮beam的候选</span></span><br><span class="line">    <span class="comment"># TODO：为什么是2*beam个？</span></span><br><span class="line">    <span class="comment"># [batch_size, beam_size * vocab_size]</span></span><br><span class="line">    curr_scores = torch.reshape(curr_scores, [<span class="number">-1</span>, beam_size * vocab_size])</span><br><span class="line">    <span class="comment"># [batch_size, 2 * beam_size]</span></span><br><span class="line">    top_scores, top_indices = torch.topk(curr_scores, k=<span class="number">2</span>*beam_size)</span><br><span class="line">    <span class="comment"># Shape: [batch_size, 2 * beam_size]</span></span><br><span class="line">    <span class="comment"># 新的beam是从哪一个beam延伸出来的</span></span><br><span class="line">    beam_indices = top_indices // vocab_size</span><br><span class="line">    <span class="comment"># 新的beam对应的具体是哪个词</span></span><br><span class="line">    symbol_indices = top_indices % vocab_size</span><br><span class="line">    <span class="comment"># Expand sequences</span></span><br><span class="line">    <span class="comment"># [batch_size, 2 * beam_size, time + 1]</span></span><br><span class="line">    candidate_seqs = _gather_2d(seqs, beam_indices)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure><p>这里的<code>_gather_2d</code>函数的功能可能不太好理解。<a href="https://pytorch.org/docs/stable/generated/torch.gather.html" target="_blank" rel="noopener">torch.gather</a>这个函数的本意是从tensor中挑出一些组合在一起；在这里就是根据<code>beam_indices</code>挑出一些beam然后组合在一起。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># params: [batch, beam, time + 1]</span></span><br><span class="line"><span class="comment"># indices: [batch, 2 * beam]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_gather_2d</span><span class="params">(params, indices, name=None)</span>:</span></span><br><span class="line">    batch_size = params.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># range_size = 2 * beam</span></span><br><span class="line">    range_size = indices.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 返回一个长度为2 * batch * beam的tensor，内容是[0, 1, ......, 2 * batch * beam - 1]</span></span><br><span class="line">    batch_pos = torch.arange(batch_size * range_size, device=params.device)</span><br><span class="line">    <span class="comment"># batch_pos = [0, 0, ... 0, 1, 1, ..., 1, ..., batch - 1, batch - 1, ..., batch - 1]</span></span><br><span class="line">    <span class="comment"># 每种数字出现的次数是2 * beam次</span></span><br><span class="line">    batch_pos = batch_pos // range_size</span><br><span class="line">    <span class="comment"># batch_pos: [batch, 2 * beam]</span></span><br><span class="line">    <span class="comment"># batch_pos = [[0, 0, ..., 0], [1, 1, ..., 1], ..., [batch - 1, batch - 1, ..., batch - 1]]</span></span><br><span class="line">    batch_pos = torch.reshape(batch_pos, [batch_size, range_size])</span><br><span class="line">    <span class="comment"># TODO：这里实在想象不出来了。。。</span></span><br><span class="line">    output = params[batch_pos, indices]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>再次回到<code>_beam_search_step</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_beam_search_step</span><span class="params">(time, func, state, batch_size, beam_size, alpha,</span></span></span><br><span class="line"><span class="function"><span class="params">                      pad_id, eos_id, max_length, inf=<span class="number">-1e9</span>)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    <span class="comment"># 接下来把词连上去</span></span><br><span class="line">    <span class="comment"># candidate_seqs: [batch, 2 * beam, time + 2]</span></span><br><span class="line">    candidate_seqs = torch.cat([candidate_seqs,</span><br><span class="line">                                torch.unsqueeze(symbol_indices, <span class="number">2</span>)], <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Expand sequences</span></span><br><span class="line">    <span class="comment"># Suppress finished sequences</span></span><br><span class="line">    <span class="comment"># 找出那些产生了&lt;eos&gt;的句子</span></span><br><span class="line">    flags = torch.eq(symbol_indices, eos_id).to(torch.bool)</span><br><span class="line">    <span class="comment"># 将已经结束的句子的分数置为无穷小</span></span><br><span class="line">    <span class="comment"># [batch, 2 * beam_size]</span></span><br><span class="line">    alive_scores = top_scores + flags.to(torch.float32) * inf</span><br><span class="line">    <span class="comment"># 在每个句子中挑出beam个分数最高的句子</span></span><br><span class="line">    <span class="comment"># [batch, beam_size]</span></span><br><span class="line">    alive_scores, alive_indices = torch.topk(alive_scores, beam_size)</span><br><span class="line">    <span class="comment"># 找出对应的词</span></span><br><span class="line">    <span class="comment"># TODO</span></span><br><span class="line">    <span class="comment"># alive_symbols: [batch, beam]</span></span><br><span class="line">    alive_symbols = _gather_2d(symbol_indices, alive_indices)</span><br><span class="line">    <span class="comment"># 找出对应的beam位置</span></span><br><span class="line">    <span class="comment"># TODO</span></span><br><span class="line">    <span class="comment"># alive_indices: [batch, beam]</span></span><br><span class="line">    alive_indices = _gather_2d(beam_indices, alive_indices)</span><br><span class="line">    <span class="comment"># 找出对应的sequence</span></span><br><span class="line">    <span class="comment"># TODO</span></span><br><span class="line">    <span class="comment"># alive_seqs: [batch, beam, time + 1]</span></span><br><span class="line">    alive_seqs = _gather_2d(seqs, alive_indices)</span><br><span class="line">    <span class="comment"># 把新的词连接上去</span></span><br><span class="line">    <span class="comment"># alive_seqs: [batch_size, beam_size, time + 2]</span></span><br><span class="line">    alive_seqs = torch.cat([alive_seqs, torch.unsqueeze(alive_symbols, <span class="number">2</span>)], <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># alive_state[0]["encoder_output"]: [batch, beam, length_s, hidden]</span></span><br><span class="line">    <span class="comment"># alive_state[0]["enc_attn_bias"]: [batch, beam, 1, 1, length_s]</span></span><br><span class="line">    <span class="comment"># alive_state[0]["decoder"]["layer_0"]["k"]: [batch, beam, time + 1, hidden]</span></span><br><span class="line">    alive_state = map_structure(</span><br><span class="line">        <span class="keyword">lambda</span> x: _gather_2d(x, alive_indices),</span><br><span class="line">        next_state)</span><br><span class="line">    alive_log_probs = alive_scores * length_penalty</span><br><span class="line">    <span class="comment"># Check length constraint</span></span><br><span class="line">    <span class="comment"># 如果句子长度超过限制，则分数设为无穷小</span></span><br><span class="line">    length_flags = torch.le(max_length, time + <span class="number">1</span>).float()</span><br><span class="line">    alive_log_probs = alive_log_probs + length_flags * inf</span><br><span class="line">    alive_scores = alive_scores + length_flags * inf</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Select finished sequences</span></span><br><span class="line">    prev_fin_flags, prev_fin_seqs, prev_fin_scores = state.finish</span><br><span class="line">    <span class="comment"># [batch, 2 * beam_size]</span></span><br><span class="line">    step_fin_scores = top_scores + (<span class="number">1.0</span> - flags.to(torch.float32)) * inf</span><br><span class="line">    <span class="comment"># [batch, 3 * beam_size]</span></span><br><span class="line">    fin_flags = torch.cat([prev_fin_flags, flags], dim=<span class="number">1</span>)</span><br><span class="line">    fin_scores = torch.cat([prev_fin_scores, step_fin_scores], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># [batch, beam_size]</span></span><br><span class="line">    fin_scores, fin_indices = torch.topk(fin_scores, beam_size)</span><br><span class="line">    fin_flags = _gather_2d(fin_flags, fin_indices)</span><br><span class="line">    pad_seqs = prev_fin_seqs.new_full([batch_size, beam_size, <span class="number">1</span>], pad_id)</span><br><span class="line">    prev_fin_seqs = torch.cat([prev_fin_seqs, pad_seqs], dim=<span class="number">2</span>)</span><br><span class="line">    fin_seqs = torch.cat([prev_fin_seqs, candidate_seqs], dim=<span class="number">1</span>)</span><br><span class="line">    fin_seqs = _gather_2d(fin_seqs, fin_indices)</span><br><span class="line"></span><br><span class="line">    new_state = BeamSearchState(</span><br><span class="line">        inputs=(alive_seqs, alive_log_probs, alive_scores),</span><br><span class="line">        state=alive_state,</span><br><span class="line">        finish=(fin_flags, fin_seqs, fin_scores),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_state</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">beam_search</span><span class="params">(models, features, params)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    final_state = state</span><br><span class="line">    alive_seqs = final_state.inputs[<span class="number">0</span>]</span><br><span class="line">    alive_scores = final_state.inputs[<span class="number">2</span>]</span><br><span class="line">    final_flags = final_state.finish[<span class="number">0</span>].byte()</span><br><span class="line">    final_seqs = final_state.finish[<span class="number">1</span>]</span><br><span class="line">    final_scores = final_state.finish[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    final_seqs = torch.where(final_flags[:, :, <span class="keyword">None</span>], final_seqs, alive_seqs)</span><br><span class="line">    final_scores = torch.where(final_flags, final_scores, alive_scores)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Append extra &lt;eos&gt;</span></span><br><span class="line">    final_seqs = torch.nn.functional.pad(final_seqs, (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">                                         value=eos_id)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> final_seqs[:, :top_beams, <span class="number">1</span>:], final_scores[:, :top_beams]</span><br></pre></td></tr></table></figure><p>然后再回到<code>_evaluate_model</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_evaluate_model</span><span class="params">(model, sorted_key, dataset, references, params)</span>:</span></span><br><span class="line">            ......</span><br><span class="line">            <span class="comment"># Padding</span></span><br><span class="line">            seqs = torch.squeeze(seqs, dim=<span class="number">1</span>)</span><br><span class="line">            pad_batch = params.decode_batch_size - seqs.shape[<span class="number">0</span>]</span><br><span class="line">            pad_length = pad_max - seqs.shape[<span class="number">1</span>]</span><br><span class="line">            seqs = torch.nn.functional.pad(seqs, (<span class="number">0</span>, pad_length, <span class="number">0</span>, pad_batch))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Synchronization</span></span><br><span class="line">            size.zero_()</span><br><span class="line">            size[dist.get_rank()].copy_(torch.tensor(batch_size))</span><br><span class="line">            dist.all_reduce(size)</span><br><span class="line">            dist.all_gather(t_list, seqs)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> size.sum() == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> dist.get_rank() != <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(params.decode_batch_size):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(dist.get_world_size()):</span><br><span class="line">                    n = size[j]</span><br><span class="line">                    seq = _convert_to_string(t_list[j][i], params)</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> i &gt;= n:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Restore BPE segmentation</span></span><br><span class="line">                    seq = BPE.decode(seq)</span><br><span class="line"></span><br><span class="line">                    results.append(seq.split())</span><br><span class="line"></span><br><span class="line">            t = time.time() - t</span><br><span class="line">            print(<span class="string">"Finished batch: %d (%.3f sec)"</span> % (counter, t))</span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> dist.get_rank() == <span class="number">0</span>:</span><br><span class="line">        restored_results = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(len(results)):</span><br><span class="line">            restored_results.append(results[sorted_key[idx]])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> bleu(restored_results, references)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.0</span></span><br></pre></td></tr></table></figure><p>最后回到<code>evaluate</code>函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, sorted_key, dataset, base_dir, references, params)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    <span class="comment"># 接下来的工作就是保存和替换checkpoint，暂时没什么用</span></span><br><span class="line">    <span class="comment"># Save records</span></span><br><span class="line">    <span class="keyword">if</span> dist.get_rank() == <span class="number">0</span>:</span><br><span class="line">        scalar(<span class="string">"BLEU/score"</span>, score, global_step, write_every_n_steps=<span class="number">1</span>)</span><br><span class="line">        print(<span class="string">"BLEU at step %d: %f"</span> % (global_step, score))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save checkpoint to save_path</span></span><br><span class="line">        save(&#123;<span class="string">"model"</span>: model.state_dict(), <span class="string">"step"</span>: global_step&#125;, save_path)</span><br><span class="line"></span><br><span class="line">        _save_log(log_name, (<span class="string">"BLEU"</span>, global_step, score))</span><br><span class="line">        records = _read_score_record(record_name)</span><br><span class="line">        record = [latest_checkpoint(save_path).split(<span class="string">"/"</span>)[<span class="number">-1</span>], score]</span><br><span class="line"></span><br><span class="line">        added, removed, records = _add_to_record(records, record, max_to_keep)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> added <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="comment"># Remove latest checkpoint</span></span><br><span class="line">            filename = latest_checkpoint(save_path)</span><br><span class="line">            print(<span class="string">"Removing %s"</span> % filename)</span><br><span class="line">            files = glob.glob(filename + <span class="string">"*"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> files:</span><br><span class="line">                os.remove(name)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> removed <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            filename = os.path.join(save_path, removed)</span><br><span class="line">            print(<span class="string">"Removing %s"</span> % filename)</span><br><span class="line">            files = glob.glob(filename + <span class="string">"*"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> files:</span><br><span class="line">                os.remove(name)</span><br><span class="line"></span><br><span class="line">        _save_score_record(record_name, records)</span><br><span class="line"></span><br><span class="line">        best_score = records[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">        print(<span class="string">"Best score at step %d: %f"</span> % (global_step, best_score))</span><br></pre></td></tr></table></figure><h2>附录：infer阶段的decoder</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/post/thumt-code-summary-1&quot;&gt;简介篇地址&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="https://zhanghuimeng.github.io/categories/NLP/"/>
    
    
      <category term="THUMT" scheme="https://zhanghuimeng.github.io/tags/THUMT/"/>
    
  </entry>
  
  <entry>
    <title>THUMT代码详解（3）：训练阶段模型和数据流</title>
    <link href="https://zhanghuimeng.github.io/post/thumt-code-summary-3/"/>
    <id>https://zhanghuimeng.github.io/post/thumt-code-summary-3/</id>
    <published>2020-08-31T12:04:23.000Z</published>
    <updated>2020-08-31T12:04:23.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="/post/thumt-code-summary-1">简介篇地址</a></p><a id="more"></a><p>接下来是训练阶段的模型和数据流。入口在<a href="https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/bin/trainer.py" target="_blank" rel="noopener">trainer.py</a>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(args)</span>:</span></span><br><span class="line">    model_cls = models.get_model(args.model)</span><br><span class="line">    ......</span><br><span class="line">    model = model_cls(params).cuda()</span><br><span class="line">    ......</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_fn</span><span class="params">(inputs)</span>:</span></span><br><span class="line">        features, labels = inputs</span><br><span class="line">        loss = model(features, labels)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    ......</span><br><span class="line">    loss = train_fn(features)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure><p>THUMT-PyTorch版本中只实现了Transformer这个模型，因此<code>models.get_model</code>必然会返回它。</p><p>之后就是具体的模型和数据流了，分成模型入口、encoder和decoder来讲。</p><p>下文中会用以下符号标出变量的维度：</p><ul><li><code>batch</code>：batch size</li><li><code>length_s</code>：batch中源端句子的长度</li><li><code>length_t</code>：batch中目标端句子的长度</li><li><code>hidden</code>：表示的长度，默认为512</li><li><code>h2</code>: <code>= hidden // heads</code></li></ul><h2>模型入口</h2><p>模型初始化时首先执行<code>Transformer.__init__</code>函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"transformer"</span>)</span>:</span></span><br><span class="line">    super(Transformer, self).__init__(name=name)</span><br><span class="line">    self.params = params</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        <span class="comment"># 创建embedding，默认不共享</span></span><br><span class="line">        self.build_embedding(params)</span><br><span class="line">        <span class="comment"># 创建positional embedding，详见Transformer论文</span></span><br><span class="line">        self.encoding = modules.PositionalEmbedding()</span><br><span class="line">        <span class="comment"># 创建encoder及其参数，默认为6层</span></span><br><span class="line">        self.encoder = TransformerEncoder(params)</span><br><span class="line">        <span class="comment"># 创建decoder及其参数，默认为6层</span></span><br><span class="line">        self.decoder = TransformerDecoder(params)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算翻译结果和标签之间的交叉熵，label_smooting默认0.1</span></span><br><span class="line">    self.criterion = modules.SmoothedCrossEntropyLoss(</span><br><span class="line">        params.label_smoothing)</span><br><span class="line">    <span class="comment"># dropout，默认0.1</span></span><br><span class="line">    self.dropout = params.residual_dropout</span><br><span class="line">    <span class="comment"># hidden_size，默认512</span></span><br><span class="line">    self.hidden_size = params.hidden_size</span><br><span class="line">    <span class="comment"># encoder层数，默认6层</span></span><br><span class="line">    self.num_encoder_layers = params.num_encoder_layers</span><br><span class="line">    <span class="comment"># decoder层数，默认6层</span></span><br><span class="line">    self.num_decoder_layers = params.num_decoder_layers</span><br><span class="line">    <span class="comment"># 初始化embedding</span></span><br><span class="line">    self.reset_parameters()</span><br></pre></td></tr></table></figure><p>其中调用的<code>Transformer.build_embedding</code>函数是这样的：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_embedding</span><span class="params">(self, params)</span>:</span></span><br><span class="line">    svoc_size = len(params.vocabulary[<span class="string">"source"</span>])</span><br><span class="line">    tvoc_size = len(params.vocabulary[<span class="string">"target"</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> params.shared_source_target_embedding <span class="keyword">and</span> svoc_size != tvoc_size:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Cannot share source and target embedding."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> params.shared_embedding_and_softmax_weights:</span><br><span class="line">        self.softmax_weights = torch.nn.Parameter(</span><br><span class="line">            torch.empty([tvoc_size, params.hidden_size]))</span><br><span class="line">        self.add_name(self.softmax_weights, <span class="string">"softmax_weights"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> params.shared_source_target_embedding:</span><br><span class="line">        self.source_embedding = torch.nn.Parameter(</span><br><span class="line">            torch.empty([svoc_size, params.hidden_size]))</span><br><span class="line">        self.target_embedding = torch.nn.Parameter(</span><br><span class="line">            torch.empty([tvoc_size, params.hidden_size]))</span><br><span class="line">        self.add_name(self.source_embedding, <span class="string">"source_embedding"</span>)</span><br><span class="line">        self.add_name(self.target_embedding, <span class="string">"target_embedding"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.weights = torch.nn.Parameter(</span><br><span class="line">            torch.empty([svoc_size, params.hidden_size]))</span><br><span class="line">        self.add_name(self.weights, <span class="string">"weights"</span>)</span><br><span class="line"></span><br><span class="line">    self.bias = torch.nn.Parameter(torch.zeros([params.hidden_size]))</span><br><span class="line">    self.add_name(self.bias, <span class="string">"bias"</span>)</span><br></pre></td></tr></table></figure><p>还是很简单易懂的。</p><p><code>Transformer.__init__</code>函数中分别创建了一个<code>TransformerEncoder</code>和一个<code>TransformerDecoder</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"encoder"</span>)</span>:</span></span><br><span class="line">    super(TransformerEncoder, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    self.normalization = params.normalization</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            TransformerEncoderLayer(params, name=<span class="string">"layer_%d"</span> % i)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(params.num_encoder_layers)])</span><br><span class="line">        <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">            self.layer_norm = modules.LayerNorm(params.hidden_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.layer_norm = <span class="keyword">None</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"decoder"</span>)</span>:</span></span><br><span class="line">    super(TransformerDecoder, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    self.normalization = params.normalization</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            TransformerDecoderLayer(params, name=<span class="string">"layer_%d"</span> % i)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(params.num_decoder_layers)])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">            self.layer_norm = modules.LayerNorm(params.hidden_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.layer_norm = <span class="keyword">None</span></span><br></pre></td></tr></table></figure><p>具体内容放到encoder和decoder部分再去说。</p><p><code>Transformer.__init__</code>函数最后调用的<code>Transformer.reset_parameters</code>函数也很简单：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span><span class="params">(self)</span>:</span></span><br><span class="line">    nn.init.normal_(self.src_embedding, mean=<span class="number">0.0</span>,</span><br><span class="line">                    std=self.params.hidden_size ** <span class="number">-0.5</span>)</span><br><span class="line">    nn.init.normal_(self.tgt_embedding, mean=<span class="number">0.0</span>,</span><br><span class="line">                    std=self.params.hidden_size ** <span class="number">-0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.params.shared_embedding_and_softmax_weights:</span><br><span class="line">        nn.init.normal_(self.softmax_weights, mean=<span class="number">0.0</span>,</span><br><span class="line">                        std=self.params.hidden_size ** <span class="number">-0.5</span>)</span><br></pre></td></tr></table></figure><p>值得注意的是这里的<code>src_embedding</code>、<code>tgt_embedding</code>和<code>softmax_weights</code>都是使用<code>@property</code>装饰器的属性，因为它们是可以共享权重的。</p><p>然后把features和labels输入到模型中，调用的是<code>Transformer.forward</code>函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, features, labels, mode=<span class="string">"train"</span>, level=<span class="string">"sentence"</span>)</span>:</span></span><br><span class="line">    <span class="comment"># mask: [batch, length_s]</span></span><br><span class="line">    mask = features[<span class="string">"target_mask"</span>]</span><br><span class="line"></span><br><span class="line">    state = self.empty_state(features[<span class="string">"target"</span>].shape[<span class="number">0</span>],</span><br><span class="line">                                labels.device)</span><br><span class="line">    state = self.encode(features, state)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure><p>其中调用的<code>Transformer.empty_state</code>函数是这样的：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">empty_state</span><span class="params">(self, batch_size, device)</span>:</span></span><br><span class="line">    state = &#123;</span><br><span class="line">        <span class="string">"decoder"</span>: &#123;</span><br><span class="line">            <span class="string">"layer_%d"</span> % i: &#123;</span><br><span class="line">                <span class="string">"k"</span>: torch.zeros([batch_size, <span class="number">0</span>, self.hidden_size],</span><br><span class="line">                                    device=device),</span><br><span class="line">                <span class="string">"v"</span>: torch.zeros([batch_size, <span class="number">0</span>, self.hidden_size],</span><br><span class="line">                                    device=device)</span><br><span class="line">            &#125; <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_decoder_layers)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> state</span><br></pre></td></tr></table></figure><p>大概就是创建了decoder每层的key和value的一个位置。</p><p>之后就是encoder部分了。</p><h2>encoder</h2><p>首先是<code>encode</code>函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, features, state)</span>:</span></span><br><span class="line">    <span class="comment"># src_seq: [batch, length_s]</span></span><br><span class="line">    src_seq = features[<span class="string">"source"</span>]</span><br><span class="line">    <span class="comment"># src_mask: [batch, length_s]</span></span><br><span class="line">    src_mask = features[<span class="string">"source_mask"</span>]</span><br><span class="line">    enc_attn_bias = self.masking_bias(src_mask)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure><p><code>encode</code>函数调用了<code>masking_bias</code>，这个函数的主要功能是把<code>src_mask</code>中为0的部分变成无穷大，在attention的时候用来去掉padding部分的attention：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mask: [batch, length_s]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masking_bias</span><span class="params">(mask, inf=<span class="number">-1e9</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 把mask中非0部分变成无穷小</span></span><br><span class="line">    ret = (<span class="number">1.0</span> - mask) * inf</span><br><span class="line">    <span class="comment"># unsqueeze后的形状：[batch, 1, 1, length]</span></span><br><span class="line">    <span class="keyword">return</span> torch.unsqueeze(torch.unsqueeze(ret, <span class="number">1</span>), <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>然后回到<code>Transformer.encode</code>函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, features, state)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    <span class="comment"># 实际进行embedding</span></span><br><span class="line">    <span class="comment"># input: [batch, length_s, hidden]</span></span><br><span class="line">    inputs = torch.nn.functional.embedding(src_seq, self.src_embedding)</span><br><span class="line">    <span class="comment"># 保证inputs的方差和均值都是1</span></span><br><span class="line">    inputs = inputs * (self.hidden_size ** <span class="number">0.5</span>)</span><br><span class="line">    inputs = inputs + self.bias</span><br><span class="line">    <span class="comment"># 在dropout之前先把positional embedding加上</span></span><br><span class="line">    inputs = nn.functional.dropout(self.encoding(inputs), self.dropout,</span><br><span class="line">                                    self.training)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将enc_attn_bias的dtype的device修改为与inputs匹配</span></span><br><span class="line">    enc_attn_bias = enc_attn_bias.to(inputs)</span><br><span class="line">    <span class="comment"># 调用TransformerEncoder.forward</span></span><br><span class="line">    encoder_output = self.encoder(inputs, enc_attn_bias)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure><p>前面我们已经看到了，<code>Transformer</code>是先把对应的Module和权重都创建出来，再在运行过程中进行计算的。因此我们首先来看一下创建的过程，首先是<code>TransformerEncoder.__init__</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"encoder"</span>)</span>:</span></span><br><span class="line">    super(TransformerEncoder, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    self.normalization = params.normalization</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        <span class="comment"># 创建了若干个TransformerEncoderLayer，默认数量为6</span></span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            TransformerEncoderLayer(params, name=<span class="string">"layer_%d"</span> % i)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(params.num_encoder_layers)])</span><br><span class="line">        <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">            self.layer_norm = modules.LayerNorm(params.hidden_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.layer_norm = <span class="keyword">None</span></span><br></pre></td></tr></table></figure><p>然后是<code>TransformerEncoderLayer.__init__</code>，每层包括两个sub layer：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"layer"</span>)</span>:</span></span><br><span class="line">    super(TransformerEncoderLayer, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        self.self_attention = AttentionSubLayer(params)</span><br><span class="line">        self.feed_forward = FFNSubLayer(params)</span><br></pre></td></tr></table></figure><p>然后是<code>AttentionSubLayer.__init__</code>和<code>FFNSubLayer.__init__</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"attention"</span>)</span>:</span></span><br><span class="line">    super(AttentionSubLayer, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    self.dropout = params.residual_dropout</span><br><span class="line">    self.normalization = params.normalization</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        self.attention = modules.MultiHeadAttention(</span><br><span class="line">            params.hidden_size, params.num_heads, params.attention_dropout)</span><br><span class="line">        self.layer_norm = modules.LayerNorm(params.hidden_size)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, dtype=None, name=<span class="string">"ffn_layer"</span>)</span>:</span></span><br><span class="line">    super(FFNSubLayer, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    self.dropout = params.residual_dropout</span><br><span class="line">    self.normalization = params.normalization</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        self.ffn_layer = modules.FeedForward(params.hidden_size,</span><br><span class="line">                                                params.filter_size,</span><br><span class="line">                                                dropout=params.relu_dropout)</span><br><span class="line">        self.layer_norm = modules.LayerNorm(params.hidden_size)</span><br></pre></td></tr></table></figure><p><code>AttentionSubLayer.__init__</code>还调用了<code>MultiHeadAttention.__init__</code>（<code>FFNSubLayer.__init__</code>也调用了<code>FeedForward.__init__</code>，但很简单，就不说了）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, num_heads, dropout=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                name=<span class="string">"multihead_attention"</span>)</span>:</span></span><br><span class="line">    super(MultiHeadAttention, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    self.num_heads = num_heads</span><br><span class="line">    self.hidden_size = hidden_size</span><br><span class="line">    self.dropout = dropout</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        <span class="comment"># attention中的q、k、v</span></span><br><span class="line">        self.q_transform = Affine(hidden_size, hidden_size,</span><br><span class="line">                                    name=<span class="string">"q_transform"</span>)</span><br><span class="line">        self.k_transform = Affine(hidden_size, hidden_size,</span><br><span class="line">                                    name=<span class="string">"k_transform"</span>)</span><br><span class="line">        self.v_transform = Affine(hidden_size, hidden_size,</span><br><span class="line">                                    name=<span class="string">"v_transform"</span>)</span><br><span class="line">        <span class="comment"># combine heads之后最后再做一个变换</span></span><br><span class="line">        self.o_transform = Affine(hidden_size, hidden_size,</span><br><span class="line">                                    name=<span class="string">"o_transform"</span>)</span><br><span class="line"></span><br><span class="line">    self.reset_parameters()</span><br></pre></td></tr></table></figure><p>然后再回到<code>Transformer.encode</code>函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, features, state)</span>:</span></span><br><span class="line">    encoder_output = self.encoder(inputs, enc_attn_bias)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure><p>首先调用<code>TransformerEncoder.forward</code>函数（因为这些类都是继承<code>nn.Module</code>的）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, bias)</span>:</span></span><br><span class="line">    <span class="comment"># x: [batch, length_s]</span></span><br><span class="line">    <span class="comment"># bias: [batch, 1, 1, length_s]</span></span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">        x = layer(x, bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">        x = self.layer_norm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>直接调用每一层的<code>TransformerEncoderLayer.forward</code>，把<code>x</code>和<code>bias</code>传过去：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, bias)</span>:</span></span><br><span class="line">    x = self.self_attention(x, bias)</span><br><span class="line">    x = self.feed_forward(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>只是简单地把<code>x</code>和<code>bias</code>传给了<code>MultiHeadAttention.forward</code>函数，接下来的部分比较重要：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在encoder self-attention的情况下，memory和kv都是None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, bias, memory=None, kv=None)</span>:</span></span><br><span class="line">    <span class="comment"># q: [batch, length_s, hidden]</span></span><br><span class="line">    <span class="comment"># bias: [batch, 1, 1, length_s]</span></span><br><span class="line">    q = self.q_transform(query)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 因为是self-attention，所以if部分的内容没用</span></span><br><span class="line">    <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            k, v = kv</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            k, v = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># encoder-decoder attention</span></span><br><span class="line">        k = k <span class="keyword">or</span> self.k_transform(memory)</span><br><span class="line">        v = v <span class="keyword">or</span> self.v_transform(memory)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># self-attention</span></span><br><span class="line">        <span class="comment"># k: [batch, length_s, hidden]</span></span><br><span class="line">        k = self.k_transform(query)</span><br><span class="line">        <span class="comment"># v: [batch, length_s, hidden]</span></span><br><span class="line">        v = self.v_transform(query)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># kv is None，不用管</span></span><br><span class="line">        <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            k = torch.cat([kv[<span class="number">0</span>], k], dim=<span class="number">1</span>)</span><br><span class="line">            v = torch.cat([kv[<span class="number">1</span>], v], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># split heads</span></span><br><span class="line">    <span class="comment"># 主要功能是把维度为[batch, length_s, hidden]的矩阵变成</span></span><br><span class="line">    <span class="comment"># [batch, heads, length_s, h2]</span></span><br><span class="line">    <span class="comment"># 其中h2 = hidden // heads</span></span><br><span class="line">    qh = self.split_heads(q, self.num_heads)</span><br><span class="line">    kh = self.split_heads(k, self.num_heads)</span><br><span class="line">    vh = self.split_heads(v, self.num_heads)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># scale query</span></span><br><span class="line">    qh = qh * (self.hidden_size // self.num_heads) ** <span class="number">-0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># dot-product attention</span></span><br><span class="line">    <span class="comment"># kh: [batch, heads, h2, length_s]</span></span><br><span class="line">    kh = torch.transpose(kh, <span class="number">-2</span>, <span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># 在matmul时，前两维不变，后两维相乘，得到维度为</span></span><br><span class="line">    <span class="comment"># logits: [batch, heads, length_s, length_s]</span></span><br><span class="line">    <span class="comment"># 相当于每一个句子有若干个head，每个head都有一个length_s*length_s的矩阵，代表q到k的attention</span></span><br><span class="line">    logits = torch.matmul(qh, kh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># bias: [batch, 1, 1, length_s]</span></span><br><span class="line">    <span class="comment"># 在每一行加上对应的bias，使padding部分的attention被屏蔽</span></span><br><span class="line">    <span class="comment"># logits维度不变: [batch, heads, length_s, length_s]</span></span><br><span class="line">    <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        logits = logits + bias</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对最后一维（每行）做softmax，然后做dropout，得到</span></span><br><span class="line">    <span class="comment"># weight: [batch, heads, length_s, length_s]</span></span><br><span class="line">    weights = torch.nn.functional.dropout(torch.softmax(logits, dim=<span class="number">-1</span>),</span><br><span class="line">                                            p=self.dropout,</span><br><span class="line">                                            training=self.training)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x: [batch, heads, length_s, h2]</span></span><br><span class="line">    x = torch.matmul(weights, vh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># combine heads</span></span><br><span class="line">    <span class="comment"># output: [batch, length_s, hidden]</span></span><br><span class="line">    output = self.o_transform(self.combine_heads(x))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> output, k, v</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后直接返回output</span></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>说起来，这里面可能有几个不太好理解的地方。第一个是<code>MultiHeadAttentionBase.split_heads</code>和<code>MultiHeadAttentionBase.combine_heads</code>的具体实现方法：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_heads</span><span class="params">(x, heads)</span>:</span></span><br><span class="line">    batch = x.shape[<span class="number">0</span>]</span><br><span class="line">    length = x.shape[<span class="number">1</span>]</span><br><span class="line">    channels = x.shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    y = torch.reshape(x, [batch, length, heads, channels // heads])</span><br><span class="line">    <span class="keyword">return</span> torch.transpose(y, <span class="number">2</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>为什么要先reshape再transpose，而不是直接reshape呢？显然，我们需要修改<code>head</code>这一维度的位置。reshape成<code>[batch, length, heads, h2]</code>的维度之后，相当于每个token长度为<code>heads * h2</code>的表示向量被分成了<code>heads</code>个向量，每个向量的长度为<code>h2</code>。然而我们希望的是每个句子的表示分成<code>heads</code>个矩阵，每个矩阵的维度是<code>[length, h2]</code>。为了做到这一点，需要进行transpose，也就相当于把后三维旋转一下：</p><p><img src="transpose.png" alt="transpose的过程"></p><p>而如果直接reshape的话，实际效果是把<code>length</code>一维分到<code>heads</code>一维去了，这显然不太合理。</p><p>如果还是觉得不够形象的话，请参见这篇文章：<a href="https://lihan.me/2018/01/numpy-reshape-and-transpose/" target="_blank" rel="noopener">Numpy reshape and transpose</a></p><p>而<code>combine_heads</code>正好是把上述过程反过来：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combine_heads</span><span class="params">(x)</span>:</span></span><br><span class="line">    batch = x.shape[<span class="number">0</span>]</span><br><span class="line">    heads = x.shape[<span class="number">1</span>]</span><br><span class="line">    length = x.shape[<span class="number">2</span>]</span><br><span class="line">    channels = x.shape[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    y = torch.transpose(x, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.reshape(y, [batch, length, heads * channels])</span><br></pre></td></tr></table></figure><p>第二个地方是<code>torch.matmul</code>的broadcast。在上述过程中，一个维度为<code>[batch, heads, length_s, h2]</code>和一个维度为<code>[batch, heads, h2, length_s]</code>的矩阵相乘，实际效果是有<code>batch * heads</code>个维度为<code>[length_s, h2]</code>的矩阵和<code>[h2, length_s]</code>的矩阵相乘。这倒是很好理解，不过还是应该说一下<code>torch.matmul</code>的broadcast规则。在多维数据的情况下，<code>matmul</code>使用两个矩阵的后两个维度进行相乘，其他的维度都可以认为是batch维度。详情见<a href="https://pytorch.org/docs/stable/generated/torch.matmul.html" target="_blank" rel="noopener">TORCH.MATMUL</a></p><p>第三个地方是<code>bias</code>和<code>logits</code>加法的broadcast（好吧，怎么又是broadcast）：一个维度为<code>[batch, 1, 1, length_s]</code>的矩阵加到维度为<code>[batch, heads, length_s, length_s]</code>的矩阵上，如何broadcast呢？这里需要参照<a href="https://pytorch.org/docs/stable/notes/broadcasting.html" target="_blank" rel="noopener">BROADCASTING SEMANTICS</a>：</p><blockquote><p>Two tensors are “broadcastable” if the following rules hold:</p><ul><li>Each tensor has at least one dimension.</li><li>When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.</li></ul></blockquote><p>显然，<code>bias</code>矩阵的大小是符合这个条件的。最后的效果就是，对于batch里的每一个句子，它的每一个head对应的attention矩阵的每一行都加上了对应于mask的一个偏置，把padding部分的k和v都屏蔽掉了。</p><p>（Feed-forward部分忽略）</p><p>最后回到<code>Transformer.encode</code>函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, features, state)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    state[<span class="string">"encoder_output"</span>] = encoder_output</span><br><span class="line">    state[<span class="string">"enc_attn_bias"</span>] = enc_attn_bias</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> state</span><br></pre></td></tr></table></figure><p>此时的<code>state</code>是这样的（在encode的过程中<code>&quot;decoder&quot;</code>的部分根本就没动，只是加了几个属性）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"encoder_output"</span>: [batch, length_s, hidden]</span><br><span class="line">    <span class="string">"enc_attn_bias"</span>: [batch, <span class="number">1</span>, <span class="number">1</span>, length_s]</span><br><span class="line">    <span class="string">"decoder"</span>: ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2>decoder</h2><p>和之前一样，我们先来看看创建权重的过程。首先是<code>TransformerDecoder.__init__</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"decoder"</span>)</span>:</span></span><br><span class="line">    super(TransformerDecoder, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    self.normalization = params.normalization</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            TransformerDecoderLayer(params, name=<span class="string">"layer_%d"</span> % i)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(params.num_decoder_layers)])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">            self.layer_norm = modules.LayerNorm(params.hidden_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.layer_norm = <span class="keyword">None</span></span><br></pre></td></tr></table></figure><p>很显然，它创建了若干个（默认为6个）<code>TransformerDecoderLayer</code>。<code>TransformerDecoderLayer.__init__</code>的内容如下所示：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"layer"</span>)</span>:</span></span><br><span class="line">    super(TransformerDecoderLayer, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        self.self_attention = AttentionSubLayer(params,</span><br><span class="line">                                                name=<span class="string">"self_attention"</span>)</span><br><span class="line">        self.encdec_attention = AttentionSubLayer(params,</span><br><span class="line">                                                name=<span class="string">"encdec_attention"</span>)</span><br><span class="line">        self.feed_forward = FFNSubLayer(params)</span><br></pre></td></tr></table></figure><p>它创建了两个<code>AttentionSubLayer</code>（self-attention和enc-dec attention）和一个<code>FFNSublayer</code>，这些在encoder的部分都已经说过了，所以不再说了。</p><p>之后我们回到<code>Transformer.forward</code>函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, features, labels, mode=<span class="string">"train"</span>, level=<span class="string">"sentence"</span>)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    logits, _ = self.decode(features, state, mode=mode)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure><p>它调用了<code>Transformer.decode</code>函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, features, state, mode=<span class="string">"infer"</span>)</span>:</span></span><br><span class="line">    <span class="comment"># [batch, length_t]</span></span><br><span class="line">    tgt_seq = features[<span class="string">"target"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch, 1, 1, length_s]</span></span><br><span class="line">    enc_attn_bias = state[<span class="string">"enc_attn_bias"</span>]</span><br><span class="line">    <span class="comment"># [1, 1, length_t, length_t]</span></span><br><span class="line">    <span class="comment"># 是一个下三角部分为0（包括对角线），上三角部分为无穷小的矩阵</span></span><br><span class="line">    dec_attn_bias = self.causal_bias(tgt_seq.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对目标端进行embedding</span></span><br><span class="line">    <span class="comment"># targets: [batch, length_t, hidden]</span></span><br><span class="line">    targets = torch.nn.functional.embedding(tgt_seq, self.tgt_embedding)</span><br><span class="line">    targets = targets * (self.hidden_size ** <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 去掉tgt_seq开头的&lt;bos&gt;，把表示全都换成0</span></span><br><span class="line">    decoder_input = torch.cat(</span><br><span class="line">        [targets.new_zeros([targets.shape[<span class="number">0</span>], <span class="number">1</span>, targets.shape[<span class="number">-1</span>]]),</span><br><span class="line">            targets[:, <span class="number">1</span>:, :]], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 加入positional encoding，进行dropout</span></span><br><span class="line">    decoder_input = nn.functional.dropout(self.encoding(decoder_input),</span><br><span class="line">                                            self.dropout, self.training)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch, length_s, hidden]</span></span><br><span class="line">    encoder_output = state[<span class="string">"encoder_output"</span>]</span><br><span class="line">    <span class="comment"># 调整dec_attn_bias的dtype和device</span></span><br><span class="line">    dec_attn_bias = dec_attn_bias.to(targets)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># train阶段先不用管</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">"infer"</span>:</span><br><span class="line">        decoder_input = decoder_input[:, <span class="number">-1</span>:, :]</span><br><span class="line">        dec_attn_bias = dec_attn_bias[:, :, <span class="number">-1</span>:, :]</span><br><span class="line"></span><br><span class="line">    decoder_output = self.decoder(decoder_input, dec_attn_bias,</span><br><span class="line">                                    enc_attn_bias, encoder_output, state)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure><p>为什么这里要把<code>decoder_input</code>每个batch的第一列都换成0呢？这和训练模式下我们到底是如何进行训练的有关。事实上，假设我们有一个句子是</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    <span class="string">""</span>,</span><br><span class="line">    <span class="string">"Ich"</span>,</span><br><span class="line">    <span class="string">"bin"</span>,</span><br><span class="line">    <span class="string">"ein"</span>,</span><br><span class="line">    <span class="string">"Student"</span>,</span><br><span class="line">    <span class="string">"."</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>那么我们在完成decoder的解码过程后，在每个位置期望得到的是：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    <span class="string">"Ich"</span> (The token after <span class="string">""</span>),</span><br><span class="line">    <span class="string">"bin"</span> (The token after <span class="string">"Ich"</span>),</span><br><span class="line">    <span class="string">"ein"</span> (The token after <span class="string">"Ich bin"</span>),</span><br><span class="line">    <span class="string">"Student"</span> (The token after <span class="string">"Ich bin ein"</span>),</span><br><span class="line">    <span class="string">"."</span> (The token after <span class="string">"Ich bin ein Student"</span>),</span><br><span class="line">    <span class="string">"&lt;eos&gt;"</span> (The token after <span class="string">"Ich bin ein Student ."</span>)</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>这种方法叫做Teacher Forcing：在训练阶段，我们不考虑每一步decoder实际输出的token，而是直接把正确的（一部分）句子输入到decoder中。</p><p>然后就拿着这些input、bias、output和state去调用<code>TransformerDecoder.forward</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x: [batch, length_t, hidden]</span></span><br><span class="line"><span class="comment"># attn_bias: [1, 1, length_t, length_t]，下三角为0（包括对角线，上三角为无穷小）</span></span><br><span class="line"><span class="comment"># enc_attn_bias: [batch, 1, 1, length_s]，实际句子部分为0，padding部分为无穷小</span></span><br><span class="line"><span class="comment"># memory: [batch, length_s, hidden]，就是encoder_output...</span></span><br><span class="line"><span class="comment"># state: 见encoder一节最后部分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, attn_bias, encdec_bias, memory, state=None)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(self.layers):</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="comment"># 从encoder节可以看出，这里的state["decoder"]["layer_%d" % i]是两个空的矩阵</span></span><br><span class="line">            <span class="comment"># 传过去只是为了记录</span></span><br><span class="line">            x = layer(x, attn_bias, encdec_bias, memory,</span><br><span class="line">                        state[<span class="string">"decoder"</span>][<span class="string">"layer_%d"</span> % i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = layer(x, attn_bias, encdec_bias, memory, <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">        x = self.layer_norm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p><code>x</code>被逐层传给<code>TransformerDecoderLayer.__call__</code>函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x, attn_bias, encdec_bias, memory, state=None)</span>:</span></span><br><span class="line">    x = self.self_attention(x, attn_bias, state=state)</span><br><span class="line">    x = self.encdec_attention(x, encdec_bias, memory)</span><br><span class="line">    x = self.feed_forward(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>这看起来非常简单，只是逐层调用了两个attention和一个ffn而已。</p><p>接下来首先看self_attention部分（下面会出现大量的重复attention代码，但是它们的功能和之前提到的会有一些区别）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x: [batch, length_t, hidden]，target端embed的结果或上一层的输出</span></span><br><span class="line"><span class="comment"># bias: [1, 1, length_t, length_t]，下三角为0（包括对角线），上三角为无穷小</span></span><br><span class="line"><span class="comment"># memory: None</span></span><br><span class="line"><span class="comment"># state: 见encoder一节最后部分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, bias, memory=None, state=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">        y = self.layer_norm(x)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y = x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 因为目前处于训练阶段，因此调用attention时传过去的state是None</span></span><br><span class="line">    <span class="keyword">if</span> self.training <span class="keyword">or</span> state <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        y = self.attention(y, bias, memory, <span class="keyword">None</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        kv = [state[<span class="string">"k"</span>], state[<span class="string">"v"</span>]]</span><br><span class="line">        y, k, v = self.attention(y, bias, memory, kv)</span><br><span class="line">        state[<span class="string">"k"</span>], state[<span class="string">"v"</span>] = k, v</span><br><span class="line"></span><br><span class="line">    y = nn.functional.dropout(y, self.dropout, self.training)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">        <span class="keyword">return</span> x + y</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(x + y)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># query: [batch, length_t, hidden]，target端embed的结果或上一层的输出</span></span><br><span class="line"><span class="comment"># bias: [batch, 1, 1, length_s]，下三角为0（包括对角线），上三角为无穷小</span></span><br><span class="line"><span class="comment"># memory: None</span></span><br><span class="line"><span class="comment"># kv: None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, bias, memory=None, kv=None)</span>:</span></span><br><span class="line">    <span class="comment"># 用query计算出q</span></span><br><span class="line">    <span class="comment"># q: [batch, length_t, hidden]</span></span><br><span class="line">    q = self.q_transform(query)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            k, v = kv</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            k, v = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># encoder-decoder attention</span></span><br><span class="line">        k = k <span class="keyword">or</span> self.k_transform(memory)</span><br><span class="line">        v = v <span class="keyword">or</span> self.v_transform(memory)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># memory=None，因此是self-attention，看这部分</span></span><br><span class="line">        <span class="comment"># self-attention</span></span><br><span class="line">        <span class="comment"># 用query直接计算出k和v</span></span><br><span class="line">        <span class="comment"># 我 query 我 自 己</span></span><br><span class="line">        <span class="comment"># k, v: [batch, length_t, hidden]</span></span><br><span class="line">        k = self.k_transform(query)</span><br><span class="line">        v = self.v_transform(query)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># kv=None，因此不用管这段</span></span><br><span class="line">        <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            k = torch.cat([kv[<span class="number">0</span>], k], dim=<span class="number">1</span>)</span><br><span class="line">            v = torch.cat([kv[<span class="number">1</span>], v], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># split heads</span></span><br><span class="line">    <span class="comment"># qh, kh, vh: [batch, heads, length_t, h2]</span></span><br><span class="line">    qh = self.split_heads(q, self.num_heads)</span><br><span class="line">    kh = self.split_heads(k, self.num_heads)</span><br><span class="line">    vh = self.split_heads(v, self.num_heads)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># scale query</span></span><br><span class="line">    qh = qh * (self.hidden_size // self.num_heads) ** <span class="number">-0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># dot-product attention</span></span><br><span class="line">    <span class="comment"># kh: [batch, heads, h2, length_t]</span></span><br><span class="line">    kh = torch.transpose(kh, <span class="number">-2</span>, <span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># logits: [batch, heads, length_t, length_t]</span></span><br><span class="line">    logits = torch.matmul(qh, kh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加入bias</span></span><br><span class="line">    <span class="comment"># bias: [1, 1, length_t, length_t]</span></span><br><span class="line">    <span class="comment"># 这里的bias不是mask bias，而是causal bias，也就是在每个attention矩阵中，</span></span><br><span class="line">    <span class="comment"># token不能attend到未来的token</span></span><br><span class="line">    <span class="comment"># 最后得到的attention是一个下三角矩阵</span></span><br><span class="line">    <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        logits = logits + bias</span><br><span class="line">    <span class="comment"># logits: [batch, heads, length_t, length_t]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对最后一维做softmax</span></span><br><span class="line">    <span class="comment"># weights: [batch, heads, length_t, length_t]</span></span><br><span class="line">    weights = torch.nn.functional.dropout(torch.softmax(logits, dim=<span class="number">-1</span>),</span><br><span class="line">                                            p=self.dropout,</span><br><span class="line">                                            training=self.training)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x: [batch, heads, length_t, h2]</span></span><br><span class="line">    x = torch.matmul(weights, vh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># combine heads</span></span><br><span class="line">    <span class="comment"># output: [batch, length_t, hidden]</span></span><br><span class="line">    output = self.o_transform(self.combine_heads(x))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># kv=None，所以不用管</span></span><br><span class="line">    <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> output, k, v</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>那么问题来了。这里到底为什么要做一个带causal bias的attention呢？</p><p>这是因为，对每一个位置来说，它实际能够attend到的部分是已经解码出的部分，不能attend到那些对它来说还没出现的部分。</p><p>然后是enc-dec attention：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x: [batch, length_t, hidden]，本层self-attention的输出</span></span><br><span class="line"><span class="comment"># bias: [batch, 1, 1, lenth_s]，source端的masking bias</span></span><br><span class="line"><span class="comment"># memory: [batch, length_s, hidden]，encoder端的输出</span></span><br><span class="line"><span class="comment"># state: None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, bias, memory=None, state=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">        y = self.layer_norm(x)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y = x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.training <span class="keyword">or</span> state <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="comment"># 因为training=True，因此还是调用这里</span></span><br><span class="line">        y = self.attention(y, bias, memory, <span class="keyword">None</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        kv = [state[<span class="string">"k"</span>], state[<span class="string">"v"</span>]]</span><br><span class="line">        y, k, v = self.attention(y, bias, memory, kv)</span><br><span class="line">        state[<span class="string">"k"</span>], state[<span class="string">"v"</span>] = k, v</span><br><span class="line"></span><br><span class="line">    y = nn.functional.dropout(y, self.dropout, self.training)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">        <span class="keyword">return</span> x + y</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(x + y)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># query: [batch, length_t, hidden]，本层self-attention的输出</span></span><br><span class="line"><span class="comment"># bias: [batch, 1, 1, lenth_s]，source端的masking bias</span></span><br><span class="line"><span class="comment"># memory: [batch, length_s, hidden]，encoder端的输出</span></span><br><span class="line"><span class="comment"># kv: None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, bias, memory=None, kv=None)</span>:</span></span><br><span class="line">    <span class="comment"># q: [batch, length_t, hidden]</span></span><br><span class="line">    <span class="comment"># query来自decoder端</span></span><br><span class="line">    q = self.q_transform(query)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            k, v = kv</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            k, v = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># encoder-decoder attention</span></span><br><span class="line">        <span class="comment"># k, v: [batch, length_s, hidden]</span></span><br><span class="line">        <span class="comment"># kv来自encoder端</span></span><br><span class="line">        k = k <span class="keyword">or</span> self.k_transform(memory)</span><br><span class="line">        v = v <span class="keyword">or</span> self.v_transform(memory)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># self-attention</span></span><br><span class="line">        k = self.k_transform(query)</span><br><span class="line">        v = self.v_transform(query)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            k = torch.cat([kv[<span class="number">0</span>], k], dim=<span class="number">1</span>)</span><br><span class="line">            v = torch.cat([kv[<span class="number">1</span>], v], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># split heads</span></span><br><span class="line">    <span class="comment"># qh: [batch, heads, length_t, h2]</span></span><br><span class="line">    <span class="comment"># kh, vh: [batch, heads, length_s, h2]</span></span><br><span class="line">    qh = self.split_heads(q, self.num_heads)</span><br><span class="line">    kh = self.split_heads(k, self.num_heads)</span><br><span class="line">    vh = self.split_heads(v, self.num_heads)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># scale query</span></span><br><span class="line">    qh = qh * (self.hidden_size // self.num_heads) ** <span class="number">-0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># dot-product attention</span></span><br><span class="line">    <span class="comment"># kh: [batch, heads, h2, length_s]</span></span><br><span class="line">    kh = torch.transpose(kh, <span class="number">-2</span>, <span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># logits: [batch, heads, length_t, length_s]</span></span><br><span class="line">    logits = torch.matmul(qh, kh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每行加上source端的masking bias</span></span><br><span class="line">    <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        logits = logits + bias</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对最后一维做softmax</span></span><br><span class="line">    <span class="comment"># weights: [batch, heads, length_t, length_s]</span></span><br><span class="line">    weights = torch.nn.functional.dropout(torch.softmax(logits, dim=<span class="number">-1</span>),</span><br><span class="line">                                            p=self.dropout,</span><br><span class="line">                                            training=self.training)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x: [batch, heads, length_t, h2]</span></span><br><span class="line">    x = torch.matmul(weights, vh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># combine heads</span></span><br><span class="line">    <span class="comment"># output: [batch, length_t, hidden]</span></span><br><span class="line">    output = self.o_transform(self.combine_heads(x))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> output, k, v</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>回到<code>Transformer.decode</code>的最后一部分：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, features, state, mode=<span class="string">"infer"</span>)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    <span class="comment"># decoder_output: [batch * length_t, hidden]</span></span><br><span class="line">    decoder_output = torch.reshape(decoder_output, [<span class="number">-1</span>, self.hidden_size])</span><br><span class="line">    <span class="comment"># decoder_output: [hidden, batch * length_t]</span></span><br><span class="line">    decoder_output = torch.transpose(decoder_output, <span class="number">-1</span>, <span class="number">-2</span>)</span><br><span class="line">    <span class="comment"># 把每个词的softmax embedding和每个表示做点积，得到每个位置词的概率分布</span></span><br><span class="line">    <span class="comment"># （虽然logits还没有做过处理，不是概率分布）</span></span><br><span class="line">    <span class="comment"># [tvoc_size, hidden] * [hidden, batch * length_t] = [tvoc_size, batch * length_t]</span></span><br><span class="line">    logits = torch.matmul(self.softmax_embedding, decoder_output)</span><br><span class="line">    <span class="comment"># logits: [batch * length_t, tvoc_size]</span></span><br><span class="line">    logits = torch.transpose(logits, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> logits, state</span><br></pre></td></tr></table></figure><p>再回到<code>Transformer.forward</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, features, labels, mode=<span class="string">"train"</span>, level=<span class="string">"sentence"</span>)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    <span class="comment"># 此处state被丢掉了，反正也没用</span></span><br><span class="line">    logits, _ = self.decode(features, state, mode=mode)</span><br><span class="line">    <span class="comment"># labels就是数据处理中target端句子的id，后面加了&lt;eos&gt;</span></span><br><span class="line">    <span class="comment"># logits: [batch * length_t, tvoc_size]</span></span><br><span class="line">    <span class="comment"># labels: [batch, length_t]</span></span><br><span class="line">    <span class="comment"># loss: [batch, length_t]</span></span><br><span class="line">    loss = self.criterion(logits, labels)</span><br><span class="line">    <span class="comment"># mask: [batch, length_t]</span></span><br><span class="line">    mask = mask.to(logits)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 不是eval，就先不管了</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">"eval"</span>:</span><br><span class="line">        <span class="keyword">if</span> level == <span class="string">"sentence"</span>:</span><br><span class="line">            <span class="keyword">return</span> -torch.sum(loss * mask, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>  torch.exp(-loss) * mask - (<span class="number">1</span> - mask)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把loss和mask逐元素相乘，得到mask后的结果，取平均值为loss</span></span><br><span class="line">    <span class="keyword">return</span> torch.sum(loss * mask) / torch.sum(mask)</span><br></pre></td></tr></table></figure><p>这里的<code>self.criterion</code>是<a href="https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/modules/losses.py" target="_blank" rel="noopener">losses.py</a>中实现的<code>SmoothedCrossEntropyLoss</code>，在实现的时候会把<code>labels</code>拉平，具体在这里就不展开了。</p><p>这之后的处理就先不说了，总之，训练阶段的模型和数据流讲完了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/post/thumt-code-summary-1&quot;&gt;简介篇地址&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="https://zhanghuimeng.github.io/categories/NLP/"/>
    
    
      <category term="THUMT" scheme="https://zhanghuimeng.github.io/tags/THUMT/"/>
    
  </entry>
  
  <entry>
    <title>THUMT代码详解（2）：数据处理</title>
    <link href="https://zhanghuimeng.github.io/post/thumt-code-summary-2/"/>
    <id>https://zhanghuimeng.github.io/post/thumt-code-summary-2/</id>
    <published>2020-08-31T11:47:29.000Z</published>
    <updated>2020-08-31T11:47:29.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="/post/thumt-code-summary-1">简介篇地址</a></p><a id="more"></a><p>这篇分为以下三个部分：训练集数据输入、验证集数据输入、测试集数据输入。</p><h2>训练集数据输入</h2><p><a href="https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/bin/trainer.py" target="_blank" rel="noopener">trainer.py</a>中，<code>dataset = data.get_dataset(params.input, &quot;train&quot;, params)</code>一行创建了训练用的数据集。具体的创建方式是：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span><span class="params">(filenames, mode, params)</span>:</span></span><br><span class="line">    input_fn = build_input_fn(filenames, mode, params)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.device(<span class="string">"/cpu:0"</span>):</span><br><span class="line">        dataset = input_fn()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure><p>以下列数据为例，说明<code>train_input_fn</code>对输入数据的处理：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file 0: I am a student .</span><br><span class="line">file 1: Ich bin ein Student .</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_input_fn</span><span class="params">()</span>:</span></span><br><span class="line">    src_dataset = tf.data.TextLineDataset(filenames[<span class="number">0</span>])</span><br><span class="line">    tgt_dataset = tf.data.TextLineDataset(filenames[<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># src_dataset: ["I am a student .", ...]</span></span><br><span class="line">    <span class="comment"># tgt_dataset: ["Ich bin ein Student .", ...]</span></span><br><span class="line">    <span class="comment"># 注：请把最外层的中括号理解成dataset结构</span></span><br><span class="line"></span><br><span class="line">    dataset = tf.data.Dataset.zip((src_dataset, tgt_dataset))</span><br><span class="line">    dataset = dataset.shard(torch.distributed.get_world_size(),</span><br><span class="line">                            torch.distributed.get_rank())</span><br><span class="line">    dataset = dataset.prefetch(params.buffer_size)</span><br><span class="line">    dataset = dataset.shuffle(params.buffer_size)</span><br><span class="line">    <span class="comment"># dataset: [("I am a student .", "Ich bin ein Student ."), ...]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Split string</span></span><br><span class="line">    dataset = dataset.map(</span><br><span class="line">        <span class="keyword">lambda</span> x, y: (tf.strings.split([x]).values,</span><br><span class="line">                        tf.strings.split([y]).values),</span><br><span class="line">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line">    <span class="comment"># dataset: [(["I", "am", "a", "student", "."], ["Ich". "bin", "ein", "Student", "."]), ...]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Append BOS and EOS</span></span><br><span class="line">    dataset = dataset.map(</span><br><span class="line">        <span class="keyword">lambda</span> x, y: (</span><br><span class="line">            (tf.concat([x, [tf.constant(params.eos)]], axis=<span class="number">0</span>),</span><br><span class="line">                tf.concat([[tf.constant(params.bos)], y], axis=<span class="number">0</span>)),</span><br><span class="line">            tf.concat([y, [tf.constant(params.eos)]], axis=<span class="number">0</span>)),</span><br><span class="line">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line">    <span class="comment"># dataset: [((["I", "am", "a", "student", ".", "&lt;eos&gt;"], ["&lt;bos&gt;", "Ich". "bin", "ein", "Student", "."]),</span></span><br><span class="line">    <span class="comment">#           ["Ich". "bin", "ein", "Student", ".", "&lt;eos&gt;"]), ...]</span></span><br><span class="line"></span><br><span class="line">    dataset = dataset.map(</span><br><span class="line">        <span class="keyword">lambda</span> x, y: (&#123;</span><br><span class="line">            <span class="string">"source"</span>: x[<span class="number">0</span>],</span><br><span class="line">            <span class="string">"source_length"</span>: tf.shape(x[<span class="number">0</span>])[<span class="number">0</span>],</span><br><span class="line">            <span class="string">"target"</span>: x[<span class="number">1</span>],</span><br><span class="line">            <span class="string">"target_length"</span>: tf.shape(x[<span class="number">1</span>])[<span class="number">0</span>]</span><br><span class="line">        &#125;, y),</span><br><span class="line">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line">    <span class="comment"># dataset = [(&#123;</span></span><br><span class="line">    <span class="comment">#     "source": ["I", "am", "a", "student", ".", "&lt;eos&gt;"],</span></span><br><span class="line">    <span class="comment">#     "source_length": 6,</span></span><br><span class="line">    <span class="comment">#     "target": ["&lt;bos&gt;", "Ich". "bin", "ein", "Student", "."],</span></span><br><span class="line">    <span class="comment">#     "target_length": 6,</span></span><br><span class="line">    <span class="comment">#     &#125;, ["Ich". "bin", "ein", "Student", ".", "&lt;eos&gt;"]), ...]</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bucket_boundaries</span><span class="params">(max_length, min_length=<span class="number">8</span>, step=<span class="number">8</span>)</span>:</span></span><br><span class="line">        x = min_length</span><br><span class="line">        boundaries = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> x &lt;= max_length:</span><br><span class="line">            boundaries.append(x + <span class="number">1</span>)</span><br><span class="line">            x += step</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> boundaries</span><br><span class="line"></span><br><span class="line">    batch_size = params.batch_size</span><br><span class="line">    max_length = (params.max_length // <span class="number">8</span>) * <span class="number">8</span></span><br><span class="line">    min_length = params.min_length</span><br><span class="line">    boundaries = bucket_boundaries(max_length)</span><br><span class="line">    batch_sizes = [max(<span class="number">1</span>, batch_size // (x - <span class="number">1</span>))</span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> params.fixed_batch_size <span class="keyword">else</span> batch_size</span><br><span class="line">                    <span class="keyword">for</span> x <span class="keyword">in</span> boundaries] + [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">element_length_func</span><span class="params">(x, y)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.maximum(x[<span class="string">"source_length"</span>], x[<span class="string">"target_length"</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">valid_size</span><span class="params">(x, y)</span>:</span></span><br><span class="line">        size = element_length_func(x, y)</span><br><span class="line">        <span class="keyword">return</span> tf.logical_and(size &gt;= min_length, size &lt;= max_length)</span><br><span class="line"></span><br><span class="line">    transformation_fn = tf.data.experimental.bucket_by_sequence_length(</span><br><span class="line">        element_length_func,</span><br><span class="line">        boundaries,</span><br><span class="line">        batch_sizes,</span><br><span class="line">        padded_shapes=(&#123;</span><br><span class="line">            <span class="string">"source"</span>: tf.TensorShape([<span class="keyword">None</span>]),</span><br><span class="line">            <span class="string">"source_length"</span>: tf.TensorShape([]),</span><br><span class="line">            <span class="string">"target"</span>: tf.TensorShape([<span class="keyword">None</span>]),</span><br><span class="line">            <span class="string">"target_length"</span>: tf.TensorShape([])</span><br><span class="line">            &#125;, tf.TensorShape([<span class="keyword">None</span>])),</span><br><span class="line">        padding_values=(&#123;</span><br><span class="line">            <span class="string">"source"</span>: params.pad,</span><br><span class="line">            <span class="string">"source_length"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="string">"target"</span>: params.pad,</span><br><span class="line">            <span class="string">"target_length"</span>: <span class="number">0</span></span><br><span class="line">            &#125;, params.pad),</span><br><span class="line">        pad_to_bucket_boundary=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    dataset = dataset.filter(valid_size)</span><br><span class="line">    <span class="comment"># 分为batch，进行padding</span></span><br><span class="line">    dataset = dataset.apply(transformation_fn)</span><br><span class="line">    <span class="comment"># 注：下面只写形状，不写具体的值了</span></span><br><span class="line">    <span class="comment"># dataset = [(&#123;</span></span><br><span class="line">    <span class="comment">#     "source": [batch, length],</span></span><br><span class="line">    <span class="comment">#     "source_length": 6,</span></span><br><span class="line">    <span class="comment">#     "target": [batch, length],</span></span><br><span class="line">    <span class="comment">#     "target_length": 6,</span></span><br><span class="line">    <span class="comment">#     &#125;, [batch, length]), ...]</span></span><br><span class="line"></span><br><span class="line">    dataset = dataset.map(</span><br><span class="line">        <span class="keyword">lambda</span> x, y: (&#123;</span><br><span class="line">            <span class="string">"source"</span>: x[<span class="string">"source"</span>],</span><br><span class="line">            <span class="string">"source_mask"</span>: tf.sequence_mask(x[<span class="string">"source_length"</span>],</span><br><span class="line">                                            tf.shape(x[<span class="string">"source"</span>])[<span class="number">1</span>],</span><br><span class="line">                                            tf.float32),</span><br><span class="line">            <span class="string">"target"</span>: x[<span class="string">"target"</span>],</span><br><span class="line">            <span class="string">"target_mask"</span>: tf.sequence_mask(x[<span class="string">"target_length"</span>],</span><br><span class="line">                                            tf.shape(x[<span class="string">"target"</span>])[<span class="number">1</span>],</span><br><span class="line">                                            tf.float32)</span><br><span class="line">        &#125;, y),</span><br><span class="line">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure><p>具体的过程代码里已经说得很清楚了，只有几点需要注意的：</p><ul><li>在调用<a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/bucket_by_sequence_length" target="_blank" rel="noopener">tf.data.experimental.bucket_by_sequence_length</a>函数时，<code>source</code>和<code>target</code> pad后的长度是相同的</li><li><code>target</code>前面是<code>&lt;bos&gt;</code>，label后面是<code>&lt;eos&gt;</code></li><li><code>source_mask</code>和<code>target_mask</code>的作用是在attention的时候帮助创建bias，后面会提到</li></ul><p>之后，在具体的训练过程中，我们显然还需要把token转换成id，参见<a href="https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/bin/trainer.py" target="_blank" rel="noopener">trainer.py</a>中的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">features = data.lookup(features, <span class="string">"train"</span>, params)</span><br></pre></td></tr></table></figure><p>这一行，调用了<a href="https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/data/vocab.py" target="_blank" rel="noopener">vocab.py</a>中的<code>lookup</code>函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lookup</span><span class="params">(inputs, mode, params)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> mode != <span class="string">"infer"</span>:</span><br><span class="line">        features, labels = inputs</span><br><span class="line">        source, target = features[<span class="string">"source"</span>], features[<span class="string">"target"</span>]</span><br><span class="line">        <span class="comment"># 把tf.Tensor转换成np.ndarray</span></span><br><span class="line">        source = source.numpy()</span><br><span class="line">        target = target.numpy()</span><br><span class="line">        labels = labels.numpy()</span><br><span class="line">        src_mask = torch.FloatTensor(features[<span class="string">"source_mask"</span>].numpy()).cuda()</span><br><span class="line">        tgt_mask = torch.FloatTensor(features[<span class="string">"target_mask"</span>].numpy()).cuda()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 把token转换成id，再把id转换成torch.LongTensor</span></span><br><span class="line">        source = _lookup(source, params.lookup[<span class="string">"source"</span>])</span><br><span class="line">        target = _lookup(target, params.lookup[<span class="string">"target"</span>])</span><br><span class="line">        labels = _lookup(labels, params.lookup[<span class="string">"target"</span>])</span><br><span class="line"></span><br><span class="line">        features = &#123;</span><br><span class="line">            <span class="string">"source"</span>: source,</span><br><span class="line">            <span class="string">"source_mask"</span>: src_mask,</span><br><span class="line">            <span class="string">"target"</span>: target,</span><br><span class="line">            <span class="string">"target_mask"</span>: tgt_mask</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> features, labels</span><br><span class="line">    <span class="comment"># 在训练阶段后面都不用看了</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        source = inputs[<span class="string">"source"</span>].numpy()</span><br><span class="line">        source = _lookup(source, params.lookup[<span class="string">"source"</span>])</span><br><span class="line">        src_mask = torch.FloatTensor(inputs[<span class="string">"source_mask"</span>].numpy()).cuda()</span><br><span class="line"></span><br><span class="line">        features = &#123;</span><br><span class="line">            <span class="string">"source"</span>: source,</span><br><span class="line">            <span class="string">"source_mask"</span>: src_mask</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> features</span><br></pre></td></tr></table></figure><p><code>lookup</code>函数之前的数据处理是用TensorFlow做的，大概是因为PyTorch版是用TensorFlow改过来的，数据处理就沿用了tf版本的做法。</p><p>最后features和label就可以输到模型里了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = train_fn(features)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_fn</span><span class="params">(inputs)</span>:</span></span><br><span class="line">    features, labels = inputs</span><br><span class="line">    loss = model(features, labels)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h2>验证集数据输入</h2><p>在<a href="https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/bin/trainer.py" target="_blank" rel="noopener">trainer.py</a>里，如果验证集存在，那么模型会加载一堆东西：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> params.validation:</span><br><span class="line">    sorted_key, eval_dataset = data.get_dataset(</span><br><span class="line">        params.validation, <span class="string">"infer"</span>, params)</span><br><span class="line">    references = load_references(params.references)</span><br></pre></td></tr></table></figure><p>下面我们来逐个看一下。</p><p>首先是<code>data.get_dataset</code>。注意到这里调用<code>data.get_dataset</code>时使用的参数是<code>&quot;infer&quot;</code>，也就是说会使用<a href="https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/data/dataset.py" target="_blank" rel="noopener">dataset.py</a>里的那个<code>infer_input_fn()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">infer_input_fn</span><span class="params">()</span>:</span></span><br><span class="line">    sorted_key, sorted_data = sort_input_file(filenames)</span><br></pre></td></tr></table></figure><p><code>infer_input_fn</code>首先调用了<code>sort_input_file</code>函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sort_input_file</span><span class="params">(filename, reverse=True)</span>:</span></span><br><span class="line">    <span class="comment"># 实际上只读入了source端</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">"rb"</span>) <span class="keyword">as</span> fd:</span><br><span class="line">        inputs = [line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> fd]</span><br><span class="line"></span><br><span class="line">    input_lens = [</span><br><span class="line">        (i, len(line.split())) <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(inputs)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对source端的句子按长度从大到小进行排序</span></span><br><span class="line">    sorted_input_lens = sorted(input_lens, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>],</span><br><span class="line">                               reverse=reverse)</span><br><span class="line">    sorted_keys = &#123;&#125;</span><br><span class="line">    sorted_inputs = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, (idx, _) <span class="keyword">in</span> enumerate(sorted_input_lens):</span><br><span class="line">        sorted_inputs.append(inputs[idx])</span><br><span class="line">        sorted_keys[idx] = i</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sorted_keys, sorted_inputs</span><br></pre></td></tr></table></figure><p>然后对数据继续进行处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">infer_input_fn</span><span class="params">()</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">        tf.constant(sorted_data))</span><br><span class="line">    dataset = dataset.shard(torch.distributed.get_world_size(),</span><br><span class="line">                            torch.distributed.get_rank())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将source端句子进行分词，在后面加上&lt;eos&gt;</span></span><br><span class="line">    dataset = dataset.map(</span><br><span class="line">        <span class="keyword">lambda</span> x: tf.strings.split([x]).values,</span><br><span class="line">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line">    dataset = dataset.map(</span><br><span class="line">        <span class="keyword">lambda</span> x: tf.concat([x, [tf.constant(params.eos)]], axis=<span class="number">0</span>),</span><br><span class="line">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line">    dataset = dataset.map(</span><br><span class="line">        <span class="keyword">lambda</span> x: &#123;</span><br><span class="line">            <span class="string">"source"</span>: x,</span><br><span class="line">            <span class="string">"source_length"</span>: tf.shape(x)[<span class="number">0</span>]</span><br><span class="line">        &#125;,</span><br><span class="line">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行padding</span></span><br><span class="line">    dataset = dataset.padded_batch(</span><br><span class="line">        params.decode_batch_size,</span><br><span class="line">        padded_shapes=&#123;</span><br><span class="line">            <span class="string">"source"</span>: tf.TensorShape([<span class="keyword">None</span>]),</span><br><span class="line">            <span class="string">"source_length"</span>: tf.TensorShape([])</span><br><span class="line">        &#125;,</span><br><span class="line">        padding_values=&#123;</span><br><span class="line">            <span class="string">"source"</span>: tf.constant(params.pad),</span><br><span class="line">            <span class="string">"source_length"</span>: <span class="number">0</span></span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加source_mask</span></span><br><span class="line">    dataset = dataset.map(</span><br><span class="line">        <span class="keyword">lambda</span> x: &#123;</span><br><span class="line">            <span class="string">"source"</span>: x[<span class="string">"source"</span>],</span><br><span class="line">            <span class="string">"source_mask"</span>: tf.sequence_mask(x[<span class="string">"source_length"</span>],</span><br><span class="line">                                            tf.shape(x[<span class="string">"source"</span>])[<span class="number">1</span>],</span><br><span class="line">                                            tf.float32),</span><br><span class="line">        &#125;,</span><br><span class="line">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sorted_key, dataset</span><br></pre></td></tr></table></figure><p>其中<code>sorted_key</code>的主要作用是在翻译完之后把译文按照原来的次序重新排列。那么为啥要把source端排序呢，这主要是为了分batch的方便，在evaluation阶段每个batch包含的句子数量是相同的，和train阶段不同。（不过为什么要这样设计呢……？）很明显的是，此处把target端相关的东西全都拿走了，因为实际上执行的是一个翻译的过程，没有参考的target。</p><p>接下来就是<a href="https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/bin/trainer.py" target="_blank" rel="noopener">trainer.py</a>中的<code>load_reference</code>函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_references</span><span class="params">(pattern)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pattern:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    files = glob.glob(pattern)</span><br><span class="line">    references = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> files:</span><br><span class="line">        ref = []</span><br><span class="line">        <span class="keyword">with</span> open(name, <span class="string">"rb"</span>) <span class="keyword">as</span> fd:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> fd:</span><br><span class="line">                items = line.strip().split()</span><br><span class="line">                ref.append(items)</span><br><span class="line">        references.append(ref)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> list(zip(*references))</span><br></pre></td></tr></table></figure><p>可以看出这个函数的主要功能就是把所有的reference都加载进来，然后分词，准备在翻译完后与译文进行比较。</p><h2>测试集数据输入</h2><p>测试集数据输入的入口在<a href="https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/bin/translator.py" target="_blank" rel="noopener">translator.py</a>，和验证集一样，也调用了<code>get_dataset</code>，只不过这次没有reference：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> len(args.input) == <span class="number">1</span>:</span><br><span class="line">    mode = <span class="string">"infer"</span></span><br><span class="line">    sorted_key, dataset = data.get_dataset(</span><br><span class="line">        args.input[<span class="number">0</span>], mode, params)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># Teacher-forcing</span></span><br><span class="line">    mode = <span class="string">"eval"</span></span><br><span class="line">    dataset = data.get_dataset(args.input, mode, params)</span><br><span class="line">    sorted_key = <span class="keyword">None</span></span><br></pre></td></tr></table></figure><p>接下来的部分和验证集用的是同一个函数，就不细讲了：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">infer_input_fn</span><span class="params">()</span>:</span></span><br><span class="line">    sorted_key, sorted_data = sort_input_file(filenames)</span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">        tf.constant(sorted_data))</span><br><span class="line">    dataset = dataset.shard(torch.distributed.get_world_size(),</span><br><span class="line">                            torch.distributed.get_rank())</span><br><span class="line"></span><br><span class="line">    dataset = dataset.map(</span><br><span class="line">        <span class="keyword">lambda</span> x: tf.strings.split([x]).values,</span><br><span class="line">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line">    dataset = dataset.map(</span><br><span class="line">        <span class="keyword">lambda</span> x: tf.concat([x, [tf.constant(params.eos)]], axis=<span class="number">0</span>),</span><br><span class="line">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line">    dataset = dataset.map(</span><br><span class="line">        <span class="keyword">lambda</span> x: &#123;</span><br><span class="line">            <span class="string">"source"</span>: x,</span><br><span class="line">            <span class="string">"source_length"</span>: tf.shape(x)[<span class="number">0</span>]</span><br><span class="line">        &#125;,</span><br><span class="line">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">    dataset = dataset.padded_batch(</span><br><span class="line">        params.decode_batch_size,</span><br><span class="line">        padded_shapes=&#123;</span><br><span class="line">            <span class="string">"source"</span>: tf.TensorShape([<span class="keyword">None</span>]),</span><br><span class="line">            <span class="string">"source_length"</span>: tf.TensorShape([])</span><br><span class="line">        &#125;,</span><br><span class="line">        padding_values=&#123;</span><br><span class="line">            <span class="string">"source"</span>: tf.constant(params.pad),</span><br><span class="line">            <span class="string">"source_length"</span>: <span class="number">0</span></span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">    dataset = dataset.map(</span><br><span class="line">        <span class="keyword">lambda</span> x: &#123;</span><br><span class="line">            <span class="string">"source"</span>: x[<span class="string">"source"</span>],</span><br><span class="line">            <span class="string">"source_mask"</span>: tf.sequence_mask(x[<span class="string">"source_length"</span>],</span><br><span class="line">                                            tf.shape(x[<span class="string">"source"</span>])[<span class="number">1</span>],</span><br><span class="line">                                            tf.float32),</span><br><span class="line">        &#125;,</span><br><span class="line">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sorted_key, dataset</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/post/thumt-code-summary-1&quot;&gt;简介篇地址&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="https://zhanghuimeng.github.io/categories/NLP/"/>
    
    
      <category term="THUMT" scheme="https://zhanghuimeng.github.io/tags/THUMT/"/>
    
  </entry>
  
  <entry>
    <title>2020-08-29-THUMT代码详解（1）：简介</title>
    <link href="https://zhanghuimeng.github.io/post/thumt-code-summary-1/"/>
    <id>https://zhanghuimeng.github.io/post/thumt-code-summary-1/</id>
    <published>2020-08-29T16:40:43.000Z</published>
    <updated>2020-08-29T16:40:43.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/THUNLP-MT/THUMT" target="_blank" rel="noopener">THUMT</a>是清华大学自然语言处理实验室开发的一款开源神经机器翻译工具箱，使用Theano、Tensorflow和PyTorch三种语言实现了Transformer、Seq2Seq和RNNSearch等模型。本文将分析<a href="https://github.com/thumt/THUMT/tree/pytorch" target="_blank" rel="noopener">THUMT-PyTorch</a>的实现。代码的运行方法参见<a href="https://github.com/zhanghuimeng/THUMT/blob/struc/docs/index.md" target="_blank" rel="noopener">THUMT Documentation</a>。</p><a id="more"></a><p>后续文章列表：</p><ul><li><a href="/post/thumt-code-summary-2">THUMT代码详解（2）：数据处理</a></li><li><a href="/post/thumt-code-summary-3">THUMT代码详解（3）：训练阶段模型和数据流</a></li><li><a href="/post/thumt-code-summary-4">THUMT代码详解（4）：infer阶段模型和数据流</a></li></ul><p>在读文章之前，默认你已经了解Transformer的基本知识了。</p><h2>文件列表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">|- bin/</span><br><span class="line">|    |- scorer.py</span><br><span class="line">|    |- trainer.py</span><br><span class="line">|    |- translator.py</span><br><span class="line">|- data/</span><br><span class="line">|    |- dataset.py</span><br><span class="line">|    |- vocab.py</span><br><span class="line">|- models/</span><br><span class="line">|    |- transformer.py</span><br><span class="line">|- modules/</span><br><span class="line">|    |- affine.py</span><br><span class="line">|    |- attention.py</span><br><span class="line">|    |- embedding.py</span><br><span class="line">|    |- feed_forward.py</span><br><span class="line">|    |- layer_norm.py</span><br><span class="line">|    |- losses.py</span><br><span class="line">|    |- module.py</span><br><span class="line">|    |- recurrent.py</span><br><span class="line">|- optimizers/</span><br><span class="line">|    |- clipping.py</span><br><span class="line">|    |- optimizers.py</span><br><span class="line">|    |- schedules.py</span><br><span class="line">|- scripts/</span><br><span class="line">|    |- average_checkpoints.py</span><br><span class="line">|    |- build_vocab.py</span><br><span class="line">|    |- convert_checkpoint.py</span><br><span class="line">|    |- shuffle_corpus.py</span><br><span class="line">|- utils/</span><br><span class="line">|    |- bleu.py</span><br><span class="line">|    |- bpe.py</span><br><span class="line">|    |- checkpoint.py</span><br><span class="line">|    |- evaluation.py</span><br><span class="line">|    |- hparams.py</span><br><span class="line">|    |- inference.py</span><br><span class="line">|    |- misc.py</span><br><span class="line">|    |- nest.py</span><br><span class="line">|    |- scope.py</span><br><span class="line">|    |- summary.py</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/THUNLP-MT/THUMT&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;THUMT&lt;/a&gt;是清华大学自然语言处理实验室开发的一款开源神经机器翻译工具箱，使用Theano、Tensorflow和PyTorch三种语言实现了Transformer、Seq2Seq和RNNSearch等模型。本文将分析&lt;a href=&quot;https://github.com/thumt/THUMT/tree/pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;THUMT-PyTorch&lt;/a&gt;的实现。代码的运行方法参见&lt;a href=&quot;https://github.com/zhanghuimeng/THUMT/blob/struc/docs/index.md&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;THUMT Documentation&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="https://zhanghuimeng.github.io/categories/NLP/"/>
    
    
      <category term="THUMT" scheme="https://zhanghuimeng.github.io/tags/THUMT/"/>
    
  </entry>
  
  <entry>
    <title>日记</title>
    <link href="https://zhanghuimeng.github.io/post/2020-08-24-diary/"/>
    <id>https://zhanghuimeng.github.io/post/2020-08-24-diary/</id>
    <published>2020-08-24T15:18:06.000Z</published>
    <updated>2020-08-24T15:18:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>之前那篇日记太丧了，老是摆在那里也不好，不如来写点最近的感受。</p><a id="more"></a><p>最近终于返校了，感觉在实验室里效率比在家高很多。</p><p>最近一段时间总是腰疼，到医院一查，发现腰椎间盘突出了。于是最近在打针和做牵引治疗，这几天好多了。可能我太胖了而且坐得太久了吧。总之以后要注意了。</p><p>我还是想把阅读PRML并写笔记的事情捡起来的，不过因为数学公式太多了，缺乏一个比较好的预览和引用的环境，所以暂时在搁置。当然，重点还是科研工作。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前那篇日记太丧了，老是摆在那里也不好，不如来写点最近的感受。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="https://zhanghuimeng.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>日记</title>
    <link href="https://zhanghuimeng.github.io/post/2020-06-11-diary/"/>
    <id>https://zhanghuimeng.github.io/post/2020-06-11-diary/</id>
    <published>2020-06-11T00:49:01.000Z</published>
    <updated>2020-06-11T00:49:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>不怎么想玩博客了。</p><a id="more"></a><p>最近一两个月发生了很多事，最大的事就是我的某一门课可能要挂科重修了，关键是这门课还特别难，我要付出很大的努力才能不挂科。</p><p>大概会非常耽误研究工作吧……我还想好好读一遍PRML呢……</p><p>我觉得很迷茫，很孤单，不知道自己在做什么，不知道自己想做什么，没有什么感兴趣的东西。这是双相导致的吗？</p><p>我真的很迷茫。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不怎么想玩博客了。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="https://zhanghuimeng.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>多项分布混合模型的EM算法</title>
    <link href="https://zhanghuimeng.github.io/post/em-for-mixture-of-multinomials/"/>
    <id>https://zhanghuimeng.github.io/post/em-for-mixture-of-multinomials/</id>
    <published>2020-04-16T20:31:25.000Z</published>
    <updated>2020-04-16T20:31:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍如何用EM算法求解多项分布混合模型（Mixture of Multinomials）。</p><a id="more"></a><h2>EM算法的基本步骤</h2><p>给定观测变量$\boldsymbol{X}$和隐变量$\boldsymbol{Z}$上的一个联合概率分布$p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})$，由参数$\boldsymbol{\theta}$控制，目标是关于$\boldsymbol{\theta}$最大化似然函数$p(\boldsymbol{X} | \boldsymbol{\theta})$：</p><ul><li>初始化参数$\boldsymbol{\theta}$，记为$\boldsymbol{\theta}^{\text{old}}$</li><li>E步骤：计算$\boldsymbol{Z}$在$\boldsymbol{\theta}^{\text{old}}$下的后验分布$p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\theta}^{\text{old}})$</li><li>M步骤：计算$\boldsymbol{\theta}^{\text{new}}$<ul><li>$\mathcal{Q}\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}}\right)=\sum_{\boldsymbol{Z}} p\left(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\theta}^{\text{old}}\right) \ln p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})$（对数似然的期望）</li><li>$\boldsymbol{\theta}^{\text{new}}=\underset{\boldsymbol{\theta}}{\arg \max } \mathcal{Q}\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}}\right)$（最大化期望）</li></ul></li><li>检查对数似然函数或者参数值的收敛性。如果不满足收敛准则，则令$\boldsymbol{\theta}^{\text{old}} = \boldsymbol{\theta}^{\text{new}}$，回到E步骤</li></ul><h2>多项混合模型</h2><p>假定我们共有$T$个文档，词表大小为$W$，$T_{dw}$表示文档$d$中词$w$出现的次数。每个文档都有一个主题$c_d$，主题共有$K$种，满足分布</p><p>$$<br>P(c_d = k) = \pi_k, , k = 1, 2, ..., K<br>$$</p><p>每个主题下的词汇都满足多项分布$Mult(\boldsymbol{\mu} _ k)$，其中$\boldsymbol{\mu} _ k = (\mu_{k1}, \cdots, \mu _ {kW})$。因此文档$d$的主题为$k$时，词汇分布的概率为</p><p>$$<br>P(d | c_d = k) = \frac{n_d!}{\prod_{w=1}^W T_{dw}!} \prod_{w=1}^W \mu_{kw}^{T_{dw}}, , n_d = \sum_{w=1}^W T_{dw}<br>$$</p><p>因此文档$d$总的概率分布为</p>$$\begin{aligned}P(d) &= \sum_{k=1}^K P(d|c_d=k)P(c_d=k) \\&= \frac{n_d!}{\prod_{w=1}^W T_{dw}!} \sum_{k=1}^K \pi_k \prod_{w=1}^W \mu_{kw}^{T_{dw}}\end{aligned}$$<h2>用EM求解多项混合模型</h2><h3>E步骤</h3><p>此时$d$相当于观测变量，$c_d$相当于隐变量。首先通过贝叶斯公式求解$P(c_d | d)$。</p>$$\begin{aligned}P(d, c_d = k) &= P(d | c_d = k) P(c_d = k) \\&= \frac{n_d!}{\prod_{w=1}^W T_{dw}!} \pi_k \prod_{w=1}^W \mu_{kw}^{T_{dw}}\end{aligned}$$$$\begin{aligned}P(c_d = k | d) &= \frac{P(d, c_d = k)}{P(d)} \\&= \frac{\frac{n_d!}{\prod_{w=1}^W T_{dw}!} \pi_k \prod_{w=1}^W \mu_{kw}^{T_{dw}}}{\frac{n_d!}{\prod_{w=1}^W T_{dw}!} \sum_{j=1}^K \pi_j \prod_{w=1}^W \mu_{jw}^{T_{dw}}} \\&= \frac{\pi_k \prod_{w=1}^W \mu_{kw}^{T_{dw}}}{\sum_{j=1}^K \pi_j \prod_{w=1}^W \mu_{jw}^{T_{dw}}}\end{aligned}$$<p>记$\gamma_{dk} = P(c_d = k | d)$，在接下来的M步骤里，这一项是固定的。</p><h3>M步骤</h3><p>接下来求解对数似然的期望。首先，单个文档的对数似然为（省略常数项，令$\boldsymbol{D}$表示文档集合，$\boldsymbol{C}$表示主题集合）：</p>$$\log{P(d, c_d)} = \log{\pi_{c_d}} + \sum_{w=1}^W T_{dw} \log{\mu_{c_d w}}$$$$\log{P(\boldsymbol{D}, \boldsymbol{C})} = \log{\prod_{d=1}^D} P(d, c_d) = \sum_{d=1}^D \log{P(d, c_d)}$$<p>对数似然的期望为</p>$$\begin{aligned}\mathbb{E}_{\boldsymbol{C}} [ \log{P(\boldsymbol{D}, \boldsymbol{C})} ] &= \sum_{d=1}^D \mathbb{E}_{\boldsymbol{C}} [\log{P(d, c_d)}] \\&= \sum_{d=1}^D \sum_{k=1}^K \log{P(d, c_d=k)} P(c_d = k | d) \\&= \sum_{d=1}^D \sum_{k=1}^K \gamma_{dk} \left[\log{\pi_{k}} + \sum_{w=1}^W T_{dw} \log{\mu_{kw}}\right]\end{aligned}$$<p>接下来需要对$\boldsymbol{\pi}$和$\boldsymbol{\mu}$最大化$\mathbb{E}_{\boldsymbol{C}} [ \log{P(\boldsymbol{D}, \boldsymbol{C})} ]$。由于有限制条件</p>$$\begin{aligned}\sum_{k=1}^K \pi_k &= 1 \\\sum_{w=1}^W \mu_{kw} &= 1, \, k=1, \cdots, K\end{aligned}$$<p>因此需要用拉格朗日乘数法进行优化。令</p>$$\begin{aligned}L &= \mathbb{E}_{\boldsymbol{C}} [ \log{P(\boldsymbol{D}, \boldsymbol{C})} ] - \lambda_0 \left(\sum_{k=1}^K \pi_k - 1\right) - \sum_{k=1}^K \lambda_k \left(\sum_{w=1}^W \mu_{kw} - 1\right)\end{aligned}$$<p>由于</p>$$\begin{aligned}\frac{\partial L}{\partial \pi_k} = \frac{\sum_{d=1}^D \gamma_{dk}}{\pi_k} - \lambda_0 &= 0 \\\sum_{k=1}^K \pi_k &= 1\end{aligned}$$<p>可以求得</p>$$\begin{aligned}\lambda_0 &= \sum_{d=1}^D \sum_{k=1}^K \gamma_{dk} \\\pi_k &= \frac{\sum_{d=1}^D \gamma_{dk}}{\sum_{d=1}^D \sum_{k=1}^K \gamma_{dk}}\end{aligned}$$<p>由于</p>$$\begin{aligned}\frac{\partial L}{\partial \mu_{kw}} = \frac{\sum_{d=1}^D \gamma_{dk} T_{dw}}{\mu_{kw}} - \lambda_k &= 0 \\\sum_{w=1}^W \mu_{kw} &= 1\end{aligned}$$<p>可以求得</p>$$\begin{aligned}\lambda_k &= \sum_{d=1}^D \sum_{w=1}^W \gamma_{dk} T_{dw} \\\mu_{kw} &= \frac{\sum_{d=1}^D \gamma_{dk} T_{dw}}{\sum_{d=1}^D \sum_{w=1}^W \gamma_{dk} T_{dw}}\end{aligned}$$<p>综上，M步骤更新参数的方式为</p>$$\begin{aligned}\pi_k &= \frac{\sum_{d=1}^D \gamma_{dk}}{\sum_{d=1}^D \sum_{k=1}^K \gamma_{dk}} \\\mu_{kw} &= \frac{\sum_{d=1}^D \gamma_{dk} T_{dw}}{\sum_{d=1}^D \sum_{w=1}^W \gamma_{dk} T_{dw}}\end{aligned}$$<h2>代码</h2><p>用EM算法在<a href="http://ml.cs.tsinghua.edu.cn/~shuyu/sml/20news.zip" target="_blank" rel="noopener">20 Newsgroups dataset</a>上求解。</p><p>算法链接：<a href="https://github.com/zhanghuimeng/multinomial-mixture-em" target="_blank" rel="noopener">multinomial-mixture-em</a></p><p>代码中有一些需要注意的地方：</p><ul><li>由于不完整数据集的最大似然函数难以计算，所以就没有去求它，但如果求了，应该可以看出它的值在不断增大</li><li>在求解过程中，$\gamma_{dk} = \frac{\pi_k \prod_{w=1}^W \mu_{kw}^{T_{dw}}}{\sum_{j=1}^K \pi_j \prod_{w=1}^W \mu_{jw}^{T_{dw}}}$可能会下溢，可以先对分子取log，同时减去最大值之后再做exp，最后再对整个$\gamma_{d, \cdot}$取平均值</li><li>在求解过程中，$\mu_{kw}$可能会下溢到0，可以添加一个很小的数来规避</li></ul><h2>参考文献</h2><ul><li>PRML第9章</li><li><a href="https://www.cs.princeton.edu/courses/archive/spring12/cos424/pdf/em-mixtures.pdf" target="_blank" rel="noopener">Mixture Models and Expectation-Maximization</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍如何用EM算法求解多项分布混合模型（Mixture of Multinomials）。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://zhanghuimeng.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>PRML读书笔记：9.3 EM的另一种观点</title>
    <link href="https://zhanghuimeng.github.io/post/prml-chap-9-3-an-alternate-view-of-em/"/>
    <id>https://zhanghuimeng.github.io/post/prml-chap-9-3-an-alternate-view-of-em/</id>
    <published>2020-04-12T00:58:10.000Z</published>
    <updated>2020-04-12T00:58:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>本节中，我们介绍另一种看待EM算法的角度，其中隐变量起到重要的作用。</p><a id="more"></a><h3>EM算法即计算后验概率+期望最大化</h3><p>将所有观测数据的集合记为$\boldsymbol{X}$，所有隐变量的集合记为$\boldsymbol{Z}$，所有模型参数的集合记为$\boldsymbol{\theta}$，则对数似然函数为</p>$$\ln p(\boldsymbol{X} | \boldsymbol{\theta})=\ln \left\{\sum_{\boldsymbol{Z}} p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})\right\}$$<p>EM算法的目标即找到使这个对数似然函数取最大值的解。</p><p>问题在于，对隐变量的求和位于对数内部，这一求和式使得对数运算不能直接作用于联合概率分布，使得最大似然解的形式变得复杂。</p><p>因此我们不妨假定，对于$\boldsymbol{X}$，它的隐变量$\boldsymbol{Z}$是已知的，称$\{\boldsymbol{X}, \boldsymbol{Z}\}$为完整（complete）数据集。完整数据集的对数似然函数的形式为$\ln p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})$，当它为指数族分布函数时，对这个完整数据集的对数似然函数进行最大化是很容易的。</p><p>但是我们实际上没有完整数据集，我们对$\boldsymbol{Z}$的取值的知识仅仅来源于后验概率分布$p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\theta})$。因此我们转而考虑在隐变量的后验分布下完整数据的对数似然函数的期望值。（由于我们不知道隐变量的具体值，只知道一个分布，因此求期望是一个好的选择。在9.4节中将具体介绍使用期望的原因。）这对应于EM算法的E步骤。在接下来的M步骤中，我们最大化这个期望（对参数$\boldsymbol{\theta}$最大化），得到一个修正的对$\boldsymbol{\theta}$的估计。（从直觉上来说，取定后验分布之后再最大化数据对数似然的期望值，相当于使参数和数据更加接近了。）</p><p>一般的EM算法总结如下。每个EM循环都会增大不完整数据集的对数似然函数（除非已经达到局部极大值）（9.4节中将会证明）。</p><p>给定观测变量$\boldsymbol{X}$和隐变量$\boldsymbol{Z}$上的一个联合概率分布$p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})$，由参数$\boldsymbol{\theta}$控制，目标是关于$\boldsymbol{\theta}$最大化似然函数$p(\boldsymbol{X} | \boldsymbol{\theta})$：</p><ul><li>初始化参数$\boldsymbol{\theta}$，记为$\boldsymbol{\theta}^{\text{old}}$</li><li>E步骤：计算$\boldsymbol{Z}$在$\boldsymbol{\theta}^{\text{old}}$下的后验分布$p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\theta}^{\text{old}})$</li><li>M步骤：计算$\boldsymbol{\theta}^{\text{new}}$<ul><li>$\mathcal{Q}\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}}\right)=\sum_{\boldsymbol{Z}} p\left(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\theta}^{\text{old}}\right) \ln p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})$（对数似然的期望）</li><li>$\boldsymbol{\theta}^{\text{new}}=\underset{\boldsymbol{\theta}}{\arg \max } \mathcal{Q}\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}}\right)$（最大化期望）</li></ul></li><li>检查对数似然函数或者参数值的收敛性。如果不满足收敛准则，则令$\boldsymbol{\theta}^{\text{old}} = \boldsymbol{\theta}^{\text{new}}$，回到E步骤</li></ul><h3>EM算法的其他用途</h3><ul><li>寻找模型的MAP（最大后验概率）解</li><li>未观测变量对应于数据集中缺失值</li><li>数据集随机缺失</li></ul><h2>9.3.1 重新考察高斯混合模型</h2><p>我们考虑将EM算法的隐变量观点应用于一个具体的例子，即高斯混合模型。</p><p>由于$\boldsymbol{z}$是one-hot变量，因此概率分布可以写作</p>$$\begin{aligned}p(\boldsymbol{x}, \boldsymbol{z}) &= p(\boldsymbol{x} | \boldsymbol{z}) p(\boldsymbol{z}) \\&= \prod_{k=1}^{K} \mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)^{z_{k}} \prod_{k=1}^{K}\pi_k^{z_k} \\&= \prod_{k=1}^{K} \pi_k^{z_k} \mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)^{z_{k}}\end{aligned}$$<p>因此似然函数的形式为</p>$$p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})=\prod_{n=1}^{N} \prod_{k=1}^{K} \pi_{k}^{z_{n k}} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}^{k}\right)^{z_{n k}}$$<p>取对数，有</p>$$\ln p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})=\sum_{n=1}^{N} \sum_{k=1}^{K} z_{n k}\left\{\ln \pi_{k}+\ln \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right\}$$<p>为了进行E步骤，求解后验概率分布$p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})$，由贝叶斯定理得到</p>$$\begin{aligned}p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}) &= \frac{p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})}{p(\boldsymbol{X} | \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})}\\&\propto p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}) \\&= \prod_{n=1}^{N} \prod_{k=1}^{K} \pi_{k}^{z_{n k}} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}^{k}\right)^{z_{n k}}\end{aligned}$$<p>因此后验概率分布可以在$n$上进行分解，从而$\boldsymbol{z}<em>n$是独立的，$z</em>{nk}$也是独立的，有</p><p>$$<br>p(z_{nk}) \propto \left[ \pi_{k} \mathcal{N}\left(\boldsymbol{x}<em>{n} | \boldsymbol{\mu}</em>{k}, \boldsymbol{\Sigma}^{k}\right) \right]^{z_{nk}}<br>$$</p><p>因此可以计算出$z_{nk}$的期望（用于计算对数似然的期望）：</p>$$\begin{aligned}\mathbb{E}\left[z_{n k}\right] &=\frac{\sum_{z_{n}} z_{n k} \prod_{k^{\prime}}\left[\pi_{k^{\prime}} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k^{\prime}}, \boldsymbol{\Sigma}_{k^{\prime}}\right)\right]^{z_{n k^{\prime}}}}{\sum_{z_{n}} \prod_{j}\left[\pi_{j} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)\right]^{z_{n j}}} \\&=\frac{\pi_{k} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)}\\&=\gamma\left(z_{n k}\right)\end{aligned}$$<p>于是完整数据的对数似然函数的期望值为</p>$$\begin{aligned}& \mathbb{E}_{\boldsymbol{Z}}[\ln p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})] \\=& \mathbb{E}_{\boldsymbol{Z}}\left[\sum_{n=1}^{N} \sum_{k=1}^{K} z_{n k}\left\{\ln \pi_{k}+\ln \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right\}\right] \\=& \sum_{n=1}^{N} \sum_{k=1}^{K} \sum_{z_{nk}} z_{n k}\left\{\ln \pi_{k}+\ln \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right\} p(z_{nk}) \\=& \sum_{n=1}^{N} \sum_{k=1}^{K} \mathbb{E}_{z_{nk}}[z_{n k}]\left\{\ln \pi_{k}+\ln \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right\} \\=& \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right)\left\{\ln \pi_{k}+\ln \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right\}\end{aligned}$$<p>接下来保持$\gamma\left(z_{n k}\right)$固定，关于$\boldsymbol{\mu}<em>{k}$、$\boldsymbol{\Sigma}</em>{k}$和$\pi_k$最大化上式，方法与<a href="(/post/prml-chap-9-2-mixtures-of-gaussians/)">9.2节</a>中基本相同。</p><p>将$\mathbb{E}_{\boldsymbol{Z}}[\ln p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})]$对$\boldsymbol{\mu}_k$求导，得到</p>$$\sum_{n=1}^{N} \gamma\left(z_{n k}\right) \boldsymbol{\Sigma}_{k}^{-1} (\boldsymbol{x}_{n} - \boldsymbol{\mu}_k) = 0$$<p>求解得到</p>$$\boldsymbol{\mu}_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right) \boldsymbol{x}_{n}$$<p>将$\mathbb{E}<em>{\boldsymbol{Z}}[\ln p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})]$对$\boldsymbol{\Sigma}</em>{k}$求导，得到</p>$$\sum_{n=1}^{N} \gamma\left(z_{n k}\right) \frac{1}{2} \left[\boldsymbol{\Sigma}_{k}^{-1} (\boldsymbol{x}_{n} - \boldsymbol{\mu}_k) (\boldsymbol{x}_{n} - \boldsymbol{\mu}_k)^\top\boldsymbol{\Sigma}_{k}^{-1} - \boldsymbol{\Sigma}_{k}^{-1} \right] = 0$$<p>求解得到</p>$$\boldsymbol{\Sigma}_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right)\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{x}-\boldsymbol{\mu}_{k}\right)^{T}$$<p>考虑到$\sum_{k=1}^K \pi_k = 1$，用拉格朗日乘数法对$\pi_k$求解，得到</p><p>$$<br>\pi_{k}=\frac{N_{k}}{N}<br>$$</p><p>上述推导结果与之前完全相同。</p><h2>9.3.2 与K-means的关系</h2><p>K-means算法与高斯混合模型的EM算法有很强的相似性。K-means算法对数据点的聚类进行了“硬”分配，而EM算法基于后验概率分布进行了“软”分配。我们可以将K-means算法看成是高斯混合模型的EM算法的一个特殊的极限情况。</p><p>考虑一个高斯混合模型，混合分量的协方差矩阵是$\epsilon \boldsymbol{I}$，$\epsilon$是一个被所有分量共享的固定的方差参数，$\boldsymbol{I}$是单位矩阵，则有</p>$$\begin{aligned}p(\boldsymbol{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) &= \frac{1}{(2\pi)^{\frac{D}{2}} |\boldsymbol{\Sigma}_k|^{\frac{1}{2}}} \exp{\left[ -\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu}_k)^T \boldsymbol{\Sigma}_k^{-1} (\boldsymbol{x}-\boldsymbol{\mu}_k) \right]} \\&= \frac{1}{(2\pi\epsilon)^{\frac{D}{2}}} \exp{\left[ -\frac{1}{2\epsilon} \left\|\boldsymbol{x}-\boldsymbol{\mu}_k\right\|^2 \right]}\end{aligned}$$<p>则</p>$$\begin{aligned}\gamma(z_{nk}) &=\frac{p\left(z_{k}=1\right) p\left(\boldsymbol{x} | z_{k}=1\right)}{\sum_{j=1}^{K} p\left(z_{j}=1\right) p\left(\boldsymbol{x} | z_{j}=1\right)} \\&=\frac{\pi_{k} \frac{1}{(2\pi\epsilon)^{\frac{D}{2}}} \exp{\left[ -\frac{1}{2\epsilon} \left\|\boldsymbol{x}-\boldsymbol{\mu}_k\right\|^2 \right]}}{\sum_{j=1}^{K} \pi_{j} \frac{1}{(2\pi\epsilon)^{\frac{D}{2}}} \exp{\left[ -\frac{1}{2\epsilon} \left\|\boldsymbol{x}-\boldsymbol{\mu}_j\right\|^2 \right]}} \\&=\frac{\pi_{k} \exp{\left[ -\frac{1}{2\epsilon} \left\|\boldsymbol{x}-\boldsymbol{\mu}_k\right\|^2 \right]}}{\sum_{j=1}^{K} \pi_{j}  \exp{\left[ -\frac{1}{2\epsilon} \left\|\boldsymbol{x}-\boldsymbol{\mu}_j\right\|^2 \right]}}\end{aligned}$$<p>当$\epsilon \rightarrow 0$时，$\gamma(z_{nk}) \rightarrow r_{nk}$，因此，每个数据点都被分配为距离最近的均值的聚类。</p><h2>9.3.3 伯努利分布的混合</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本节中，我们介绍另一种看待EM算法的角度，其中隐变量起到重要的作用。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://zhanghuimeng.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="https://zhanghuimeng.github.io/tags/Machine-Learning/"/>
    
      <category term="PRML" scheme="https://zhanghuimeng.github.io/tags/PRML/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning读书笔记：6.2 基于梯度的学习</title>
    <link href="https://zhanghuimeng.github.io/post/deep-learning-chap-6-2-gradient-based-learning/"/>
    <id>https://zhanghuimeng.github.io/post/deep-learning-chap-6-2-gradient-based-learning/</id>
    <published>2020-04-09T23:14:28.000Z</published>
    <updated>2020-04-09T23:14:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>设计和训练神经网络需要指定代价函数和输出单元。</p><a id="more"></a><h2>6.2.1 代价函数</h2><p>大多数情况下，参数模型定义了分布$p(\boldsymbol{y} | \boldsymbol{x}; \boldsymbol{\theta})$，我们直接使用最大似然原理，即使用训练数据和模型预测之间的交叉熵作为代价函数。</p><p>有时我们不预测$\boldsymbol{y}$的完整概率分布，而仅预测在给定$\boldsymbol{x}$下$\boldsymbol{y}$的某种统计量。</p><p>完整的代价函数通常还要结合正则项。通常使用权重衰减方法。</p><h3>6.2.1.1 使用最大似然学习条件分布</h3><p>代价函数通常是负的对数似然，它和训练数据与模型分布之间的交叉熵等价，表示为</p>$$J(\boldsymbol{\theta})=-\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim \hat{p}_{\mathrm{data}}} \log p_{\mathrm{model}}(\boldsymbol{y} | \boldsymbol{x})$$<p>这种方法的优点是减轻了为每个模型设计代价函数的负担。</p><p>代价函数的梯度必须足够大，负对数似然的对数函数消除了某些输出单元的指数效果，可以避免输出单元饱和，梯度变小的问题。</p><p>交叉熵代价函数通常没有最小值，需要正则化技术进行修正。</p><h3>6.2.1.2 学习条件统计量</h3><p>有时我们并不是想学习一个完整的概率分布$p(\boldsymbol{y} | \boldsymbol{x}; \boldsymbol{\theta})$，而仅仅是想学习在给定$\boldsymbol{x}$时$\boldsymbol{y}$的某个条件统计量。从这个角度来看，可以把代价函数看成是一个<strong>泛函</strong>（functional），即函数到实数的映射。我们可以设计一个代价泛函，使得它在我们想要的某些特殊函数处取得最小值。对函数求解优化问题需要<strong>变分法</strong>（caculus of variations）。</p><p>使用变分法可以导出以下两个结果。</p><p>求解优化问题</p>$$f^{*}=\underset{f}{\arg \min } \mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p_{\mathrm{data}}}\|\boldsymbol{y}-f(\boldsymbol{x})\|^{2}$$<p>得到</p>$$f^{*}(\boldsymbol{x})=\mathbb{E}_{\mathbf{y} \sim p_{\mathrm{data}}(\boldsymbol{y} | \boldsymbol{x})}[\boldsymbol{y}]$$<p>即最小化均方误差代价函数将得到一个函数，它可以用来对每个$\boldsymbol{x}$预测出$\boldsymbol{y}$的均值。</p><p>求解</p>$$f^{*}=\underset{f}{\arg \min } \mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p_{\text {data }}}\|\boldsymbol{y}-f(\boldsymbol{x})\|_{1}$$<p>将得到一个可以对每个$\boldsymbol{x}$预测$\boldsymbol{y}$取值的中位数的函数。这个代价函数通常被称为<strong>平均绝对误差</strong>（mean absolute error）。</p><p>但是，均方误差和平均绝对误差在使用基于梯度的优化方法时效果并不好，所以交叉熵代价函数比它们更受欢迎。</p><h2>6.2.2 输出单元</h2><p>代价函数的选择与输出单元的选择紧密相关。选择如何输出决定了交叉熵函数的形式。</p><p>本节中，假设前馈网络提供了一组定义为$\boldsymbol{h} = f(\boldsymbol{x}; \boldsymbol{\theta})$的隐藏特征。输出层的作用是对这些特征进行额外变换。</p><h3>6.2.2.1 用于高斯输出分布的线性单元</h3><p>给定特征$\boldsymbol{h}$，线性输出单元层产生一个向量$\hat{y}=W^{\top} h+b$。</p><p>线性输出层经常用来产生条件高斯分布的均值：</p><p>$$p(\boldsymbol{y} | \boldsymbol{x})=\mathcal{N}(\boldsymbol{y} ; \hat{\boldsymbol{y}}, \boldsymbol{I})$$</p><p>最大化其对数似然此时等价于最小化均方误差。</p><p>由于线性模型不会饱和，所以它们易于采用基于梯度的优化算法。</p><h3>6.2.2.2 用于Bernoulli输出分布的sigmoid单元</h3><p>许多任务需要预测二值型变量$y$的值，此时最大似然的方法是定义$y$在$\boldsymbol{x}$条件下的Bernoulli分布。</p><p>使用sigmoid单元结合最大似然保证模型给出错误答案时，总能有一个较大的梯度。sigmoid输出单元定义为</p><p>$$\hat{y}=\sigma\left(\boldsymbol{w}^{\top} \boldsymbol{h}+b\right)$$</p><p>可以认为sigmoid输出单元具有两个部分：首先使用一个线性层来计算$z = \boldsymbol{w}^{\top} \boldsymbol{h}+b$，然后使用sigmoid激活函数将$z$转化为概率。</p><p>sigmoid激活函数参数化Bernoulli分布时，只会在模型已经得到正确答案时饱和，对于极度不正确的情况则完全不会收缩梯度，这使得基于梯度的模型可以很快地改正错误的$z$。</p><h3>6.2.2.3 用于Multinoulli输出分布的softmax单元</h3><p>当我们想要表示一个具有$n$个可能取值的离散型随机变量的分布时，我们可以使用softmax函数，它可以看做是sigmoid函数的扩展。softmax函数最常用作分类器的输出，表示$n$个不同类上的概率分布。</p><p>softmax函数的形式为</p>$$\boldsymbol{z}=W^{\top} \boldsymbol{h}+\boldsymbol{b}$$$$\operatorname{softmax}(\boldsymbol{z})_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j} \exp \left(z_{j}\right)}$$<p>使用最大化对数似然训练softmax输出目标值$y$时，使用指数函数工作得非常好。</p><h3>6.2.2.4 其他的输出类型</h3><p>// TODO</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;设计和训练神经网络需要指定代价函数和输出单元。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://zhanghuimeng.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="https://zhanghuimeng.github.io/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="https://zhanghuimeng.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning读书笔记：6.1 实例：学习XOR</title>
    <link href="https://zhanghuimeng.github.io/post/deep-learning-chap-6-1-example-learning-xor/"/>
    <id>https://zhanghuimeng.github.io/post/deep-learning-chap-6-1-example-learning-xor/</id>
    <published>2020-04-09T17:20:43.000Z</published>
    <updated>2020-04-09T17:20:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>首先介绍一个简单的前馈网络，它的功能是学习XOR函数。</p><a id="more"></a><p>XOR函数是两个二进制值$x_1$和$x_2$之间的运算，当这两个值中恰好有一个为1时，XOR函数返回值是1，否则为0。</p><p>我们考察网络在这四个点上的表现：$\mathbb{X}=\left\{[0,0]^{\top},[0,1]^{\top},[1,0]^{\top},[1,1]^{\top}\right\}$</p><p>我们可以把这个问题当做是回归问题，并使用均方误差损失函数，这样，评估整个训练集上表现的MSE损失函数为</p><p>$$J(\boldsymbol{\theta})=\frac{1}{4} \sum_{x \in \mathbb{X}}\left(f^{*}(\boldsymbol{x})-f(\boldsymbol{x} ; \boldsymbol{\theta})\right)^{2}$$</p><p>下面选择模型$f(\boldsymbol{x}; \boldsymbol{\theta})$的形式。如果我们选择一个线性模型，会发现无法解决这个问题，因为XOR问题是线性不可分的。我们可以使用一个模型来学习一个不同的特征空间，在这个空间上线性模型能够表示这个解。引入一个有一个隐藏层的前馈神经网络：</p>$$\begin{aligned}\boldsymbol{h} &= f^{(1)}(\boldsymbol{x} ; \boldsymbol{W}, \boldsymbol{c}) \\y &= f^{(2)}(\boldsymbol{h} ; \boldsymbol{w}, b) \\f(\boldsymbol{x} ; \boldsymbol{W}, \boldsymbol{c}, \boldsymbol{w}, b) &= f^{(2)}\left(f^{(1)}(\boldsymbol{x})\right)\end{aligned}$$<p>如果$f^{(1)}$是线性函数，则整个网络仍然是线性的，因此需要让它是非线性函数。定义$\boldsymbol{h}=g\left(\boldsymbol{W}^{\top} \boldsymbol{x}+\boldsymbol{c}\right)$，其中$g$是对每个元素分别起作用的函数，在现代神经网络中，默认使用$g(z)=\max {0, z}$定义的<strong>整流线性单元</strong>（rectified linear unit），又称ReLU；于是整个网络变为</p>$$f(\boldsymbol{x} ; \boldsymbol{W}, \boldsymbol{c}, \boldsymbol{w}, b)=\boldsymbol{w}^{\top} \max \left\{0, \boldsymbol{W}^{\top} \boldsymbol{x}+\boldsymbol{c}\right\}+b$$<p>下面可以给出XOR问题的一个解。令</p>$$\begin{aligned}\boldsymbol{W} &=\left[\begin{array}{cc}1 & 1 \\1 & 1\end{array}\right] \\\boldsymbol{c} &=\left[\begin{array}{c}0 \\-1\end{array}\right] \\\boldsymbol{w} &=\left[\begin{array}{c}1 \\-2\end{array}\right] \\b &= 0\end{aligned}$$<p>模型处理一批输入的方法是，将每个输入置于矩阵的每一行，得到输入矩阵</p>$$\boldsymbol{X} =\left[\begin{array}{cc}0 & 0\\0 & 1\\1 & 0\\1 & 1\end{array}\right]$$<p>神经网络的第一步是将第一层的权重矩阵乘以输入矩阵：</p>$$\boldsymbol{XW} =\left[\begin{array}{cc}0 & 0\\1 & 1\\1 & 1\\2 & 2\end{array}\right]$$<p>然后加上偏置向量$\boldsymbol{c}$（注意此处是广播式加法），得到</p>$$\left[\begin{array}{cc}0 & -1\\1 & 0\\1 & 0\\2 & 1\end{array}\right]$$<p>对上述结果使用整流线性变换，得到</p>$$\boldsymbol{h} = \left[\begin{array}{cc}0 & 0\\1 & 0\\1 & 0\\2 & 1\end{array}\right]$$<p>最后乘以权重向量$\boldsymbol{w}$，得到</p>$$\left[\begin{array}{c}0\\1\\1\\0\end{array}\right]$$<p>在这个例子中，我们直接猜出了解决方案；基于梯度的优化算法可以找到一些参数，使得产生的误差非常小，我们此处给出的解位于损失函数的全局最小值，因此梯度下降算法可以收敛到这一点。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;首先介绍一个简单的前馈网络，它的功能是学习XOR函数。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://zhanghuimeng.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="https://zhanghuimeng.github.io/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="https://zhanghuimeng.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning读书笔记：第6章 深度前馈网络</title>
    <link href="https://zhanghuimeng.github.io/post/deep-learning-chap-6-deep-feedforward-networks/"/>
    <id>https://zhanghuimeng.github.io/post/deep-learning-chap-6-deep-feedforward-networks/</id>
    <published>2020-04-09T16:51:09.000Z</published>
    <updated>2020-04-09T16:51:09.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>深度前馈网络</strong>（deep feedforward network）：又称<strong>前馈神经网络</strong>（feedforward neural network）或<strong>多层感知机</strong>（multilayer perceptron，MLP），是典型的深度学习模型，其目标是近似某个函数$f^*$。这种模型被称为前向的，是因为在模型的输入和输出之间没有反馈（feedback）连接。</p><a id="more"></a><p>前馈神经网络被称为<strong>网络</strong>（network）是因为，它通常是不同函数复合在一起构成的，如$f(\boldsymbol{x})=f^{(3)}\left(f^{(2)}\left(f^{(1)}(\boldsymbol{x})\right)\right)$，其中$f^{(1)}$称为网络的<strong>第一层</strong>（first layer），$f^{(2)}$称为网络的<strong>第二层</strong>（second layer），以此类推。复合链的全长称为模型的<strong>深度</strong>（depth）。前馈网络的最后一层称为<strong>输出层</strong>（output layer），它的目标是产生一个接近标签的值；其它层所需的输出没有被直接给出，它们被称为<strong>隐藏层</strong>（hidden layer）。网络中的隐藏层的维数决定了模型的<strong>宽度</strong>（width）。</p><p>除了把层想象成向量到向量的单个函数，我们也可以把层想象成许多并行操作的<strong>单元</strong>（unit），每个单元表示一个向量到标量的函数。每个单元类似于一个神经元，它接收的输入来源于许多其他单元，并计算它自己的激活值。</p><p>下面考虑一种用线性模型解释前馈网络的方法。为了扩展线性模型来表示$\boldsymbol{x}$的非线性函数，我们可以不把线性模型用于$\boldsymbol{x}$本身，而是用在一个变换后的输入$\phi(\boldsymbol{x})$上。深度学习的策略是学习$\phi$。在这种方法中，我们使用的模型为$y = f(\boldsymbol{x}; \boldsymbol{\theta}, \boldsymbol{w})$，其中$\boldsymbol{\theta}$用于学习$\phi$，$\boldsymbol{w}$用于将$\phi(\boldsymbol{x})$映射到所需的输出。</p><p>本章大纲：</p><ul><li>训练前馈网络的设计决策：优化模型、代价函数、输出单元形式</li><li>隐藏层和<strong>激活函数</strong>（activation function）</li><li>网络结构</li><li><strong>反向传播</strong>（back propagation）算法</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;深度前馈网络&lt;/strong&gt;（deep feedforward network）：又称&lt;strong&gt;前馈神经网络&lt;/strong&gt;（feedforward neural network）或&lt;strong&gt;多层感知机&lt;/strong&gt;（multilayer perceptron，MLP），是典型的深度学习模型，其目标是近似某个函数$f^*$。这种模型被称为前向的，是因为在模型的输入和输出之间没有反馈（feedback）连接。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://zhanghuimeng.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="https://zhanghuimeng.github.io/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="https://zhanghuimeng.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>用梯度上升法和IRLS算法训练Logistic Regression模型</title>
    <link href="https://zhanghuimeng.github.io/post/logistic-regression-gd-irls/"/>
    <id>https://zhanghuimeng.github.io/post/logistic-regression-gd-irls/</id>
    <published>2020-03-30T02:59:30.000Z</published>
    <updated>2020-03-30T02:59:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍了以下内容：</p><ul><li>什么是Logistic Regression</li><li>如何用梯度上升法训练Logistic Regression</li><li>如何用IRLS算法训练Logistic Regression</li></ul><a id="more"></a><h2>什么是Logistic Regression</h2><p>Logistic Regression是一种（二）分类器，形式为</p><p>$$<br>P(y=1 | \boldsymbol{x}) = \frac{1}{1 + \exp{(-(w_0 + \boldsymbol{w}^T \boldsymbol{x}))}}<br>$$</p><p>这种建模方式是为了用线性回归解决分类问题。如果直接令$P(y=1 | \boldsymbol{x}) = w_0 + \boldsymbol{w}^T \boldsymbol{x}$，由于线性函数是无界的，显然上述概率分布不合法。因此对$P(y=1 | \boldsymbol{x})$进行logistic变换，使得</p><p>$$<br>\log{\frac{P(y=1 | \boldsymbol{x})}{1 - P(y=1 | \boldsymbol{x})}} = w_0 + \boldsymbol{w}^T \boldsymbol{x}<br>$$</p><p>求解即得到上述形式。</p><p>在实际预测中，当$w_0 + \boldsymbol{w}^T \boldsymbol{x} &gt; 0.5$时（即$w_0 + \boldsymbol{w}^T \boldsymbol{x} &lt; 0$时），预测$y=1$，否则预测$y = 0$，因此Logistic Regression是一个线性分类器。</p><p>为简单起见，令$x_0 = 1$，$\boldsymbol{w} = [w_0, \boldsymbol{w}]$，以下将分类器简写为</p><p>$$<br>P(y=1 | \boldsymbol{x}) = \frac{1}{1 + \exp{(-\boldsymbol{w}^T \boldsymbol{x})}}<br>$$</p><h2>Logistic Regression的似然函数</h2><p>给定训练数据${(\boldsymbol{x_i}, y_i)}_{i=1}^N$，由于只有$P(Y|X)$，没有$P(X)$或$P(X|Y)$，因此无法进行最大似然估计，只能进行最大条件似然估计（Maximum Conditional Likelihood Estimate）：</p><p>$$<br>\hat{\boldsymbol{w}}=\underset{\boldsymbol{w}}{\operatorname{argmax}} \prod_{i=1}^{N} P\left(y_{i} | \boldsymbol{x}_{i}; \boldsymbol{w}\right)<br>$$</p><p>记$p(\boldsymbol{x}_i) = P(y_{i} = 1 | \boldsymbol{x}_{i}; \boldsymbol{w})$，则</p>$$\begin{aligned}\mathcal{L}(\boldsymbol{w}) &= \log{\prod_{i=1}^N P(y_{i} | \boldsymbol{x}_{i}; \boldsymbol{w})} \\&= \log{\prod_{i=1}^N p(\boldsymbol{x}_i)^{y_i} (1 - p(\boldsymbol{x}_i))^{1-y_i}} \\&= \sum_{i=1}^N y_i \log{p(\boldsymbol{x}_i)} + (1-y_i)\log{(1 - p(\boldsymbol{x}_i))} \\&= \sum_{i=1}^N y_i \log{\frac{p(\boldsymbol{x}_i)}{1 - p(\boldsymbol{x}_i)}} + \log{(1 - p(\boldsymbol{x}_i))} \\&= \sum_{i=1}^N y_i(\boldsymbol{w}^T \boldsymbol{x}_i) + \log{\frac{\exp{(-\boldsymbol{w}^T \boldsymbol{x}_i)}}{1 + \exp{(-\boldsymbol{w}^T \boldsymbol{x}_i)}}} \\&= \sum_{i=1}^N y_i(\boldsymbol{w}^T \boldsymbol{x}_i) + \log{\frac{1}{1 + \exp{\boldsymbol{w}^T \boldsymbol{x}_i}}} \\&= \sum_{i=1}^N y_i(\boldsymbol{w}^T \boldsymbol{x}_i) - \log{(1 + \exp{(\boldsymbol{w}^T \boldsymbol{x}_i)})}\end{aligned}$$<p>将似然函数对$\boldsymbol{w}$求导，得到</p>$$\left.\frac{\partial \mathcal{L}(\boldsymbol{w})}{\partial w_j} \right|_{\boldsymbol{w}_t} = \sum_{i=1}^N y_i x_{ij} - \mu_i^t x_{ij} = \sum_{i=1}^N x_{ij} (y_i - \mu_i^t)$$<p>其中</p>$$\mu_i^t = \frac{1}{1 + \exp{(-\boldsymbol{w}^T \boldsymbol{x}_i)}} = P(y_{i} = 1 | \boldsymbol{x}_{i}; \boldsymbol{w}_t)$$<p>则有</p>$$\begin{aligned}\nabla_{\boldsymbol{w}} \mathcal{L}(\boldsymbol{w}) |_{\boldsymbol{w}_t} &= \left.\left[\frac{\partial \mathcal{L}}{\partial w_0}, \frac{\partial \mathcal{L}}{\partial w_1}, \cdots, \frac{\partial \mathcal{L}}{\partial w_d}\right]^T \right|_{\boldsymbol{w}_t} \\&= \sum_{i=1}^N \boldsymbol{x}_i (y_i - \mu_i^t)\end{aligned}$$<h2>用梯度上升法训练Logistic Regression</h2><p>得到似然函数对$\boldsymbol{w}$的梯度之后，即可立即得到梯度上升法的更新函数：</p>$$\boldsymbol{w}_{t+1} = \boldsymbol{w}_t + \eta \nabla_{\boldsymbol{w}} \mathcal{L}|_{\boldsymbol{w}_t} = \boldsymbol{w}_t + \eta \sum_{i=1}^N \boldsymbol{x}_i (y_i - \mu_i^t)$$<p>由于$\mathcal{L}$是凸函数，因此梯度上升法总能收敛。</p><h2>用IRLS（Iterative reweighted least squares)法求解</h2><p>牛顿法的迭代公式为</p>$$x_{t+1} = x_t - \frac{f(x_t)}{f'(x_t)}$$<p>推广到Logistic Regression，可以得到</p>$$\boldsymbol{w}_{t+1} \leftarrow \boldsymbol{w}_{t}-\left.H^{-1} \nabla_{\boldsymbol{w}} \mathcal{L}(\boldsymbol{w})\right|_{\boldsymbol{w}_{t}}$$<p>其中$H$是Hessian矩阵：</p>$$H = \left.\nabla_{\boldsymbol{w}}^2 \mathcal{L}(\boldsymbol{w})\right|_{\boldsymbol{w}_{t}}$$<p>下面推导具体的更新式子。</p>$$\begin{aligned}\nabla_{\boldsymbol{w}} \mathcal{L}(\boldsymbol{w}) |_{\boldsymbol{w}_t} &= \sum_{i=1}^N \boldsymbol{x}_i (y_i - \mu_i^t) \\& = X (\boldsymbol{y} - \boldsymbol{\mu}^t)\end{aligned}$$<p>其中$X = [\boldsymbol{x_1}, \cdots, \boldsymbol{x_N}]$，$\boldsymbol{y} = [y_1, \cdots, y_N]^T$，$\boldsymbol{\mu}^t = [\mu_1^t, \cdots, \mu_N^t]^T$。</p>$$\begin{aligned}\left.\frac{\partial^2 \mathcal{L}(\boldsymbol{w})}{\partial w_{i_1} \partial w_{i_2}}\right|_{\boldsymbol{w}_t} &= \frac{\partial \mathcal{L}(\boldsymbol{w})}{\partial w_{i_2}} \sum_{i=1}^N x_{ii_1} (y_i - \mu_i^t) \\&= -\sum_{i=1}^N x_{ii_1} x_{ii_2} \mu_i^t (1 - \mu_i^t)\end{aligned}$$<p>即</p>$$\begin{aligned}H &= -\sum_{i=1}^N \mu_i^t (1 - \mu_i^t) \boldsymbol{x}_i \boldsymbol{x}_i^T \\&= -XR^tX^T\end{aligned}$$<p>其中$R^t$是对角阵，$R_{ii}^t = \mu_i^t (1 - \mu_i^t)$。</p><p>最后推导出$\boldsymbol{w}$的更新公式：</p>$$\begin{aligned}\boldsymbol{w}_{t+1} &= \boldsymbol{w}_{t}-\left.H^{-1} \nabla_{\boldsymbol{w}} \mathcal{L}(\boldsymbol{w})\right|_{\boldsymbol{w}_{t}} \\&= \boldsymbol{w}_{t} - \left(-XR^tX^T\right)^{-1} X (\boldsymbol{y} - \boldsymbol{\mu}^t) \\&= \boldsymbol{w}_{t} - \left(XR^tX^T\right)^{-1} X (\boldsymbol{\mu}^t - \boldsymbol{y}) \\&= \left(XR^tX^T\right)^{-1} \left[XR^tX^T\boldsymbol{w}_{t} - X (\boldsymbol{\mu}^t - \boldsymbol{y})\right] \\&= \left(XR^tX^T\right)^{-1} XR^t\boldsymbol{z}\end{aligned}$$<p>其中</p><p>$$<br>\boldsymbol{z} = X^T\boldsymbol{w}_{t} - (R^t)^{-1} (\boldsymbol{\mu}^t - \boldsymbol{y})<br>$$</p><h3>增加正则化项</h3><p>增加正则化项后，对数似然函数变为</p><p>$$<br>-\frac{\lambda}{2} |\boldsymbol{w}|^2 + \mathcal{L}(\boldsymbol{w})<br>$$</p><p>此时</p>$$\begin{aligned}\nabla_{\boldsymbol{w}} \mathcal{L}(\boldsymbol{w}) |_{\boldsymbol{w}_t} &= X (\boldsymbol{y} - \boldsymbol{\mu}^t) - \lambda \boldsymbol{w} \\H = \left.\nabla_{\boldsymbol{w}}^2 \mathcal{L}(\boldsymbol{w})\right|_{\boldsymbol{w}_{t}} &= -XR^tX^T - \lambda I\end{aligned}$$<h2>代码</h2><p>代码见<a href="https://github.com/zhanghuimeng/logistic-regression" target="_blank" rel="noopener">logistic-regression</a>，使用的数据集是<a href="http://ml.cs.tsinghua.edu.cn/~wenbo/data/a9a.zip" target="_blank" rel="noopener">UCI a9a</a>，实现了梯度上升法和IRLS算法。算法的具体使用方法和运行结果见README。需要注意的几点是：</p><ul><li>梯度上升法对初始值不敏感，但IRLS对初始值敏感，$w$的绝对值不能太大</li><li>由于数据的稀疏性，IRLS求Hessian矩阵时可能会出现奇异矩阵，此时可以用伪逆（<code>np.linalg.pinv</code>）来代替逆，也可以增加正则项使得Hessian矩阵不再奇异</li></ul><h2>参考文献</h2><p>感谢助教和sls与我的讨论。</p><ul><li><a href="http://www.stat.cmu.edu/~cshalizi/402/lectures/14-logistic-regression/lecture-14.pdf" target="_blank" rel="noopener">14. Logistic Regression and Newton’s Method</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍了以下内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;什么是Logistic Regression&lt;/li&gt;
&lt;li&gt;如何用梯度上升法训练Logistic Regression&lt;/li&gt;
&lt;li&gt;如何用IRLS算法训练Logistic Regression&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://zhanghuimeng.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://zhanghuimeng.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PRML读书笔记：11.1 基本采样算法</title>
    <link href="https://zhanghuimeng.github.io/post/prml-chap-11-1-basic-sampling-algorithms/"/>
    <id>https://zhanghuimeng.github.io/post/prml-chap-11-1-basic-sampling-algorithms/</id>
    <published>2020-03-27T23:36:46.000Z</published>
    <updated>2020-03-27T23:36:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>本节中将研究如何从一个给定的概率分布中生成随机样本。假定已经有一个算法，能够生成$(0, 1)$之间均匀分布的伪随机数。</p><a id="more"></a><h2>11.1.1 标准概率分布</h2><p>首先考虑有了一个均匀分布的随机数来源时，如何从简单的非均匀分布中生成随机数。假定$z$在$(0, 1)$上均匀分布，令$y = f(z)$，则</p><p>$$<br>p(y) = \frac{dh}{dy} = \frac{dh}{dz} \frac{dz}{dy} = p(z) \frac{dz}{dy}<br>$$</p><p>其中$h(\cdot)$是$p(\cdot)$的CDF，且$p(z) = 1$（$z$是$(0, 1)$上的均匀分布，每个点的概率密度都是1），因此</p><p>$$<br>p(y) = \frac{dz}{dy}<br>$$</p><p>对上式积分，得到</p>$$\begin{aligned}z &= \int_{-\infty}^y \frac{dz}{dy} dy \\&= \int_{-\infty}^y p(y) dy \\&\equiv h(y)\end{aligned}$$<p>因此$y = h^{-1}(z)$，$h$是$y$的概率分布的不定积分，如下图所示：</p><p><img src="11-2.png" alt="h(y)是p(y)的不定积分"></p><h3>生成指数分布</h3><p>指数分布的概率分布函数是</p><p>$$<br>p(y) = \lambda \exp{(-\lambda y)}, ,0 \le y &lt; \infty<br>$$</p><p>此时</p>$$\begin{aligned}h(y) &= \int_{-\infty}^y p(y) dy \\&= \int_{0}^y  \lambda \exp{(-\lambda y)} dy \\&= 1 - \exp{(-\lambda y)}\end{aligned}$$<p>令$z = h(y)$，得$h^{-1}(z) = -\frac{1}{\lambda} \ln{(1-z)}$，这样变换可以使得$y$服从指数分布。</p><h3>生成柯西分布</h3><p>柯西分布的概率分布函数是</p><p>$$<br>p(y) = \frac{1}{\pi} \frac{1}{1+y^2}<br>$$</p><p>此时</p>$$\begin{aligned}h(y) &= \int_{-\infty}^y p(y) dy \\&= \int_{-\infty}^y \frac{1}{\pi} \frac{1}{1+y^2} dy \\&= \frac{1}{\pi} \arctan{(y)} + \frac{1}{2}\end{aligned}$$<p>令$z = h(y)$，得$h^{-1}(z) = \tan{(\pi(z-\frac{1}{2}))}$。</p><h3>多变量情形的推广：Box-Muller方法</h3><p>多变量情形下，只需使用Jacobian行列式：</p><p>$$p\left(y_{1}, \ldots, y_{M}\right)=p\left(z_{1}, \ldots, z_{M}\right)\left|\frac{\partial\left(z_{1}, \ldots, z_{M}\right)}{\partial\left(y_{1}, \ldots, y_{M}\right)}\right|$$</p><p>下面介绍用于生成高斯分布的Box-Muller方法。（此部分参考了<a href="https://mathworld.wolfram.com/Box-MullerTransformation.html" target="_blank" rel="noopener">Box-Muller Transformation</a>，因为我实在看不懂书上的写法）</p><p>假定$z_1, z_2$是$(0, 1)$上均匀分布的变量，则通过下列方法生成的$(y_1, y_2)$是两个相互独立的高斯分布变量：</p>$$\begin{aligned}y_1 &= \sqrt{-2\ln{z_1}} \cos{(2\pi z_2)} \\y_2 &= \sqrt{-2\ln{z_1}} \sin{(2\pi z_2)}\end{aligned}$$<p>可将上式重写为</p>$$\begin{aligned}z_1 &= \exp{\left(-\frac{y_1^2 + y_2^2}{2}\right)} \\z_2 &= \frac{1}{2\pi} \arctan{\left(\frac{y_2}{y_1}\right)}\end{aligned}$$<p>则</p>$$\begin{aligned}\frac{\partial z_1}{\partial y_1} &= -y_1 \exp{\left(-\frac{y_1^2 + y_2^2}{2}\right)} \\\frac{\partial z_1}{\partial y_2} &= -y_2 \exp{\left(-\frac{y_1^2 + y_2^2}{2}\right)} \\\frac{\partial z_2}{\partial y_1} &= -\frac{1}{2\pi} \frac{y_2}{y_1^2 + y_2^2} \\\frac{\partial z_2}{\partial y_2} &= \frac{1}{2\pi} \frac{y_1}{y_1^2 + y_2^2} \\\end{aligned}$$<p>Jacobian矩阵为</p>$$\begin{aligned}\left|\frac{\partial\left(z_{1}, z_{2}\right)}{\partial\left(y_{1}, y_{2}\right)}\right| &=\begin{vmatrix}\frac{\partial z_1}{\partial y_1} & \frac{\partial z_1}{\partial y_2} \\\frac{\partial z_2}{\partial y_1} & \frac{\partial z_2}{\partial y_2}\end{vmatrix}\\&=\begin{vmatrix}-y_1 \exp{\left(-\frac{y_1^2 + y_2^2}{2}\right)} & -y_2 \exp{\left(-\frac{y_1^2 + y_2^2}{2}\right)} \\-\frac{1}{2\pi} \frac{y_2}{y_1^2 + y_2^2} & \frac{1}{2\pi} \frac{y_1}{y_1^2 + y_2^2}\end{vmatrix}\\&= -\frac{1}{2\pi} \exp{\left(-\frac{y_1^2 + y_2^2}{2}\right)}\end{aligned}$$<p>代入到原式中</p>$$\begin{aligned}p\left(y_{1}, y_{2}\right) &=p\left(z_{1}, z_{2}\right)\left|\frac{\partial\left(z_{1}, z_{2}\right)}{\partial\left(y_{1}, y_{2}\right)}\right| \\&= \frac{1}{2\pi} \exp{\left(-\frac{y_1^2 + y_2^2}{2}\right)} \\&=\left[\frac{1}{\sqrt{2 \pi}} \exp \left(\frac{-y_{1}^{2}}{2}\right)\right]\left[\frac{1}{\sqrt{2 \pi}} \exp \left(\frac{-y_{2}^{2}}{2}\right)\right]\end{aligned}$$<h3>总结</h3><p>变换法依赖于概率分布和概率分布的不定积分的反函数，这样的计算只对一些非常简单的概率分布可行。下面考虑拒绝采样（rejection sampling）和重要采样（importance sampling）。</p><h2>11.1.2 拒绝采样</h2><p>假定我们希望从概率分布$p(z)$中采样，直接采样很困难，但计算任意给定的$z$值的$p(z)$（或者$\hat{p}(z) = Z_p p(z)$）很容易。为了应用拒绝采样方法，我们需要一个可以采样的简单的概率分布$q(z)$，称为提议分布（proposal distribution），然后引入常数$k$，使得对于任意$z$，有$kq(z) \ge p(z)$。</p><p>每次拒绝采样包括下列步骤：</p><ol><li>从概率分布$q(z)$中生成一个数$z_0$</li><li>在区间$[0, kq(z_0)]$上的均匀分布中生成一个数$u_0$；此时$(z_0, u_0)$在函数$kq(z)$的曲线下方是均匀分布的</li><li>如果$u_0 &gt; p(z_0)$，则样本被拒绝；否则样本被保留</li></ol><p>此时剩余的点对在曲线$p(z)$下方是均匀分布的，因此服从概率分布$p(z)$。</p><p>如下图所示，蓝色曲线为$kq(z)$，红色曲线为$p(z)$，如果样本落到灰色区域则被拒绝。</p><p><img src="11-4.png" alt="拒绝采样"></p><p>一个样本被接受的概率为</p>$$\begin{aligned}P &= \int \frac{p(z)}{kq(z)} q(z) dz \\&= \frac{1}{k} \int p(z) dz\end{aligned}$$<p>因此被拒绝的点的概率取决于$p(z)$下方的面积的比例，因此常数$k$应尽量小，且满足$kq(z) &gt;= p(z)$的限制。</p><h3>例：对Gamma分布进行采样</h3><p>Gamma分布的形式为</p><p>$$\operatorname{Gam}(z | a, b)=\frac{b^{a} z^{a-1} \exp (-b z)}{\Gamma(a)}$$</p><p>当$\alpha &gt; 1$时，它的形状是钟形曲线，因此可以用柯西分布（同样是钟形曲线）来进行拒绝采样。令</p><p>$$<br>q(z) = \frac{k}{\pi\gamma(1 + (\frac{x-x_0}{\gamma})^2)}<br>$$</p><p>当$x_0 = \frac{a-1}{b}$，$\gamma^2 = 2a-1$，$k=\frac{\pi \gamma b^ (a-1)^{a-1} \exp (-(a-1))}{\Gamma(a)}$（虽然没有严格证明，但我认为$k$取极值点时的比值即可）时，可以达到最小的拒绝率，如下图所示（绿线表示Gamma分布，红线表示放缩后的柯西分布）：</p><p><img src="11-5.png" alt="拒绝采样"></p><p><strong>用拒绝采样从Gamma分布中采样</strong></p><p><a href="https://github.com/zhanghuimeng/prml-code/blob/master/chp_11/11-01_01-gamma-rejection-sampling.py" target="_blank" rel="noopener">代码</a></p><p>结果如下图所示：</p><p><img src="11-01_01-gamma-rejection-sampling.png" alt="拒绝采样"></p><h2>11.1.3 可调节的拒绝采样</h2><p>很多时候确定概率分布$q(z)$的解析形式是很困难的。下面介绍一种基于$p(z)$的值直接构造函数形式的方法。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本节中将研究如何从一个给定的概率分布中生成随机样本。假定已经有一个算法，能够生成$(0, 1)$之间均匀分布的伪随机数。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://zhanghuimeng.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="https://zhanghuimeng.github.io/tags/Machine-Learning/"/>
    
      <category term="PRML" scheme="https://zhanghuimeng.github.io/tags/PRML/"/>
    
  </entry>
  
  <entry>
    <title>PRML读书笔记：第11章 采样方法</title>
    <link href="https://zhanghuimeng.github.io/post/prml-chap-11-sampling-methods/"/>
    <id>https://zhanghuimeng.github.io/post/prml-chap-11-sampling-methods/</id>
    <published>2020-03-26T09:37:13.000Z</published>
    <updated>2020-03-26T09:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>本章内容是基于数值采样的近似推断方法，也称为蒙特卡洛（Monte Carlo）方法。</p><a id="more"></a><p>本章解决的基本问题是在概率分布$p(\boldsymbol{z})$下计算函数$f(\boldsymbol{z})$的期望，即</p><p>$$<br>\mathbb{E}[f] = \int f(\boldsymbol{z})p(\boldsymbol{z}) d\boldsymbol{z}<br>$$</p><p>通常的思路是从概率分布$p(\boldsymbol{z})$中独立抽取一组变量$\boldsymbol{z}^{(l)}, , l=1,\cdots,L$，使得期望可以通过求和的方法计算：</p><p>$$<br>\hat{f} = \frac{1}{L} \sum_{l=1}^L f(\boldsymbol{z}^{(l)})<br>$$</p><p>由于$\boldsymbol{z}^{(l)}$是从概率分布$p(\boldsymbol{z})$中抽取的，因此$\mathbb{E}[\hat{f}] = \mathbb{E}[f]$。</p><p>在图模型中，以下几种情况下可以简单确定$p(\boldsymbol{z})$：</p><ul><li>没有观测变量的有向图：通过祖先采样，$p(\boldsymbol{z}) = \prod_{i=1}^M p(\boldsymbol{z}_i | pa_i)$</li><li>某些结点被观测值初始化的有向图：使用逻辑采样，每一步中进行采样并与观测值比较，如果不相符则丢弃</li><li>无向图：Gibbs采样</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本章内容是基于数值采样的近似推断方法，也称为蒙特卡洛（Monte Carlo）方法。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://zhanghuimeng.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="https://zhanghuimeng.github.io/tags/Machine-Learning/"/>
    
      <category term="PRML" scheme="https://zhanghuimeng.github.io/tags/PRML/"/>
    
  </entry>
  
  <entry>
    <title>用Gibbs采样训练LDA模型</title>
    <link href="https://zhanghuimeng.github.io/post/lda-gibbs-sampling/"/>
    <id>https://zhanghuimeng.github.io/post/lda-gibbs-sampling/</id>
    <published>2020-03-18T20:39:56.000Z</published>
    <updated>2020-03-18T20:39:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文将详细讲解LDA模型的推导过程，并附有示例代码。</p><a id="more"></a><p>LDA是一种文档主题模型，它生成文档的过程是这样的：</p><ul><li>从Dirichlet分布$\alpha$中采样，生成文档$i$的主题分布$\theta_i$</li><li>从多项式分布$\theta_i$中采样，生成文档$i$的第$j$个词的主题$z_{i,j}$</li><li>从Dirichlet分布$\beta$中采样，生成主题$z_{i,j}$对应的词语分布$\phi_{z_{i,j}}$</li><li>从多项式分布$\phi_{z_{i,j}}$中采样，最终生成词$w_{i,j}$</li></ul><h2>数学基础</h2><h3>二项分布</h3><p>抛一硬币$n$次，硬币正面朝上的概率为$p$，反面朝上的概率为$1-p$，则骰子正面朝上$k$次的概率为</p><p>$$<br>P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}<br>$$</p><p>记作二项分布$X \sim B(n, p)$。</p><h3>二项分布的共轭先验分布：Beta分布</h3><p>贝叶斯公式为</p><p>$$<br>P(\theta|X) = \frac{P(X|\theta)P(\theta)}{P(X)} \propto P(X|\theta)P(\theta)<br>$$</p><p>其中$P(X|\theta)$称为似然函数，$P(\theta)$称为先验分布，$P(\theta|X)$称为后验分布。如果先验分布和似然函数可以使得先验分布和后验分布的形式相同，则称先验分布和似然函数是共轭分布，$P(\theta)$是$P(\theta|X)$的共轭先验。</p><p>Beta分布有两个参数$\alpha&gt;0$和$\beta&gt;0$，其概率密度函数为</p><p>$$f(x ; \alpha, \beta)=\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1}$$</p><p>记为$X \sim Beta(\alpha, \beta)$，其中</p><p>$$B(\alpha, \beta)=\frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha+\beta)}$$</p><p>将二项分布作为似然函数，得到</p><p>$$P(X | \theta) \propto \theta^{k}(1-\theta)^{n-k}$$</p><p>Beta分布作为先验分布，得到</p><p>$$<br>P(\theta) \propto \theta^{\alpha - 1}(1 - \theta)^{\beta - 1}<br>$$</p><p>于是有</p>$$\begin{aligned}P(\theta | X) & \propto P(X|\theta)P(\theta) \\& \propto \theta^{k}(1-\theta)^{n-k} \theta^{\alpha - 1}(1 - \theta)^{\beta - 1} \\& = \theta^{\alpha+k-1} (1 - \theta)^{\beta+n-k-1}\end{aligned}$$<p>可以发现，后验分布服从$Beta(\alpha+k, \beta+n-k)$，因此Beta分布是二项分布的共轭先验分布。</p><h3>多项分布</h3><p>多项分布是二项分布的推广形式。令$x_1+x_2+ \cdots x_k = n$，$p_1 + p_2 + \cdots + p_k = 1$，则多项分布的概率为</p><p>$$<br>f(x_1, x_2, \cdots x_k | n, p_1, p_2, \cdots, p_k) = \frac{n!}{x_1!\cdots x_k!} p_1^{n_1} \cdots p_k^{n_k}<br>$$</p><p>记为$X \sim Mult(n, p_1, \cdots, p_n)$。</p><h3>多项分布的共轭先验分布：Dirichlet分布</h3><p>Dirichlet分布的概率密度函数为</p><p>$$<br>f(p_1, p_2, \cdots, p_k | \alpha_1, \alpha_2, \cdots \alpha_k) = \frac{1}{\Delta(\alpha_1, \cdots, \alpha_k)} p_1^{\alpha_1-1}\cdots p_k^{\alpha_k-1}<br>$$</p><p>其中</p><p>$$\Delta(\alpha_1, \cdots, \alpha_k)=\frac{\Gamma(\alpha_1)\cdots\Gamma(\alpha_k)}{\Gamma(\alpha_1+\cdots+\alpha_k)}$$</p><p>记为$P \sim Dir(\alpha_1, \cdots, \alpha_k)$。它的期望为$E(p_i)  = \frac{\alpha_i}{\sum_{j=1}^k \alpha_j}$。</p><p>将多项分布作为似然函数，得到</p><p>$$P(X | \theta_1, \cdots, \theta_k) \propto \theta_1^{n_1}\cdots\theta_k^{n_k}$$</p><p>Dirichlet分布作为先验分布，得到</p><p>$$<br>P(\theta_1, \cdots, \theta_k) \propto \theta_1^{\alpha_1 - 1}\cdots\theta_k^{\alpha_k - 1}<br>$$</p><p>于是有</p>$$\begin{aligned}P(\theta_1, \cdots, \theta_k | X) & \propto P(X|\theta_1, \cdots, \theta_k)P(\theta_1, \cdots, \theta_k) \\& \propto \theta_1^{n_1}\cdots\theta_k^{n_k} \theta_1^{\alpha_1 - 1}\cdots\theta_k^{\alpha_k - 1} \\& = \theta_1^{\alpha_1+n_1-1}\cdots\theta_k^{\alpha_k+n_k-1}\end{aligned}$$<p>后验分布也服从Dirichlet分布，因此Dirichlet分布是多项分布的共轭先验分布。</p><h2>LDA模型</h2><p>LDA模型的参数：</p><ul><li>$\vec{\alpha}$：语料级别的参数，用于生成每篇文档的主题分布</li><li>$\vec{\beta}$：语料级别的参数，用于生成每个主题的词汇分布</li><li>$\Phi$：语料级别的参数，每个主题的词汇分布</li><li>$\Theta$：文档级别的参数，每个文档的主题分布</li><li>$z_{m,n}$：词级别的参数，每个词对应的主题</li></ul><p>LDA模型生成文档的过程：</p><ul><li>对于每个主题$z_k$（$k\in [1, K]$）<ul><li>生成主题词汇分布$\vec{\phi}_k \sim Dir(\vec{\beta})$</li></ul></li><li>对于每篇文档$d_m$（$m \in [1, M]$）<ul><li>生成文档主题分布$\vec{\theta}_m \sim Dir(\vec{\alpha})$</li><li>生成文档长度$N_m \sim Poiss(\xi)$</li><li>对于文档中的每个词$w_{m,n}$（$n \in [1, N_m]$）<ul><li>生成主题$z_{m,n} \sim Mult(\vec{\theta}_m)$</li><li>生成词汇$w _ {m,n} \sim Mult(\vec{\phi} _ {z _ {m,n}})$</li></ul></li></ul></li></ul><p>第$m$篇文档中的第$n$个词是$t$的概率为</p>$$P\left(w_{m, n}=t | \vec{\theta}_{m}, \Phi\right)=\sum_{k=1}^{K} P\left(w_{m, n}=t | \vec{\phi}_{k}\right) P\left(z_{m, n}=k | \vec{\theta}_{m}\right)$$<p>文档$m$中词、隐变量和参数的联合概率分布为</p>$$P\left(\vec{w}_{m}, \vec{z}_{m}, \vec{\theta}_{m}, \Phi | \vec{\alpha}, \vec{\beta}\right)=P\left(\vec{\theta}_{m} | \vec{\alpha}\right) P(\Phi | \vec{\beta}) \prod_{n=1}^{N_{m}} P\left(w_{m, n} | \vec{\phi}_{z_{m, n}}\right) P\left(z_{m, n} | \vec{\theta}_{m}\right)$$<p>所有文档的联合概率分布为</p>$$P(\mathcal{W}, \mathcal{Z}, \Theta, \Phi | \vec{\alpha}, \vec{\beta})=\prod_{k=1}^{K} P\left(\vec{\phi}_{k} | \vec{\beta}\right) \prod_{m=1}^{M}\left(P\left(\vec{\theta}_{m} | \vec{\alpha}\right) \prod_{n=1}^{N_{m}} P\left(w_{m, n} | \vec{\phi}_{z_{m, n}}\right) P\left(z_{m, n} | \vec{\theta}_{m}\right)\right)$$<h2>Gibbs采样</h2><p>Gibbs采样是一种特殊的马尔科夫链采样方法。已知观测值为$y$，给定一个带有参数的向量$\theta = (\theta_1, \theta_2, \cdots, \theta_d)$，若$\theta_j^t$表示$\theta_j$在第$t$次迭代的采样值，则该采样值随机地取自概率分布$p(\theta_j^t | \theta_1^t, \cdots, \theta_{j-1}^t, \theta_{j+1}^{t-1}, \cdots, \theta_{d}^{t-1}, y)$。</p><h2>用Gibbs采样训练LDA模型</h2><h3>求解联合概率</h3><p>由于$\Theta$和$\Phi$分别是由$\alpha$和$\beta$生成的，因此可以将$P(\mathcal{W}, \mathcal{Z}, \Theta, \Phi | \vec{\alpha}, \vec{\beta})$分解为</p><p>$$<br>P(\mathcal{W}, \mathcal{Z}, \Theta, \Phi | \vec{\alpha}, \vec{\beta}) = P(\mathcal{W}, \mathcal{Z}| \vec{\alpha}, \vec{\beta}) = P(\mathcal{W}| \mathcal{Z}, \vec{\beta}) P(\mathcal{Z}| \vec{\alpha})<br>$$</p><p>其中$P(\mathcal{W}| \mathcal{Z}, \vec{\beta})$表示根据确定的主题$\mathcal{Z}$和词分布的先验分布参数$\vec{\beta}$采样词的过程，$P(\mathcal{Z}| \vec{\alpha})$表示根据主题分布的先验分布参数$\vec{\alpha}$采样主题的过程，这两者是独立的，可以分别计算。</p>$$\begin{aligned}P(\mathcal{W}| \mathcal{Z}, \vec{\beta}) =& \int P(\mathcal{W}| \mathcal{Z}, \Phi) P(\Phi | \vec{\beta}) d\Phi\end{aligned}$$<p>由于文档中每个词的生成是独立的，且每个词都服从多项分布，因此$P(\mathcal{W}| \mathcal{Z}, \Phi)$可以写成每个主题中每个词出现的概率的乘积，即</p>$$\begin{aligned}P(\mathcal{W}| \mathcal{Z}, \Phi) =& \prod_{m=1}^M \prod_{n=1}^{N_m} P(w_{m,n} | \vec{\phi}_{z_{m,n}}) \\=& \prod_{k=1}^K \prod_{t=1}^V \phi_{k,t}^{n_{\cdot,k,t}}\end{aligned}$$<p>其中$n_{\cdot,k,t}$表示主题$k$中词$t$的出现次数。</p><p>由于$\vec{\phi}_k \sim Dir(\vec{\beta})$，有</p><p>$$<br>P(\Phi | \vec{\beta}) = \frac{1}{\Delta(\vec{\beta})} \prod_{k=1}^K \prod_{t=1}^V \phi_{k,t}^{\beta_t - 1}<br>$$</p><p>则</p>$$\begin{aligned}P(\mathcal{W}| \mathcal{Z}, \vec{\beta}) =& \int P(\mathcal{W}| \mathcal{Z}, \Phi) P(\Phi | \vec{\beta}) d\Phi \\=& \int \prod_{k=1}^K \prod_{t=1}^V \phi_{k,t}^{n_{\cdot,k,t}} \frac{1}{\Delta(\vec{\beta})} \prod_{k=1}^K \prod_{t=1}^V \phi_{k,t}^{\beta_t - 1} d\Phi \\=& \int \prod_{k=1}^K \prod_{t=1}^V \frac{1}{\Delta(\vec{\beta})} \phi_{k,t}^{n_{\cdot,k,t} + \beta_t - 1} d\Phi \\=& \prod_{k=1}^K \frac{\Delta(\vec{n}_k + \vec{\beta})}{\Delta(\vec{\beta})}\end{aligned}$$<p>其中$\vec{n} _ k = [n _ {\cdot,k,t}], t=1,\cdots,V$。</p><p>$P(\mathcal{Z}| \vec{\alpha})$也可以用同样的方法计算：</p><p>$$<br>P(\mathcal{Z}| \vec{\alpha}) = \int P(\mathcal{Z} | \Theta) P(\Theta | \vec{\alpha}) d\Theta<br>$$</p><p>其中</p>$$\begin{aligned}P(\mathcal{Z} | \Theta) =& \prod_{m=1}^M \prod_{n=1}^{N_m} P(z_{m,n} | d_m) \\=& \prod_{m=1}^M \prod_{k=1}^K \theta_{m,k}^{n_{m,k,\cdot}}\end{aligned}$$<p>$n_{m,k,\cdot}$是主题$k$在文章$m$中出现的次数。</p><p>由于$\vec{\theta}_k \sim Dir(\vec{\alpha})$，有</p><p>$$<br>P(\Theta | \vec{\alpha}) = \frac{1}{\Delta(\vec{\alpha})} \prod_{m=1}^M \prod_{k=1}^K \theta_{m,k}^{\alpha_k - 1}<br>$$</p><p>因此</p>$$\begin{aligned}P(\mathcal{Z}| \vec{\alpha}) =& \int P(\mathcal{Z} | \Theta) P(\Theta | \vec{\alpha}) d\Theta \\=& \int \prod_{m=1}^M \prod_{k=1}^K \theta_{m,k}^{n_{m,k,\cdot}} \frac{1}{\Delta(\vec{\alpha})} \prod_{m=1}^M \prod_{k=1}^K \theta_{m,k}^{\alpha_k - 1} d\Theta \\=& \int \prod_{m=1}^M \prod_{k=1}^K \frac{1}{\Delta(\vec{\alpha})} \theta_{m,k}^{n_{m,k,\cdot} + \alpha_k - 1} d\Theta \\=& \prod_{m=1}^M \frac{\Delta(\vec{n}_m + \vec{\alpha})}{\Delta(\vec{\alpha})}\end{aligned}$$<p>其中$\vec{n} _ m = [n _ {m,k,\cdot}], k=1,\cdots,K$。</p><p>综上可得</p>$$\begin{aligned}P(\mathcal{W}, \mathcal{Z}| \vec{\alpha}, \vec{\beta}) =& P(\mathcal{W}| \mathcal{Z}, \vec{\beta}) P(\mathcal{Z}| \vec{\alpha}) \\=& \prod_{k=1}^K \frac{\Delta(\vec{n}_k + \vec{\beta})}{\Delta(\vec{\beta})} \prod_{m=1}^M \frac{\Delta(\vec{n}_m + \vec{\alpha})}{\Delta(\vec{\alpha})}\end{aligned}$$<h3>计算后验分布</h3><p>下面根据联合分布$P(\mathcal{W}, \mathcal{Z})$求解后验分布$P(\mathcal{Z} | \mathcal{W})$。根据Gibbs取样的思路，我们需要逐一排除每个词的主题分配，再根据其他词的主题分配和观察到的单词计算当前词的主题，即求解$P\left(z_{m, n}=k | \mathcal{Z}^{\neg m, n}, \mathcal{W}\right)$。</p>$$\begin{aligned}& P\left(z_{m, n}=k | \mathcal{Z}^{\neg m, n}, \mathcal{W}\right) \\=& \frac{P(\mathcal{Z}, \mathcal{W})}{P(\mathcal{Z}^{\neg m, n}, \mathcal{W})} \\=& \frac{\prod_{k=1}^K \frac{\Delta(\vec{n}_k + \vec{\beta})}{\Delta(\vec{\beta})} \prod_{m=1}^M \frac{\Delta(\vec{n}_m + \vec{\alpha})}{\Delta(\vec{\alpha})}}{\prod_{k=1}^K \frac{\Delta(\vec{n}_k^{\neg m, n} + \vec{\beta})}{\Delta(\vec{\beta})} \prod_{m=1}^M \frac{\Delta(\vec{n}_m^{\neg m, n} + \vec{\alpha})}{\Delta(\vec{\alpha})}} \\=& \frac{\Delta(\vec{n}_k + \vec{\beta})}{\Delta(\vec{n}_k^{\neg m, n} + \vec{\beta})} \frac{\Delta(\vec{n}_k + \vec{\alpha})}{\Delta(\vec{n}_k^{\neg m, n} + \vec{\alpha})} \\=& \frac{\Gamma\left(n_{\cdot,k,t}+\beta_{t}\right) \Gamma\left(\sum_{t=1}^{V} n_{\cdot, k, t}^{\neg m, n}+\beta_{t}\right)}{\Gamma\left(n_{\cdot, k, t}^{\neg m, n}+\beta_{t}\right) \Gamma\left(\sum_{t=1}^{V} n_{\cdot, k, t}+\beta_{t}\right)} \frac{\Gamma\left(n_{m, k, \cdot},+\alpha_{k}\right) \Gamma\left(\sum_{k=1}^{K} n_{m, k,\cdot}^{\neg m, n}+\alpha_{k}\right)}{\Gamma\left(n_{m, k, t}^{\neg m, n}+\alpha_{k}\right) \Gamma\left(\sum_{k=1}^{K} n_{m, k, \cdot}+\alpha_{k}\right)} \\=& \frac{n_{\cdot, k, t}^{\neg m, n} +\beta_{t}-1}{\left[\sum_{t=1}^{V} n_{\cdot, k, t}^{\neg m, n}+\beta_{t}\right]-1} \frac{n_{m, k, \cdot}^{\neg m, n}+\alpha_{k}-1}{\left[\sum_{k=1}^{K} n_{m, k,\cdot}^{\neg m, n}+\alpha_{k}\right]-1} \\\propto& \frac{n_{\cdot, k, t}^{\neg m, n} +\beta_{t}-1}{\left[\sum_{t=1}^{V} n_{\cdot, k, t}^{\neg m, n}+\beta_{t}\right]-1} (n_{m, k, \cdot}^{\neg m, n}+\alpha_{k}-1)\end{aligned}$$<h3>求解参数</h3><p>最后一步是求解$\Theta$和$\Phi$。</p>$$P\left(\vec{\theta}_{m} | \vec{z}_{m}, \vec{\alpha}\right)=\frac{1}{Z_{\theta_{m}}} \prod_{n=1}^{N_{m}} P\left(z_{m, n} | \vec{\theta}_{m}\right) P\left(\vec{\theta}_{m} | \vec{\alpha}\right)=Dir\left(\vec{\theta}_{m} | \vec{n}_{m}+\vec{\alpha}\right)$$$$P\left(\vec{\phi}_{k} | \mathcal{Z}, \mathcal{W}, \vec{\beta}\right)=\frac{1}{Z_{\phi_{k}}} \prod_{m, n: z_{m, n}=k} P\left(w_{m, n} | \vec{\phi}_{k}\right) P\left(\vec{\phi}_{k} | \vec{\beta}\right)=Dir\left(\vec{\phi}_{k} | \vec{n}_{k}+\vec{\beta}\right)$$<p>求期望可得</p>$$\hat{\theta}_{m, k}=\frac{n_{m, k, \cdot}+\alpha_{k}}{\sum_{k=1}^{K} n_{m, k, \cdot}+\alpha_{k}}$$$$\hat{\phi}_{k, t}=\frac{n_{\cdot,k,t}+\beta_{t}}{\sum_{t=1}^{V} n_{\cdot, k, t}+\beta_{t}}$$<h3>算法</h3><ul><li>将$n_{m, k, \cdot}$和$n_{\cdot,k,t}$初始化为0</li><li>令$\alpha_i = 50/K$，$\beta_i = 0.01$</li><li>随机初始化$z_{m,n}$</li><li>计算$n_{m, k, \cdot}$和$n_{\cdot,k,t}$</li><li>重复下列步骤<ul><li>$m = 1\cdots M$<ul><li>$n = 1 \cdots N_m$<ul><li>$n_{m, z_{m,n}, \cdot} = n_{m, z_{m,n}, \cdot} - 1$</li><li>$n_{\cdot,z_{m,n},w_{m,n}} = n_{\cdot,z_{m,n},w_{m,n}} - 1$</li><li>根据后验分布$P\left(z_{m, n}=k | \mathcal{Z}^{\neg m, n}, \mathcal{W}\right)$采样得到$z_k$</li><li>$n_{m, k, \cdot} = n_{m, k, \cdot} + 1$</li><li>$n_{\cdot,k,w_{m,n}} = n_{\cdot,k,w_{m,n}} + 1$</li></ul></li></ul></li></ul></li><li>直到收敛</li><li>根据$n_{m, k, \cdot}$和$n_{\cdot,k,t}$计算$\Theta$和$\Phi$</li></ul><h3>似然函数</h3><p>似然函数为</p>$$\begin{aligned}P\left(\mathcal{W} | \Theta, \Phi\right)=& \prod_m \prod_n \sum_{k=1}^{K} P\left(w_{m, n}=w | z_{m,n}=k, \phi_{k, w_{m,n}} \right) P\left(z_{m, n}=k | \theta_{m,k}\right) \\=& \prod_m \prod_n \sum_{k=1}^{K} \phi_{k, w_{m,n}} \theta_{m,k}\end{aligned}$$<p>对数似然为</p>$$\begin{aligned}\log{P\left(\mathcal{W} | \Theta, \Phi\right)} =& \log{\prod_m \prod_n \sum_{k=1}^{K} \phi_{k, w_{m,n}} \theta_{m,k}} \\=& \sum_{m} \sum_n \log{\sum_{k=1}^{K} \phi_{k, w_{m,n}} \theta_{m,k}}\end{aligned}$$<h2>代码</h2><p><a href="https://github.com/zhanghuimeng/gibbs-lda" target="_blank" rel="noopener">Gibbs Sampling参考数据和代码</a></p><p>以ICLR 2018-2019年论文题目为输入文档，进行<code>K=3</code>，<code>max_step=1000</code>的Gibbs Sampling，得到3个主题如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Topics:</span><br><span class="line">  Theme=0</span><br><span class="line">    word=for, prob=0.050888</span><br><span class="line">    word=neural, prob=0.041612</span><br><span class="line">    word=with, prob=0.032336</span><br><span class="line">    word=models, prob=0.012459</span><br><span class="line">    word=generative, prob=0.010339</span><br><span class="line">    word=using, prob=0.009809</span><br><span class="line">    word=training, prob=0.009809</span><br><span class="line">    word=unsupervised, prob=0.006893</span><br><span class="line">    word=optimization, prob=0.006363</span><br><span class="line">    word=by, prob=0.005833</span><br><span class="line">  Theme=1</span><br><span class="line">    word=networks, prob=0.044695</span><br><span class="line">    word=and, prob=0.028752</span><br><span class="line">    word=deep, prob=0.028229</span><br><span class="line">    word=in, prob=0.023002</span><br><span class="line">    word=the, prob=0.022741</span><br><span class="line">    word=a, prob=0.019866</span><br><span class="line">    word=reinforcement, prob=0.009411</span><br><span class="line">    word=from, prob=0.008627</span><br><span class="line">    word=representations, prob=0.007059</span><br><span class="line">    word=through, prob=0.004968</span><br><span class="line">  Theme=2</span><br><span class="line">    word=learning, prob=0.063051</span><br><span class="line">    word=of, prob=0.031916</span><br><span class="line">    word=to, prob=0.017646</span><br><span class="line">    word=adversarial, prob=0.016867</span><br><span class="line">    word=via, prob=0.014273</span><br><span class="line">    word=on, prob=0.011938</span><br><span class="line">    word=network, prob=0.009343</span><br><span class="line">    word=recurrent, prob=0.007527</span><br><span class="line">    word=an, prob=0.006230</span><br><span class="line">    word=variational, prob=0.006230</span><br></pre></td></tr></table></figure><p>可以看出，第一个主题因为没有去除停用词难以看出指向，第二个和第三个主题则分别是关于神经网络和学习的。</p><p>采样过程中，对数似然变化如下图所示：</p><p><img src="ll_k-3_step-1000.png" alt="对数似然的变化"></p><h2>参考文献</h2><ul><li><a href="https://blog.csdn.net/v_july_v/article/details/41209515" target="_blank" rel="noopener">通俗理解LDA主题模型</a></li><li><a href="https://www.jianshu.com/p/bb7bce40a15a" target="_blank" rel="noopener">共轭先验、共轭分布——为LDA做准备</a></li><li><a href="https://zhuanlan.zhihu.com/p/25072161" target="_blank" rel="noopener">浅谈「Gibbs采样」</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将详细讲解LDA模型的推导过程，并附有示例代码。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>PRML读书笔记：9.2 混合高斯</title>
    <link href="https://zhanghuimeng.github.io/post/prml-chap-9-2-mixtures-of-gaussians/"/>
    <id>https://zhanghuimeng.github.io/post/prml-chap-9-2-mixtures-of-gaussians/</id>
    <published>2020-03-18T05:22:15.000Z</published>
    <updated>2020-03-18T05:22:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>首先用离散隐变量来描述高斯混合模型。</p><a id="more"></a><p>高斯混合概率分布可以写成以下形式：</p>$$p(\boldsymbol{x})=\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{k}, \mathbf{\Sigma}_{k}\right)$$<p>引入$K$维one-hot变量$\boldsymbol{z}$作为隐变量（这个隐变量的含义大概是$\boldsymbol{x}$来自哪一个高斯分布），其中</p><p>$$<br>p(z_k = 1) = \pi_k<br>$$</p><p>则$\boldsymbol{x}$的条件分布为</p>$$p\left(\boldsymbol{x} | z_{k}=1\right)=\mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{k}, \mathbf{\Sigma}_{k}\right)$$<p>考虑到$\boldsymbol{z}$的其他分量都为0，上式也可以写成</p>$$p(\boldsymbol{x} | \boldsymbol{z})=\prod_{k=1}^{K} \mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)^{z_{k}}$$<p>于是$\boldsymbol{x}$的边缘概率分布可以通过对所有可能的$\boldsymbol{z}$的联合概率求和得到：</p>$$\begin{aligned}p(\boldsymbol{x}) &= \sum_{\boldsymbol{z}} p(\boldsymbol{z}) p(\boldsymbol{x} | \boldsymbol{z}) \\&= \sum_{k=1}^N p(z_k = 1) p\left(\boldsymbol{x} | z_{k}=1\right)\\&= \sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{k}, \mathbf{\Sigma}_{k}\right)\end{aligned}$$<p>于是就验证了$\boldsymbol{z}$作为隐变量时与高斯混合分布的等价性。对于每个观测数据点$\boldsymbol{x}_n$，都存在一个对应的潜在变量$\boldsymbol{z}_n$。这样做使得我们能够操作联合概率分布$p(\boldsymbol{x}, \boldsymbol{z})$，能够简化EM算法的计算。</p><p>另一个重要的量是给定$\boldsymbol{x}$的条件下$\boldsymbol{z}$的条件概率：</p>$$\begin{aligned}\gamma\left(z_{k}\right) \equiv p\left(z_{k}=1 | \boldsymbol{x}\right) &=\frac{p\left(z_{k}=1\right) p\left(\boldsymbol{x} | z_{k}=1\right)}{\sum_{j=1}^{K} p\left(z_{j}=1\right) p\left(\boldsymbol{x} | z_{j}=1\right)} \\&=\frac{\pi_{k} \mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{k}, \mathbf{\Sigma}_{k}\right)}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{j}, \mathbf{\Sigma}_{k}\right)}\end{aligned}$$<p>不妨认为$\pi_k$是$z_k = 1$的先验概率，$\gamma\left(z_{k}\right)$是观测到$\boldsymbol{x}$后$z_k = 1$的后验概率，它也可以被看做是分量$k$对于“解释”观测值$\boldsymbol{x}$的责任（或者说，$\boldsymbol{x}$是第$k$个高斯分布生成的概率是多少）。</p><h2>9.2.1 最大似然</h2><p>假定有一个观测数据集${\boldsymbol{x}_1, \cdots, \boldsymbol{x}_N}$，用高斯混合模型来对数据进行建模，则对数似然函数为</p>$$\ln p(\boldsymbol{X} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma})=\sum_{n=1}^{N} \ln \left\{\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right\}$$<p>上述似然函数可能具有奇异性，而且令导数为0时没有解析解，为此，我们需要引入EM算法。</p><h2>9.3.2 用于高斯混合模型的EM</h2><p>下面针对高斯混合模型的问题给出EM（expection-maximization）算法的一种形式。</p><h3>推导似然函数最大值需要满足的条件</h3><p>将$\ln p(\boldsymbol{X} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma})$对$\boldsymbol{\mu}_k$求导，导数置为0（高斯分布的导数见<a href="/post/prml-math-foundations/">数学基础</a>）：</p>$$\begin{aligned}\frac{\partial}{\partial \boldsymbol{\mu}_k}\ln p(\boldsymbol{X} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) &= \sum_{n=1}^N \frac{\pi_k \frac{\partial \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}{\partial \boldsymbol{\mu}_k} }{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)} \\&= \sum_{n=1}^N \frac{\pi_k \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right) \boldsymbol{\Sigma}_{k}^{-1} (\boldsymbol{x}_n - \boldsymbol{\mu}_k) }{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)}\end{aligned}$$<p>得到</p>$$0=\sum_{n=1}^{N} \underbrace{\frac{\pi_{k} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \mathbf{\Sigma}_{k}\right)}{\sum_{j} \pi_{j} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{j}, \mathbf{\Sigma}_{j}\right)}}_{\gamma\left(z_{n k}\right)} \boldsymbol{\Sigma}_{k}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{k}\right)$$<p>可以发现$\gamma\left(z_{n k}\right)$出现在了等式中。</p><p>两侧同乘$\boldsymbol{\Sigma}_{k}$，整理，可以得到</p>$$\boldsymbol{\mu}_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right) \boldsymbol{x}_{n}$$<p>其中</p><p>$$N_{k}=\sum_{n=1}^{N} \gamma\left(z_{n k}\right)$$</p><p>由于$\gamma\left(z_{k}\right)$可以被看成是分量$k$对于“解释”观测值$\boldsymbol{x}$的责任，因此$N_k$可以被看成是分配到聚类$k$的数据点的有效数量，$\boldsymbol{\mu} _ k$可以被看成是数据集内所有数据点的加权平均，权值是$\gamma\left(z _ {k}\right)$。</p><p>将$\ln p(\boldsymbol{X} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma})$对$\boldsymbol{\Sigma}_k$求导，导数置为0，得到</p>$$\begin{aligned}& \frac{\partial}{\partial \boldsymbol{\Sigma}_k}\ln p(\boldsymbol{X} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) \\=& \sum_{n=1}^N \frac{\pi_k \frac{\partial \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}{\partial \boldsymbol{\Sigma}_k} }{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)} \\=& \sum_{n=1}^N \frac{\pi_k \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right) }{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)} \frac{1}{2} \left[\boldsymbol{\Sigma}_k^{-1} (\boldsymbol{x}_n - \boldsymbol{\mu}_k)(\boldsymbol{x}_n - \boldsymbol{\mu}_k)^T \boldsymbol{\Sigma}_k^{-1} - \boldsymbol{\Sigma}_k^{-1} \right] \\=& \sum_{n=1}^N \frac{1}{2} \gamma(z_{nk}) \left[\boldsymbol{\Sigma}_k^{-1} (\boldsymbol{x}_n - \boldsymbol{\mu}_k)(\boldsymbol{x}_n - \boldsymbol{\mu}_k)^T \boldsymbol{\Sigma}_k^{-1} - \boldsymbol{\Sigma}_k^{-1} \right]\end{aligned}$$<p>令上式等于0，左右两边同乘$\boldsymbol{\Sigma}_k$，整理得到</p>$$\boldsymbol{\Sigma}_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right)\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{x}-\boldsymbol{\mu}_{k}\right)^{T}$$<p>这与多元高斯分布对应的结果类似，只不过同样进行了加权平均。</p><p>最后，关于系数$\pi_k$最大化$\ln p(\boldsymbol{X} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma})$。此处必须考虑$\sum_{k=1}^K \pi_k = 1$这一条件，因此我们使用拉格朗日乘数法，最大化下面的量：</p><p>$$\ln p(\boldsymbol{X} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma})+\lambda\left(\sum_{k=1}^{K} \pi_{k}-1\right)$$</p><p>将上式对$\pi_k$求导，得到</p>$$\sum_{n=1}^{N} \frac{\mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \mathbf{\Sigma}_{k}\right)}{\sum_{j} \pi_{j} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{j}, \mathbf{\Sigma}_{j}\right)}+\lambda = \sum_{n=1}^{N} \frac{\gamma\left(z_{n k}\right)}{\pi_k} + \lambda = 0$$<p>两侧乘以$\pi_k$，对$k$求和，得到</p>$$\sum_{k=1}^K\sum_{n=1}^{N} \gamma\left(z_{n k}\right) + \lambda \sum_{k=1}^K \pi_k = 0$$<p>得到$\lambda = -N$，代回原式，得到</p><p>$$\pi_{k}=\frac{N_{k}}{N}$$</p><p>因此第$k$个高斯分量的混合系数为这个分量对于解释数据点的“责任”的平均值。</p><h3>EM算法</h3><p>虽然上述推导并没有给出混合模型参数的一个解析解，但却给出了一个简单的用于寻找问题的极大似然解的迭代方法。这个迭代过程是EM算法应用于高斯混合模型的一个实例。在EM算法中，我们首先为$\boldsymbol{\mu}_k$、$\boldsymbol{\Sigma}_k$和$\pi_k$选择一个初始值，然后交替进行E和M两个更新步骤：</p><ul><li>E（期望）步骤：使用参数的当前值计算$\gamma(z_{nk})$</li><li>M（最大化）步骤：使用$\gamma(z_{nk})$的值重新计算$\boldsymbol{\mu}_k$、$\boldsymbol{\Sigma}_k$和$\pi_k$的值（其中先计算新的均值，再用新的均值计算协方差）</li></ul><p>之后将证明每一轮迭代都能够保证对数似然函数的增大。</p><p>在实际应用中，当对数似然函数的变化量或者参数的变化量低于某个阈值时，我们就认为算法收敛。</p><p>K-means算法通常被用于初始化EM算法。$\boldsymbol{\mu}_k$可以设置为聚类的中心点，$\boldsymbol{\Sigma}_k$可以设置为每一类的样本协方差，$\pi_k$可以被设置为第$k$类中数据点所占的比例。</p><p><strong>例：用EM算法进行聚类</strong></p><p>// TODO</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;首先用离散隐变量来描述高斯混合模型。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://zhanghuimeng.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="https://zhanghuimeng.github.io/tags/Machine-Learning/"/>
    
      <category term="PRML" scheme="https://zhanghuimeng.github.io/tags/PRML/"/>
    
  </entry>
  
  <entry>
    <title>PRML读书笔记：9.1 K-means聚类</title>
    <link href="https://zhanghuimeng.github.io/post/prml-chap-9-1-k-means-clustering/"/>
    <id>https://zhanghuimeng.github.io/post/prml-chap-9-1-k-means-clustering/</id>
    <published>2020-03-05T23:48:37.000Z</published>
    <updated>2020-03-05T23:48:37.000Z</updated>
    
    <content type="html"><![CDATA[<h3>问题的定义</h3><p>考虑寻找多维空间中数据点的分组或聚类的问题。</p><a id="more"></a><p>假设通过对$D$维欧式空间内的随机变量$\boldsymbol{x}$进行$N$次随机观测，取得数据集${\boldsymbol{x}_1, \boldsymbol{x}_2, \dots, \boldsymbol{x}_N}$，目标是将数据集分为$K$类，且$K$给定。</p><p>定义一组$D$维向量表示这$K$类的中心，记为${\boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_K}$。目标是找到数据点分别属于的聚类，以及一组向量${\boldsymbol{\mu}_k}$，使得每个数据点和它最近的向量$\boldsymbol{\mu}_k$之间的距离的平方和最小。</p><p>下面引入一些符号来描述数据点的聚类情况。 对于每个数据点$\boldsymbol{x} _ n$，引入一组对应的二值指示变量$r _ {nk} \in {0, 1}$，如果数据点$\boldsymbol{x} _ n$被分配到类别$k$，则$r _ {nk} = 1$，否则$r _ {nk} = 0$。于是可以定义一个目标函数（又称失真度量（distortion measure））：</p><p>$$<br>J=\sum_{n=1}^{N} \sum _ {k=1}^{K} r_{n k}\left|\boldsymbol{x} _ {n}-\boldsymbol{\mu} _ {k}\right|^{2}<br>$$</p><p>表示每个数据点与它对应的聚类中心距离的平方和。</p><h3>问题的求解</h3><p>我们可以用一种两个步骤迭代的方法来求解这个问题：</p><ol><li>$r_{nk}$的最优化：保持$\boldsymbol{\mu }_ k$固定，关于$r _ {nk}$最小化$J$</li><li>$\boldsymbol{\mu} _ k$的最优化：保持$r _ {nk}$固定，关于$\boldsymbol{\mu} _ k$最小化$J$</li></ol><p>不断重复这个优化过程直到$J$收敛。这两个步骤在EM算法中分别对应E（期望）和M（最大化）步骤。下面首先介绍K-means算法，在这一算法中，也将采用E和M步骤的说法。</p><h3>K-means算法</h3><h4>基本思路</h4><p>首先确定$r_{nk}$。由于$J$对于不同的数据点是独立的，只需将每个数据点安排到离它最近的聚类中心即可。</p><p>然后确定$\boldsymbol{\mu}_k$。将$J$对$\boldsymbol{\mu}_k$求导，得到</p>$$2 \sum_{n=1}^{N} r_{n k}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{k}\right)=0$$<p>求解得到</p>$$\boldsymbol{\mu}_{k}=\frac{\sum_{n} r_{n k} \boldsymbol{x}_{n}}{\sum_{n} r_{n k}}$$<p>上述表达式等价于对属于类别$k$的所有点取均值，因此该算法被称为K-means。</p><p>上述步骤不断重复执行，结束条件可取为聚类不再变化或迭代次数超过某个最大值。</p><p>K-means算法的初始值最好取为$K$个随机数据点组成的子集。</p><h4>拓展</h4><p>由于K-means算法涉及大量欧氏距离的计算，直接使用K-means算法会比较慢。下列方法可以加速K-means算法：将数据组织成树结构，或使用距离的三角不等式避免不必要的距离计算。</p><p>K-means算法可以被写成在线随机版本。</p><p>K-means算法使用的不相似程度的度量是平方欧氏距离，可以将其拓展为一个更加一般的不相似程度的度量$\mathcal{V}(\boldsymbol{x}, \boldsymbol{x}')$，将失真度量改写为</p>$$\tilde{J}=\sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k} \mathcal{V}\left(\boldsymbol{x}_{n}, \boldsymbol{\mu}_{k}\right)$$<p>于是得到了K-medoids算法。这一算法的E步骤（分配聚类）与K-means算法相同；而为了使算法能够适应于任何度量$\mathcal{V}$，通常会将聚类中心点限制为具体的数据点，于是M步骤变为在该聚类内的离散搜索。</p><p>K-means算法在每一次迭代中都将数据点分配到一个唯一的聚类，这不一定是合适的。下一节中将通过概率的方法对数据点进行“软”分配。</p><p><strong>K-means算法实例</strong></p><p><a href="https://github.com/zhanghuimeng/prml-code/blob/master/chp_09/09-01_01_k-means.py" target="_blank" rel="noopener">代码</a></p><p><img src="09-01_01_k-means.png" alt="输出结果"></p><p>上述代码对老忠实间歇喷泉数据集进行了聚类，算法共执行了3步。</p><h2>9.1.1 图像分割与压缩</h2><h3>图像分割</h3><p>将图像中的每个像素看做是一个独立的数据点，运行K-means算法直至收敛，即可得到图像的一个$K$-分割。</p><p><strong>图像分割</strong></p><p><a href="https://github.com/zhanghuimeng/prml-code/blob/master/chp_09/09-01_02_image-segmentation.py" target="_blank" rel="noopener">代码</a></p><p><img src="09-01_02_image-segmentation.png" alt="输出结果"></p><p>从左到右依次是$K=2, 3, 10$和原图。</p><h3>数据压缩</h3><p>通过只存储$K$个聚类中心的值和每个数据点对应哪个聚类，可以对数据进行有损压缩。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3&gt;问题的定义&lt;/h3&gt;
&lt;p&gt;考虑寻找多维空间中数据点的分组或聚类的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://zhanghuimeng.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="https://zhanghuimeng.github.io/tags/Machine-Learning/"/>
    
      <category term="PRML" scheme="https://zhanghuimeng.github.io/tags/PRML/"/>
    
  </entry>
  
  <entry>
    <title>PRML读书笔记：第9章 混合模型和EM</title>
    <link href="https://zhanghuimeng.github.io/post/prml-chap-9-mixture-models-and-em/"/>
    <id>https://zhanghuimeng.github.io/post/prml-chap-9-mixture-models-and-em/</id>
    <published>2020-03-05T23:00:38.000Z</published>
    <updated>2020-03-05T23:00:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>本章首先介绍混合模型。</p><a id="more"></a><p>混合模型是观测变量和隐变量（latent variable）的一个联合概率分布，这使得观测变量本身的概率分布可以通过求边缘概率的方法得到。这样，观测变量的复杂的边缘概率分布可以通过更简单的联合概率分布来表示。</p><p>混合模型还可以用于数据聚类。本章将讲到：</p><ul><li>非概率的K-Means算法</li><li>引入隐变量的EM算法</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本章首先介绍混合模型。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://zhanghuimeng.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="https://zhanghuimeng.github.io/tags/Machine-Learning/"/>
    
      <category term="PRML" scheme="https://zhanghuimeng.github.io/tags/PRML/"/>
    
  </entry>
  
  <entry>
    <title>PRML读书笔记：7.1 最大边缘分类器</title>
    <link href="https://zhanghuimeng.github.io/post/prml-chap-7-1-maximum-margin-classifiers/"/>
    <id>https://zhanghuimeng.github.io/post/prml-chap-7-1-maximum-margin-classifiers/</id>
    <published>2020-02-25T21:50:57.000Z</published>
    <updated>2020-02-25T21:50:57.000Z</updated>
    
    <content type="html"><![CDATA[<h3>模型的定义</h3><p>首先定义线性模型为</p><a id="more"></a><p>$$<br>y(\boldsymbol{x})=\boldsymbol{w}^{T} \boldsymbol{\phi}(\boldsymbol{x})+b<br>$$</p><p>其中$\boldsymbol{\phi}(\boldsymbol{x})$表示一个固定的特征空间变换，之后会用核函数$k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=\boldsymbol{\phi}(\boldsymbol{x})^{T} \boldsymbol{\phi}\left(\boldsymbol{x}^{\prime}\right)$的形式进行表达。</p><p>训练数据集为输入向量$\left\{\boldsymbol{x}_1, ..., \boldsymbol{x}_N\right\}$和对应的目标值$\left\{t_1, ..., t_N\right\}$，其中$t_n \in {-1, 1}$。假定训练数据集在特征空间中是线性可分的，即存在至少一种$\boldsymbol{w}$和$b$，能够使得$t_n=+1$时$y(\boldsymbol{x}_n)&gt;0$，$t_n=-1$时$y(\boldsymbol{x}_n)&lt;0$，即$t_n y(\boldsymbol{x}_n) &gt; 0$。</p><p>新的数据点$\boldsymbol{x}$根据$y(\boldsymbol{x})$的符号进行分类，正号分为正类，负号分为负类。</p><p>支持向量寻找泛化错误最小的解的方法是，引入边缘（margin）的概念，即决策边界与任意样本之间的最小距离，如下图所示。</p><p><img src="fig-7-1.png" alt="边界示意图"></p><p>由于点$\boldsymbol{x}$距离超平面$y(\boldsymbol{x})$的垂直距离为</p><p>$$<br>\frac{|y(\boldsymbol{x})|}{|\boldsymbol{w}|}<br>$$</p><p>且我们只关心能够正确分类所有数据点的解，因此$t_n y(\boldsymbol{x}_n) &gt; 0$，代入到距离公式中：</p><p>$$<br>\frac{|y(\boldsymbol{x})|}{|\boldsymbol{w}|} = \frac{t_n y(\boldsymbol{x}_n)}{|\boldsymbol{w}|} = \frac{t_n (\boldsymbol{w}^{T} \boldsymbol{\phi}(\boldsymbol{x_n})+b)}{|\boldsymbol{w}|}<br>$$</p><p>由于边缘由数据集里垂直距离最近的$\boldsymbol{x}_n$给出，我们希望最优化参数$\boldsymbol{w}$和$b$，使得这个距离能够最大化：</p>$$\underset{\boldsymbol{w}, b}{\arg \max }\left\{\frac{1}{\|\boldsymbol{w}\|} \min _{n}\left[t_{n}\left(\boldsymbol{w}^{T} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right)+b\right)\right]\right\}$$<h3>模型的转化</h3><p>下面将这个最优化问题转化为一个更容易求解的等价问题。</p><p>首先注意到，如果令$\boldsymbol{w}' = \kappa \boldsymbol{w}$，$b' = \kappa b$，则任意点距离决策面的距离不会发生改变：</p>$$\begin{aligned}\frac{|y'(\boldsymbol{x})|}{\|\boldsymbol{w}'\|} &= \frac{t_n (\kappa\boldsymbol{w}^{T} \boldsymbol{\phi}(\boldsymbol{x_n})+ \kappa b)}{\|\kappa \boldsymbol{w}\|} \\&= \frac{t_n (\boldsymbol{w}^{T} \boldsymbol{\phi}(\boldsymbol{x_n})+b)}{\|\boldsymbol{w}\|} \\&= \frac{|y(\boldsymbol{x})|}{\|\boldsymbol{w}\|}\end{aligned}$$<p>因此不妨对$\boldsymbol{w}$和$b$进行放缩，令距离决策面最近的点满足</p><p>$$<br>t_{n}\left(\boldsymbol{w}^{T} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right)+b\right)=1<br>$$</p><p>称为有效（active），其他数据点称为无效（inactive）。</p><p>在这种情况下，所有的数据点都满足限制</p><p>$$<br>t_n\left(\boldsymbol{w}^{T} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right)+b\right) \geq 1, \quad n=1, \ldots, N<br>$$</p><p>根据定义，最大化边缘之后，决策超平面两侧至少有两个有效的数据点，因此有</p>$$\min _{n}\left[t_{n}\left(\boldsymbol{w}^{T} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right)+b\right)\right] = 1$$<p>因此最优化问题简化为（在限制条件下的）</p><p>$$<br>\underset{\boldsymbol{w}, b}{\arg \max }\frac{1}{|\boldsymbol{w}|}<br>$$</p><p>即</p><p>$$<br>\underset{\boldsymbol{w}, b}{\arg \min } \frac{1}{2}|\boldsymbol{w}|^{2}<br>$$</p><h3>用拉格朗日乘数法得到对偶问题</h3><p>拉格朗日乘数法请参见<a href="/post/prml-math-foundations/">数学基础</a>。</p><p>引入拉格朗日乘子$a_n \ge 0$，得到拉格朗日函数</p>$$L(\boldsymbol{w}, b, \boldsymbol{a})=\frac{1}{2}\|\boldsymbol{w}\|^{2}-\sum_{n=1}^{N} a_{n}\left\{t_{n}\left(\boldsymbol{w}^{T} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right)+b\right)-1\right\}$$<p>将$L$分别对$\boldsymbol{w}$和$b$求导，得到</p>$$\begin{aligned}\nabla_{\boldsymbol{w}} L &= \boldsymbol{w} - \sum_{n=1}^N a_n t_n \boldsymbol{\phi}(\boldsymbol{x}_{n}) = 0 \\\frac{\partial L}{\partial b} &= \sum_{n=1}^N a_n t_n = 0\end{aligned}$$<p>将这两个条件代入到$L(\boldsymbol{w}, b, \boldsymbol{a})$中，消去$\boldsymbol{w}$和$b$，得到原问题的对偶表示：</p>$$\begin{aligned}\tilde{L}(\boldsymbol{a}) &= \frac{1}{2} \left\| \sum_{n=1}^N a_n t_n \boldsymbol{\phi}(\boldsymbol{x}_{n}) \right\|^2 - \sum_{n=1}^N a_n t_n \boldsymbol{w}^T \boldsymbol{\phi}(\boldsymbol{x}_{n}) - \sum_{n=1}^N a_n t_n b + \sum_{n=1}^N a_n \\&= \frac{1}{2} \sum_{n=1}^N a_n t_n \boldsymbol{\phi}(\boldsymbol{x}_{n})^T \sum_{m=1}^N a_m t_m \boldsymbol{\phi}(\boldsymbol{x}_{m}) - \sum_{n=1}^N a_n t_n \boldsymbol{\phi}(\boldsymbol{x}_{n}) \sum_{m=1}^N a_m t_m \boldsymbol{\phi}(\boldsymbol{x}_{m})^T + \sum_{n=1}^N a_n \\&= \sum_{n=1}^N a_n - \frac{1}{2} \sum_{n=1}^N \sum_{m=1}^N a_n a_m t_n t_m \boldsymbol{\phi}(\boldsymbol{x}_{n})^T \boldsymbol{\phi}(\boldsymbol{x}_{n}) \\&= \sum_{n=1}^N a_n - \frac{1}{2} \sum_{n=1}^N \sum_{m=1}^N a_n a_m t_n t_m k(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})\end{aligned}$$<p>其中$k(\boldsymbol{x}_ {n}, \boldsymbol{x}_ {m}) = \boldsymbol{\phi}(\boldsymbol{x}_ {n})^T \boldsymbol{\phi}(\boldsymbol{x}_ {n})$。这一二次函数的求解将在7.1.1节中进行讨论。（到时候会有一个差不多的二次函数，我们将用SMO算法对其进行求解。）</p><p>求解对偶问题相比原问题是有优势的。原问题是在限制下求解$M$维（$M$是特征空间的维数，或者说数据的维数）的二次规划问题。对偶问题同样是求解二次规划问题，维数是$N$（数据点的个数），这种做法的优点是模型能够用核函数重新表示，这样SVM就可以被高效应用于维数超过数据点个数的特征空间，包括无穷维特征空间。</p><p>将问题通过拉格朗日乘数法转换为此对偶问题需要满足KKT条件：</p>$$\begin{aligned}a_n & \ge 0 \\t_n y(\boldsymbol{x}_n) - 1 & \ge 0 \\a_n \left[t_n y(\boldsymbol{x}_n) - 1\right] &= 0\end{aligned}$$<p>这说明对于每个数据点必然有$a_n=0$或$t_n y(\boldsymbol{x}_n) = 1$。</p><h3>求出分类器的形式</h3><p>假定对偶问题已经得到求解，可以求出分类器的形式为</p>$$\begin{aligned}y(\boldsymbol{x}) &= \boldsymbol{w}^T \boldsymbol{\phi}(\boldsymbol{x}) + b \\&= \left[\sum_{n=1}^N a_n t_n \boldsymbol{\phi}(\boldsymbol{x}_{n})\right]^T \boldsymbol{\phi}(\boldsymbol{x}) + b \\&= \sum_{n=1}^N a_n t_n k(\boldsymbol{x}_{n}, \boldsymbol{x})+b\end{aligned}$$<p>可以看出，$a_n=0$的数据点对新数据点的预测没有作用，只有$t_n y(\boldsymbol{x}_n) = 1$的数据点有作用（有效），这些点被称为支持向量（support vector），它们恰好位于分类超平面距离为1的超平面上（称为最大边缘超平面）。这说明SVM具有稀疏性，训练结束之后大部分数据点都可以丢弃，只保留支持向量即可。</p><p>接下来考虑$b$的计算。由于支持向量$\boldsymbol{x}_n$满足$t_n y(\boldsymbol{x}_n) = 1$，可以利用这些支持向量求出一个数值稳定的$b$的解。令$\mathcal{S}$表示支持向量的下标集合，则</p>$$\begin{aligned}y(\boldsymbol{x}_n) &= \sum_{m=1}^N a_m t_m k(\boldsymbol{x}_{m}, \boldsymbol{x}_n)+b \\&= \sum_{m \in \mathcal{S}} a_m t_m k(\boldsymbol{x}_{m}, \boldsymbol{x}_n)+b \\t_n y(\boldsymbol{x}_n) &= t_n \left(\sum_{m \in \mathcal{S}} a_m t_m k(\boldsymbol{x}_{m}, \boldsymbol{x}_n)+b\right) = 1\end{aligned}$$<p>由于$t_n^2 = 1$，两侧乘以$t_n$，再对所有的支持向量$\boldsymbol{x}_n$取平均值：</p>$$\begin{aligned}\sum_{m \in \mathcal{S}} a_m t_m k(\boldsymbol{x}_{m}, \boldsymbol{x}_n)+b = t_n \\b = t_n - \sum_{m \in \mathcal{S}} a_m t_m k(\boldsymbol{x}_{m}, \boldsymbol{x}_n) \\\sum_{n \in \mathcal{S}} b = \sum_{n \in \mathcal{S}}\left(t_n - \sum_{m \in \mathcal{S}} a_m t_m k(\boldsymbol{x}_{m}, \boldsymbol{x}_n)\right) \\b = \frac{1}{|\mathcal{S}|} \sum_{n \in \mathcal{S}}\left(t_n - \sum_{m \in \mathcal{S}} a_m t_m k(\boldsymbol{x}_{m}, \boldsymbol{x}_n)\right)\end{aligned}$$<h3>隐含的模型误差函数</h3><p>为了接下来的模型比较，将SVM的最优化问题$\underset{\boldsymbol{w}, b}{\arg \min } \frac{1}{2}|\boldsymbol{w}|^{2}$用带有简单二次正则化项的误差函数表示，形式为</p>$$\sum_{n=1}^{N} E_{\infty}\left(y\left(\boldsymbol{x}_{n}\right) t_{n}-1\right)+\lambda\|\boldsymbol{w}\|^{2}$$<p>下面说明上述误差函数与原最优化问题的等价性。</p><ul><li>$E_{\infty}(z)$是一个函数，当$z \ge 0$时，函数值为0，其他情况下函数值为$\infty$。这保证了限制$t_n y\left(\boldsymbol{x}_{n}\right) \geq 1$成立，否则误差函数值将变成无穷大。</li><li>在保证前一项为0的前提下，正则化参数$\lambda$的实际值没有意义。</li></ul><h2>7.1.1 重叠类分布</h2><h3>引入松弛变量</h3><p>显然训练数据点不一定是线性可分的。之前已经指出，线性可分（“硬边缘”）的SVM隐式地使用了一个误差函数，当数据点被正确分类时，这个误差函数为0，而错误分类时为无穷大。我们现在修改这种方法，使得错误分类时的惩罚不再是无穷大，而是随着分错的数据点与决策边界的距离的增大而增大，这样我们就可以得到线性不可分（“软边缘”）的SVM了。</p><p>下面引入松弛变量（slack variable）$\xi_n$。将原有的精确分类限制条件$t_n y\left(\boldsymbol{x}_ {n}\right) \geq 1$修改为$t_ n y\left(\boldsymbol{x}_ {n}\right) \geq 1 - \xi_n$，考虑以下几种情况：</p><ul><li>点位于正确边界内部或边界上：原限制条件不用修改，$\xi_n = 0$</li><li>其他点：令$\xi_n$表示点距离正确分类边界的距离，$\xi_n = |t _ n y\left(\boldsymbol{x} _ {n}\right) - 1| = |y\left(\boldsymbol{x} _ {n}\right) - t_ n|$<ul><li>$0 &lt; \xi_n &lt; 1$的点位于边界内部，在决策边界的正确一侧</li><li>$\xi_n = 1$的点位于决策面上</li><li>$\xi_n &gt; 1$的点位于决策边界的错误一侧</li></ul></li></ul><p><img src="fig-7-3.png" alt="圆圈标记的数据点是支持向量"></p><h3>新的误差函数</h3><p>现在的目标是最大化边缘并惩罚分类错误的点，于是将硬边缘SVM的误差函数$\sum_{n=1}^{N} E_ {\infty}\left(y\left(\boldsymbol{x}_ {n}\right) t_ {n}-1\right)+\lambda|\boldsymbol{w}|^{2}$修改为</p><p>$$<br>C\sum_{n=1}^N \xi_n + \frac{1}{2}|\boldsymbol{w}|^{2}<br>$$</p><p>其中参数$C&gt;0$控制了松弛变量惩罚与最大化边缘之间的折中。由于任何被误分类的数据点都有$\xi_n &gt; 1$，因此$\sum_n \xi_n$是误分类数据点数量的上界，相当于训练误差，因此上式可以看作是训练误差和模型参数L2正则化之和，$C$类似于作用相反的正则化系数。</p><p>当$C \rightarrow \infty$时，上式就回到了硬边缘SVM的误差函数。</p><h3>用拉格朗日乘数法得到对偶问题</h3><p>为了在$t_n y\left(\boldsymbol{x}_{n}\right) \geq 1 - \xi_n$和$\xi_n \ge 0$的条件下最小化误差函数，引入拉格朗日乘数${a_n \ge 0}$和${\mu_n \ge 0}$，得到拉格朗日函数</p>$$L(\boldsymbol{w}, b, \boldsymbol{\xi}, \boldsymbol{a}, \boldsymbol{\mu})=\frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{n=1}^{N} \xi_{n}-\sum_{n=1}^{N} a_{n}\left\{t_{n} y\left(\boldsymbol{x}_{n}\right)-1+\xi_{n}\right\}-\sum_{n=1}^{N} \mu_{n} \xi_{n}$$<p>对应的KKT条件为</p>$$\begin{aligned}a_{n} \geq 0 \\t_{n} y\left(\boldsymbol{x}_{n}\right)-1+\xi_{n} \geq 0 \\a_{n}\left(t_{n} y\left(\boldsymbol{x}_{n}\right)-1+\xi_{n}\right)=0 \\\mu_{n} \geq 0 \\\xi_{n} \geq 0 \\\mu_{n} \xi_{n}=0\end{aligned}$$<p>下面对$\boldsymbol{w}$、$b$和${\xi_n}$求偏导：</p>$$\begin{aligned}\frac{\partial L}{\partial \boldsymbol{w}} &= \boldsymbol{w} - \sum_{n=1}^{N} a_{n} t_{n} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right) = 0 \\\frac{\partial L}{\partial b} &= \sum_{n=1}^N a_n t_n = 0 \\\frac{\partial L}{\partial \xi_n} &= C - a_n - \mu_n = 0\end{aligned}$$<p>使用上述结果从拉格朗日函数中消去$\boldsymbol{w}$、$b$和${\xi_n}$，得到拉格朗日函数的对偶形式：</p>$$\tilde{L}(\boldsymbol{a})=\sum_{n=1}^{N} a_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} a_{n} a_{m} t_{n} t_{m} k\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)$$<p>上述函数形式与硬边缘SVM完全相同，区别在于限制条件。由于$a_n, \mu_n \ge 0$，$a_n = C - \mu_n$，因此有$0 \le a_n \le C$，且要求$\sum_{n=1}^N a_n t_n = 0$。</p><h3>求出分类器的形式</h3><p>假设${a_n}$已经求出，下面求解分类器的形式。</p><p>将$\boldsymbol{w} = \sum_{n=1}^{N} a_{n} t_{n} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right)$代入$y(\boldsymbol{x}) = \boldsymbol{w}^T \boldsymbol{\phi}(\boldsymbol{x}) + b$中，得到与之前相同的分类器形式</p><p>$$<br>y(\boldsymbol{x}) = \sum_{n=1}^N a_n t_n k(\boldsymbol{x}_{n}, \boldsymbol{x})+b<br>$$</p><p>同样，$a_n = 0$的点对上述模型没有贡献；$a_n \neq 0$的数据点称为支持向量，它们满足$a_n &gt; 0$，因此根据KKT条件，必然有</p>$$t_ {n} y\left(\boldsymbol{x}_ {n}\right) = 1 - \xi_ {n}$$<p>下面讨论$a_n$和$C$的关系：</p><ul><li>如果$a_n &lt; C$：由于$C - a_n - \mu_n = 0$，有$\mu_n &gt; 0$；根据KKT条件的$\mu_{n} \xi_{n}=0$，得到$\xi_n = 0$，这些点位于正确边界内部或边界上</li><li>如果$a_n = C$：此时$\mu_n = 0$，由KKT条件（不等式约束的两个0不同时取到），此时$\xi_n &gt; 0$，说明这些点位于边缘内部，且$\xi_n &lt; 1$时被正确分类，$\xi_n \ge 1$时分类错误</li></ul><p>最后求解参数$b$。和之前一样，可以通过对所有满足$\xi_n = 0$（即$t_{n} y\left(\boldsymbol{x}_{n}\right)=1$）的支持向量取平均值，得到</p>$$b=\frac{1}{N_{\mathcal{M}}} \sum_{n \in \mathcal{M}}\left(t_{n}-\sum_{m \in \mathcal{S}} a_{m} t_{m} k\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)\right)$$<h3>ν-SVM</h3><p>$\nu$-SVM是支持向量机的另一种等价形式，它的构造方法为</p>$$\begin{aligned}&\underset{\mathbf{w} \in \mathcal{H}, \boldsymbol{\xi} \in \mathbb{R}^{m}, \rho, b \in \mathbb{R}}{\operatorname{minimize}} \quad & \tau(\mathbf{w}, \boldsymbol{\xi}, \rho)=\frac{1}{2}\|\mathbf{w}\|^{2}-\nu \rho+\frac{1}{m} \sum_{i=1}^{m} \xi_{i}\\&\text { subject to } \quad & y_{i}\left(\left\langle\mathbf{x}_{i}, \mathbf{w}\right\rangle+ b\right) \geq \rho-\xi_{i}, i=1, \ldots, m\\&\text { and } \quad & \xi_{i} \geq 0, \quad \rho \geq 0\end{aligned}$$<p>对偶函数为</p>$$\tilde{L}(\boldsymbol{a})=-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} a_{n} a_{m} t_{n} t_{m} k\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)$$<p>限制条件为</p>$$\begin{aligned}0 \le a_n \le \frac{1}{N} \\\sum_{n=1}^N a_n t_n = 0 \\\sum_{n=1}^N a_n \ge \nu\end{aligned}$$<p>这种方法的优点是，参数$\nu$代替了参数$C$，它既可以被看做边缘错误（margin error）（$\xi_n \ge 0$的点）的上界，也可以被看做支持向量比例的下界。具体可以参见<a href="https://www.csie.ntu.edu.tw/~cjlin/papers/nusvmtutorial.pdf" target="_blank" rel="noopener">A Tutorial on ν-Support Vector Machines</a>中的第6节。</p><h3>用SMO算法求解参数</h3><p>公式$\tilde{L}(\boldsymbol{a})$通常是二次的，且限制条件定义了一个凸区域，因此任意局部最优解也是全局最优解。下面列出一些常见的求解方法：</p><ul><li>分块（chunking）：将完整的二次规划问题分解为一系列小的二次规划问题，小问题的目标是找到所有的非零拉格朗日乘数，并丢弃其他的乘数</li><li>分解（decomposition）</li><li>SMO（sequential minimal optimazation）：每次只选择两个拉格朗日乘数，求出子问题的解析解</li></ul><p>下面给出一个<a href="http://cs229.stanford.edu/materials/smo.pdf" target="_blank" rel="noopener">简化的SMO算法的描述</a>。</p><p>首先选择两个参数$a_i$和$a_j$。选择方法是，首先选出一个（在一定数值范围内）不满足KKT条件（$a_{n} \geq 0$且$a _ {n}\left(t _ {n} y\left(\boldsymbol{x} _ {n}\right)-1+\xi _ {n}\right)=0$）的$a_i$，然后随机选择$a_j$。（这一简化使得算法不一定能收敛到全局最优解）</p><p>然后计算出$a_j$的下界$L$和上界$H$：</p><ul><li>如果$t_i \neq t_j$，则$L = \max{(0, a_j - a_i)}$，$H = \min{(C, C+a_j-a_i)}$</li><li>如果$t_i = t_j$，则$L = \max{(0, a_i + a_j - C)}$，$H = \min{(C, a_i+a_j)}$</li></ul><p>求出最大化目标函数的$a_j = a_j - \frac{y_j (E_i - E_j)}{\eta}$，其中</p><ul><li>$E_k = f(\boldsymbol{x}_k) - t_k$</li><li>$\eta = 2k(\boldsymbol{x}_i, \boldsymbol{x}_j) - k(\boldsymbol{x}_i, \boldsymbol{x}_i) - k(\boldsymbol{x}_j, \boldsymbol{x}_j)$（如果$\eta=0$，则换一对$a_i$，$a_j$）</li></ul><p>将$a_j$限制在$[L, H]$范围内：</p>$$a_j = \begin{cases}H & \text{if} \, a_j > H \\a_j & \text{if} \, L \le a_j \le H \\L & \text{if} \, a_j < L \\\end{cases}$$<p>然后计算出$a_i$的值：</p><p>$$<br>a_{i} = a_{i}+t_i t_j\left(a_{j}^{(\text {old })}-a_{j}\right)<br>$$</p><p>最后计算出相应的$b$的值：</p>$$\begin{aligned}b_1 &= b - E_i - t_i(a_i - a_i^{(\text{old})})k(\boldsymbol{x}_i, \boldsymbol{x}_j) - t_j(a_j - a_j^{(\text{old})})k(\boldsymbol{x}_i, \boldsymbol{x}_j) \\b_2 &= b - E_j - t_i(a_i - a_i^{(\text{old})})k(\boldsymbol{x}_i, \boldsymbol{x}_j) - t_j(a_j - a_j^{(\text{old})})k(\boldsymbol{x}_i, \boldsymbol{x}_j)\end{aligned}$$$$b = \begin{cases}b_1 & \text {if} \quad 0<a_i<c 2="" \\="" b_2="" &="" \text="" {if}="" \quad="" 0<a_i<c="" \left(b_{1}+b_{2}\right)="" {otherwise}="" \end{cases}="" $$="" <p=""><strong>例：实现一个使用SMO算法的SVM</strong><p></p><p><a href="https://github.com/zhanghuimeng/prml-code/blob/master/chp_07/07-01_01_svm.py" target="_blank" rel="noopener">代码</a></p><p>说实话，这段代码的数值稳定性不是很好，我都不知道自己写对了没有。下面的输出看起来还不错，但是所有的$a_i=C$，不禁令人怀疑有什么问题。</p><p><img src="07-01_01_svm.png" alt="一次看起来还不错的输出"></p><h3>核函数和维度灾难</h3><p>核函数对应于特征空间中的内积。特征空间可以是高维的，甚至是无穷维的。</p><p>（这一段我实在不知道他在说什么……）</p><h3>用SVM预测概率</h3><p>为了用SVM预测概率，可以用logistic sigmoid函数拟合训练过的SVM的输出。假设需要求解的条件概率具有以下形式：</p><p>$$<br>p(t=1 | \boldsymbol{x})=\sigma(A y(\boldsymbol{x})+B)<br>$$</p><ul><li>$y(\boldsymbol{x}) = \boldsymbol{w}^{T} \boldsymbol{\phi}(\boldsymbol{x})+b$</li><li>参数$A$和$B$的值通过最小化交叉熵误差函数确定</li><li>为了防止过拟合，用于拟合sigmoid函数的数据需要独立于训练原始SVM的数据</li></ul><p>这种两阶段的方法等价于假设支持向量机的输出$y(\boldsymbol{x})$表示属于类别$t=1$的$\boldsymbol{x}$的对数概率。由于SVM的训练过程并没有体现这种倾向，因此SVM给出的对后验概率的近似结果比较差。</p><h2>7.1.2 与logistic回归的关系</h2><p><img src="fig-7-5.png" alt="各种分类器的误差函数"></p><p>上图中$z = yt$。</p><p>误分类误差函数（图中的黑色线）的含义为，$z &gt; 0$（分类正确）时误差函数为0，否则为1。</p><p>铰链误差函数（图中的蓝色线）是软边缘SVM的误差函数，是对误分类误差函数的一个近似，$z &gt;= 1$（点位于正确分类边界一侧）时误差函数为0，否则误差函数随点距边界的距离线性增长。</p><p>红色线是放缩过的logistic回归的误差函数，也可以被看成是对误分类误差函数的近似。</p><p>绿色线是平方和误差函数。</p><h3>铰链误差函数</h3><p>硬边缘SVM和软边缘SVM的误差函数都可以用最小化正则化来表示：</p><p>$$\sum_{n=1}^{N} E_ {\infty}\left(y\left(\boldsymbol{x}_ {n}\right) t_ {n}-1\right)+\lambda|\boldsymbol{w}|^{2}$$</p><p>$$<br>C\sum_{n=1}^N \xi_n + \frac{1}{2}|\boldsymbol{w}|^{2}<br>$$</p><p>对于软边缘SVM：</p><ul><li>对于$y_n t_n &gt; 1$的数据点（完全正确分类），有$\xi_n = 0$</li><li>对于其他数据点，有$\xi_n = 1 - y_n t_n$</li></ul><p>因此可以将软边缘SVM的误差函数改写为</p><p>$$<br>\sum_{n=1}^{N} E_{S V}\left(y_{n} t_{n}\right)+\lambda|\boldsymbol{w}|^{2}<br>$$</p><p>其中</p><ul><li>$\lambda = (2C)^{-1}$</li><li>$E_{S V}$是铰链（hinge）误差函数，定义为$E_{S V}\left(y_{n} t_{n}\right)=\left[1-y _ {n} t _ {n}\right] _ {+}$，其中$[\cdot] _+$表示正数部分</li></ul><h3>logistic回归的误差函数</h3><p>可以通过对似然函数取负对数的方式构造一个误差函数。带有正则化项的误差函数的形式为</p><p>$$<br>\sum_{n=1}^{N} E_{L R}\left(y_{n} t_{n}\right)+\lambda|\boldsymbol{w}|^{2}<br>$$</p><p>其中</p><p>$$<br>E_{L R}(y t)=\ln (1+\exp (-y t))<br>$$</p><p>这一误差函数的形式与支持向量机的误差函数类似。</p><h3>平方和误差函数</h3><p>这一误差函数会着重强调那些被正确分类的在正确的一侧距离决策边界较远的点。</p><h2>7.1.3 多类SVM</h2><p>// TODO</p><h2>7.1.4 回归问题的SVM</h2><p>// TODO</p><h2>7.1.5 计算学习理论</h2><p>// TODO</p></a_i<c>]]></content>
    
    <summary type="html">
    
      &lt;h3&gt;模型的定义&lt;/h3&gt;
&lt;p&gt;首先定义线性模型为&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://zhanghuimeng.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="https://zhanghuimeng.github.io/tags/Machine-Learning/"/>
    
      <category term="PRML" scheme="https://zhanghuimeng.github.io/tags/PRML/"/>
    
  </entry>
  
  <entry>
    <title>PRML读书笔记：第7章 稀疏核机</title>
    <link href="https://zhanghuimeng.github.io/post/prml-chap-7-sparse-kernel-machines/"/>
    <id>https://zhanghuimeng.github.io/post/prml-chap-7-sparse-kernel-machines/</id>
    <published>2020-02-25T21:30:33.000Z</published>
    <updated>2020-02-25T21:30:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>本章中继续研究分类算法。之前的分类算法需要对所有的训练点对进行求值；而本章将介绍具有稀疏（sparse）解的基于核的算法，对新数据的预测只依赖于训练数据点的一个子集。</p><p>本章内容：</p><ul><li>支持向量机（support vector machine，SVM）<ul><li>可以解决分类问题、回归问题、异常点检测问题</li><li>模型参数的确定是一个凸的最优化问题</li><li>需要用到拉格朗日乘数法</li><li>不提供后验概率</li></ul></li><li>相关向量机（relevence vector machine，RVM）<ul><li>基于贝叶斯方法</li><li>提供后验概率输出</li><li>产生比SVM更稀疏的解</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本章中继续研究分类算法。之前的分类算法需要对所有的训练点对进行求值；而本章将介绍具有稀疏（sparse）解的基于核的算法，对新数据的预测只依赖于训练数据点的一个子集。&lt;/p&gt;
&lt;p&gt;本章内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持向量机（support vector machin
      
    
    </summary>
    
      <category term="读书笔记" scheme="https://zhanghuimeng.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="https://zhanghuimeng.github.io/tags/Machine-Learning/"/>
    
      <category term="PRML" scheme="https://zhanghuimeng.github.io/tags/PRML/"/>
    
  </entry>
  
</feed>
