<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  <title>THUMT代码详解（3）：训练阶段模型和数据流 | 张慕晖的博客</title>
  
  

  
  <link rel="alternate" href="/atom.xml" title="张慕晖的博客">
  

  <meta name="HandheldFriendly" content="True" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- meta -->
  

  <!-- link -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.6.3/css/all.min.css">
  
  
  <link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css'>
  

  
  <link rel="shortcut icon" type='image/x-icon' href="/files/favicon.ico">
  

  
  <link rel="stylesheet" href="/style.css">
  

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>

  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-119345306-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-119345306-1');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
  
</head>

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading-bar-wrapper">
  <div id="loading-bar" class="pure"></div>
</div>

    <script>setLoadingBarProgress(20)</script>
    <header class="l_header pure">
	<div class='wrapper'>
		<div class="nav-main container container--flex">
      <a class="logo flat-box" href='/' >
        
          张慕晖的博客
        
      </a>
			<div class='menu'>
				<ul class='h-list'>
          
				</ul>
			</div>

			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="搜索" />
						<span class="icon"><i class="fas fa-search fa-fw"></i></span>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a class="fas fa-search fa-fw" href='javascript:void(0)'></a></li>
				
				<li class='s-menu'><a class="fas fa-bars fa-fw" href='javascript:void(0)'></a></li>
			</ul>
		</div>

		<div class='nav-sub container container--flex'>
			<a class="logo flat-box"></a>
			<ul class='switcher h-list'>
				<li class='s-comment'><a class="flat-btn fas fa-comments fa-fw" href='javascript:void(0)'></a></li>
				<li class='s-toc'><a class="flat-btn fas fa-list fa-fw" href='javascript:void(0)'></a></li>
			</ul>
		</div>
	</div>
</header>
	<aside class="menu-phone">
    <header>
		<nav class="menu">
      <ul>
          
      </ul>
		</nav>
    </header>
	</aside>

    <script>setLoadingBarProgress(40);</script>
    <div class="l_body">
    <div class='container clearfix'>
        <div class='l_main'>
            <article id="post" class="post white-box article-type-post" itemscope itemprop="blogPost">
  
<section class='meta'>
  
  
  <div class="meta" id="header-meta">
    
      
          <h1 class="title">THUMT代码详解（3）：训练阶段模型和数据流</h1>
      
    

    <div class='new-meta-box'>
      
        <div class='new-meta-item author'>
          <a href="https://zhanghuimeng.github.io">
            <i class="fas fa-user" aria-hidden="true"></i>
            张慕晖
          </a>
        </div>
      
      
        <div class="new-meta-item date">
          <a class='notlink'>
            <i class="fas fa-calendar-alt" aria-hidden="true"></i>
            2020-08-31
          </a>
        </div>
      
      
        
          
          <div class='new-meta-item category'>
            <a href='/categories/NLP/'>
              <i class="fas fa-folder-open" aria-hidden="true"></i>
              NLP
            </a>
          </div>
        
      
      
        
          <div class="new-meta-item browse busuanzi">
            <a class='notlink'>
              <i class="fas fa-eye" aria-hidden="true"></i>
              <span id="busuanzi_value_page_pv">
                <i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i>
              </span>
            </a>
          </div>
        
      
      
    </div>
    <hr>
  </div>
</section>

    <section class="article typo">
      <div class="article-entry" itemprop="articleBody">
        <p><a href="/post/thumt-code-summary-1">简介篇地址</a></p>
<a id="more"></a>
<p>接下来是训练阶段的模型和数据流。入口在<a href="https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/bin/trainer.py" target="_blank" rel="noopener">trainer.py</a>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(args)</span>:</span></span><br><span class="line">    model_cls = models.get_model(args.model)</span><br><span class="line">    ......</span><br><span class="line">    model = model_cls(params).cuda()</span><br><span class="line">    ......</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_fn</span><span class="params">(inputs)</span>:</span></span><br><span class="line">        features, labels = inputs</span><br><span class="line">        loss = model(features, labels)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    ......</span><br><span class="line">    loss = train_fn(features)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure>
<p>THUMT-PyTorch版本中只实现了Transformer这个模型，因此<code>models.get_model</code>必然会返回它。</p>
<p>之后就是具体的模型和数据流了，分成模型入口、encoder和decoder来讲。</p>
<p>下文中会用以下符号标出变量的维度：</p>
<ul>
<li><code>batch</code>：batch size</li>
<li><code>length_s</code>：batch中源端句子的长度</li>
<li><code>length_t</code>：batch中目标端句子的长度</li>
<li><code>hidden</code>：表示的长度，默认为512</li>
<li><code>h2</code>: <code>= hidden // heads</code></li>
</ul>
<h2>模型入口</h2>
<p>模型初始化时首先执行<code>Transformer.__init__</code>函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"transformer"</span>)</span>:</span></span><br><span class="line">    super(Transformer, self).__init__(name=name)</span><br><span class="line">    self.params = params</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        <span class="comment"># 创建embedding，默认不共享</span></span><br><span class="line">        self.build_embedding(params)</span><br><span class="line">        <span class="comment"># 创建positional embedding，详见Transformer论文</span></span><br><span class="line">        self.encoding = modules.PositionalEmbedding()</span><br><span class="line">        <span class="comment"># 创建encoder及其参数，默认为6层</span></span><br><span class="line">        self.encoder = TransformerEncoder(params)</span><br><span class="line">        <span class="comment"># 创建decoder及其参数，默认为6层</span></span><br><span class="line">        self.decoder = TransformerDecoder(params)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算翻译结果和标签之间的交叉熵，label_smooting默认0.1</span></span><br><span class="line">    self.criterion = modules.SmoothedCrossEntropyLoss(</span><br><span class="line">        params.label_smoothing)</span><br><span class="line">    <span class="comment"># dropout，默认0.1</span></span><br><span class="line">    self.dropout = params.residual_dropout</span><br><span class="line">    <span class="comment"># hidden_size，默认512</span></span><br><span class="line">    self.hidden_size = params.hidden_size</span><br><span class="line">    <span class="comment"># encoder层数，默认6层</span></span><br><span class="line">    self.num_encoder_layers = params.num_encoder_layers</span><br><span class="line">    <span class="comment"># decoder层数，默认6层</span></span><br><span class="line">    self.num_decoder_layers = params.num_decoder_layers</span><br><span class="line">    <span class="comment"># 初始化embedding</span></span><br><span class="line">    self.reset_parameters()</span><br></pre></td></tr></table></figure>
<p>其中调用的<code>Transformer.build_embedding</code>函数是这样的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_embedding</span><span class="params">(self, params)</span>:</span></span><br><span class="line">    svoc_size = len(params.vocabulary[<span class="string">"source"</span>])</span><br><span class="line">    tvoc_size = len(params.vocabulary[<span class="string">"target"</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> params.shared_source_target_embedding <span class="keyword">and</span> svoc_size != tvoc_size:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Cannot share source and target embedding."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> params.shared_embedding_and_softmax_weights:</span><br><span class="line">        self.softmax_weights = torch.nn.Parameter(</span><br><span class="line">            torch.empty([tvoc_size, params.hidden_size]))</span><br><span class="line">        self.add_name(self.softmax_weights, <span class="string">"softmax_weights"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> params.shared_source_target_embedding:</span><br><span class="line">        self.source_embedding = torch.nn.Parameter(</span><br><span class="line">            torch.empty([svoc_size, params.hidden_size]))</span><br><span class="line">        self.target_embedding = torch.nn.Parameter(</span><br><span class="line">            torch.empty([tvoc_size, params.hidden_size]))</span><br><span class="line">        self.add_name(self.source_embedding, <span class="string">"source_embedding"</span>)</span><br><span class="line">        self.add_name(self.target_embedding, <span class="string">"target_embedding"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.weights = torch.nn.Parameter(</span><br><span class="line">            torch.empty([svoc_size, params.hidden_size]))</span><br><span class="line">        self.add_name(self.weights, <span class="string">"weights"</span>)</span><br><span class="line"></span><br><span class="line">    self.bias = torch.nn.Parameter(torch.zeros([params.hidden_size]))</span><br><span class="line">    self.add_name(self.bias, <span class="string">"bias"</span>)</span><br></pre></td></tr></table></figure>
<p>还是很简单易懂的。</p>
<p><code>Transformer.__init__</code>函数中分别创建了一个<code>TransformerEncoder</code>和一个<code>TransformerDecoder</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"encoder"</span>)</span>:</span></span><br><span class="line">    super(TransformerEncoder, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    self.normalization = params.normalization</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            TransformerEncoderLayer(params, name=<span class="string">"layer_%d"</span> % i)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(params.num_encoder_layers)])</span><br><span class="line">        <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">            self.layer_norm = modules.LayerNorm(params.hidden_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.layer_norm = <span class="keyword">None</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"decoder"</span>)</span>:</span></span><br><span class="line">    super(TransformerDecoder, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    self.normalization = params.normalization</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            TransformerDecoderLayer(params, name=<span class="string">"layer_%d"</span> % i)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(params.num_decoder_layers)])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">            self.layer_norm = modules.LayerNorm(params.hidden_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.layer_norm = <span class="keyword">None</span></span><br></pre></td></tr></table></figure>
<p>具体内容放到encoder和decoder部分再去说。</p>
<p><code>Transformer.__init__</code>函数最后调用的<code>Transformer.reset_parameters</code>函数也很简单：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span><span class="params">(self)</span>:</span></span><br><span class="line">    nn.init.normal_(self.src_embedding, mean=<span class="number">0.0</span>,</span><br><span class="line">                    std=self.params.hidden_size ** <span class="number">-0.5</span>)</span><br><span class="line">    nn.init.normal_(self.tgt_embedding, mean=<span class="number">0.0</span>,</span><br><span class="line">                    std=self.params.hidden_size ** <span class="number">-0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.params.shared_embedding_and_softmax_weights:</span><br><span class="line">        nn.init.normal_(self.softmax_weights, mean=<span class="number">0.0</span>,</span><br><span class="line">                        std=self.params.hidden_size ** <span class="number">-0.5</span>)</span><br></pre></td></tr></table></figure>
<p>值得注意的是这里的<code>src_embedding</code>、<code>tgt_embedding</code>和<code>softmax_weights</code>都是使用<code>@property</code>装饰器的属性，因为它们是可以共享权重的。</p>
<p>然后把features和labels输入到模型中，调用的是<code>Transformer.forward</code>函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, features, labels, mode=<span class="string">"train"</span>, level=<span class="string">"sentence"</span>)</span>:</span></span><br><span class="line">    <span class="comment"># mask: [batch, length_s]</span></span><br><span class="line">    mask = features[<span class="string">"target_mask"</span>]</span><br><span class="line"></span><br><span class="line">    state = self.empty_state(features[<span class="string">"target"</span>].shape[<span class="number">0</span>],</span><br><span class="line">                                labels.device)</span><br><span class="line">    state = self.encode(features, state)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure>
<p>其中调用的<code>Transformer.empty_state</code>函数是这样的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">empty_state</span><span class="params">(self, batch_size, device)</span>:</span></span><br><span class="line">    state = &#123;</span><br><span class="line">        <span class="string">"decoder"</span>: &#123;</span><br><span class="line">            <span class="string">"layer_%d"</span> % i: &#123;</span><br><span class="line">                <span class="string">"k"</span>: torch.zeros([batch_size, <span class="number">0</span>, self.hidden_size],</span><br><span class="line">                                    device=device),</span><br><span class="line">                <span class="string">"v"</span>: torch.zeros([batch_size, <span class="number">0</span>, self.hidden_size],</span><br><span class="line">                                    device=device)</span><br><span class="line">            &#125; <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_decoder_layers)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> state</span><br></pre></td></tr></table></figure>
<p>大概就是创建了decoder每层的key和value的一个位置。</p>
<p>之后就是encoder部分了。</p>
<h2>encoder</h2>
<p>首先是<code>encode</code>函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, features, state)</span>:</span></span><br><span class="line">    <span class="comment"># src_seq: [batch, length_s]</span></span><br><span class="line">    src_seq = features[<span class="string">"source"</span>]</span><br><span class="line">    <span class="comment"># src_mask: [batch, length_s]</span></span><br><span class="line">    src_mask = features[<span class="string">"source_mask"</span>]</span><br><span class="line">    enc_attn_bias = self.masking_bias(src_mask)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure>
<p><code>encode</code>函数调用了<code>masking_bias</code>，这个函数的主要功能是把<code>src_mask</code>中为0的部分变成无穷大，在attention的时候用来去掉padding部分的attention：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mask: [batch, length_s]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masking_bias</span><span class="params">(mask, inf=<span class="number">-1e9</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 把mask中非0部分变成无穷小</span></span><br><span class="line">    ret = (<span class="number">1.0</span> - mask) * inf</span><br><span class="line">    <span class="comment"># unsqueeze后的形状：[batch, 1, 1, length]</span></span><br><span class="line">    <span class="keyword">return</span> torch.unsqueeze(torch.unsqueeze(ret, <span class="number">1</span>), <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>然后回到<code>Transformer.encode</code>函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, features, state)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    <span class="comment"># 实际进行embedding</span></span><br><span class="line">    <span class="comment"># input: [batch, length_s, hidden]</span></span><br><span class="line">    inputs = torch.nn.functional.embedding(src_seq, self.src_embedding)</span><br><span class="line">    <span class="comment"># 保证inputs的方差和均值都是1</span></span><br><span class="line">    inputs = inputs * (self.hidden_size ** <span class="number">0.5</span>)</span><br><span class="line">    inputs = inputs + self.bias</span><br><span class="line">    <span class="comment"># 在dropout之前先把positional embedding加上</span></span><br><span class="line">    inputs = nn.functional.dropout(self.encoding(inputs), self.dropout,</span><br><span class="line">                                    self.training)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将enc_attn_bias的dtype的device修改为与inputs匹配</span></span><br><span class="line">    enc_attn_bias = enc_attn_bias.to(inputs)</span><br><span class="line">    <span class="comment"># 调用TransformerEncoder.forward</span></span><br><span class="line">    encoder_output = self.encoder(inputs, enc_attn_bias)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure>
<p>前面我们已经看到了，<code>Transformer</code>是先把对应的Module和权重都创建出来，再在运行过程中进行计算的。因此我们首先来看一下创建的过程，首先是<code>TransformerEncoder.__init__</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"encoder"</span>)</span>:</span></span><br><span class="line">    super(TransformerEncoder, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    self.normalization = params.normalization</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        <span class="comment"># 创建了若干个TransformerEncoderLayer，默认数量为6</span></span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            TransformerEncoderLayer(params, name=<span class="string">"layer_%d"</span> % i)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(params.num_encoder_layers)])</span><br><span class="line">        <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">            self.layer_norm = modules.LayerNorm(params.hidden_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.layer_norm = <span class="keyword">None</span></span><br></pre></td></tr></table></figure>
<p>然后是<code>TransformerEncoderLayer.__init__</code>，每层包括两个sub layer：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"layer"</span>)</span>:</span></span><br><span class="line">    super(TransformerEncoderLayer, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        self.self_attention = AttentionSubLayer(params)</span><br><span class="line">        self.feed_forward = FFNSubLayer(params)</span><br></pre></td></tr></table></figure>
<p>然后是<code>AttentionSubLayer.__init__</code>和<code>FFNSubLayer.__init__</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"attention"</span>)</span>:</span></span><br><span class="line">    super(AttentionSubLayer, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    self.dropout = params.residual_dropout</span><br><span class="line">    self.normalization = params.normalization</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        self.attention = modules.MultiHeadAttention(</span><br><span class="line">            params.hidden_size, params.num_heads, params.attention_dropout)</span><br><span class="line">        self.layer_norm = modules.LayerNorm(params.hidden_size)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, dtype=None, name=<span class="string">"ffn_layer"</span>)</span>:</span></span><br><span class="line">    super(FFNSubLayer, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    self.dropout = params.residual_dropout</span><br><span class="line">    self.normalization = params.normalization</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        self.ffn_layer = modules.FeedForward(params.hidden_size,</span><br><span class="line">                                                params.filter_size,</span><br><span class="line">                                                dropout=params.relu_dropout)</span><br><span class="line">        self.layer_norm = modules.LayerNorm(params.hidden_size)</span><br></pre></td></tr></table></figure>
<p><code>AttentionSubLayer.__init__</code>还调用了<code>MultiHeadAttention.__init__</code>（<code>FFNSubLayer.__init__</code>也调用了<code>FeedForward.__init__</code>，但很简单，就不说了）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, num_heads, dropout=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                name=<span class="string">"multihead_attention"</span>)</span>:</span></span><br><span class="line">    super(MultiHeadAttention, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    self.num_heads = num_heads</span><br><span class="line">    self.hidden_size = hidden_size</span><br><span class="line">    self.dropout = dropout</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        <span class="comment"># attention中的q、k、v</span></span><br><span class="line">        self.q_transform = Affine(hidden_size, hidden_size,</span><br><span class="line">                                    name=<span class="string">"q_transform"</span>)</span><br><span class="line">        self.k_transform = Affine(hidden_size, hidden_size,</span><br><span class="line">                                    name=<span class="string">"k_transform"</span>)</span><br><span class="line">        self.v_transform = Affine(hidden_size, hidden_size,</span><br><span class="line">                                    name=<span class="string">"v_transform"</span>)</span><br><span class="line">        <span class="comment"># combine heads之后最后再做一个变换</span></span><br><span class="line">        self.o_transform = Affine(hidden_size, hidden_size,</span><br><span class="line">                                    name=<span class="string">"o_transform"</span>)</span><br><span class="line"></span><br><span class="line">    self.reset_parameters()</span><br></pre></td></tr></table></figure>
<p>然后再回到<code>Transformer.encode</code>函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, features, state)</span>:</span></span><br><span class="line">    encoder_output = self.encoder(inputs, enc_attn_bias)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure>
<p>首先调用<code>TransformerEncoder.forward</code>函数（因为这些类都是继承<code>nn.Module</code>的）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, bias)</span>:</span></span><br><span class="line">    <span class="comment"># x: [batch, length_s]</span></span><br><span class="line">    <span class="comment"># bias: [batch, 1, 1, length_s]</span></span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">        x = layer(x, bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">        x = self.layer_norm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>直接调用每一层的<code>TransformerEncoderLayer.forward</code>，把<code>x</code>和<code>bias</code>传过去：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, bias)</span>:</span></span><br><span class="line">    x = self.self_attention(x, bias)</span><br><span class="line">    x = self.feed_forward(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>只是简单地把<code>x</code>和<code>bias</code>传给了<code>MultiHeadAttention.forward</code>函数，接下来的部分比较重要：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在encoder self-attention的情况下，memory和kv都是None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, bias, memory=None, kv=None)</span>:</span></span><br><span class="line">    <span class="comment"># q: [batch, length_s, hidden]</span></span><br><span class="line">    <span class="comment"># bias: [batch, 1, 1, length_s]</span></span><br><span class="line">    q = self.q_transform(query)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 因为是self-attention，所以if部分的内容没用</span></span><br><span class="line">    <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            k, v = kv</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            k, v = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># encoder-decoder attention</span></span><br><span class="line">        k = k <span class="keyword">or</span> self.k_transform(memory)</span><br><span class="line">        v = v <span class="keyword">or</span> self.v_transform(memory)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># self-attention</span></span><br><span class="line">        <span class="comment"># k: [batch, length_s, hidden]</span></span><br><span class="line">        k = self.k_transform(query)</span><br><span class="line">        <span class="comment"># v: [batch, length_s, hidden]</span></span><br><span class="line">        v = self.v_transform(query)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># kv is None，不用管</span></span><br><span class="line">        <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            k = torch.cat([kv[<span class="number">0</span>], k], dim=<span class="number">1</span>)</span><br><span class="line">            v = torch.cat([kv[<span class="number">1</span>], v], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># split heads</span></span><br><span class="line">    <span class="comment"># 主要功能是把维度为[batch, length_s, hidden]的矩阵变成</span></span><br><span class="line">    <span class="comment"># [batch, heads, length_s, h2]</span></span><br><span class="line">    <span class="comment"># 其中h2 = hidden // heads</span></span><br><span class="line">    qh = self.split_heads(q, self.num_heads)</span><br><span class="line">    kh = self.split_heads(k, self.num_heads)</span><br><span class="line">    vh = self.split_heads(v, self.num_heads)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># scale query</span></span><br><span class="line">    qh = qh * (self.hidden_size // self.num_heads) ** <span class="number">-0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># dot-product attention</span></span><br><span class="line">    <span class="comment"># kh: [batch, heads, h2, length_s]</span></span><br><span class="line">    kh = torch.transpose(kh, <span class="number">-2</span>, <span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># 在matmul时，前两维不变，后两维相乘，得到维度为</span></span><br><span class="line">    <span class="comment"># logits: [batch, heads, length_s, length_s]</span></span><br><span class="line">    <span class="comment"># 相当于每一个句子有若干个head，每个head都有一个length_s*length_s的矩阵，代表q到k的attention</span></span><br><span class="line">    logits = torch.matmul(qh, kh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># bias: [batch, 1, 1, length_s]</span></span><br><span class="line">    <span class="comment"># 在每一行加上对应的bias，使padding部分的attention被屏蔽</span></span><br><span class="line">    <span class="comment"># logits维度不变: [batch, heads, length_s, length_s]</span></span><br><span class="line">    <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        logits = logits + bias</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对最后一维（每行）做softmax，然后做dropout，得到</span></span><br><span class="line">    <span class="comment"># weight: [batch, heads, length_s, length_s]</span></span><br><span class="line">    weights = torch.nn.functional.dropout(torch.softmax(logits, dim=<span class="number">-1</span>),</span><br><span class="line">                                            p=self.dropout,</span><br><span class="line">                                            training=self.training)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x: [batch, heads, length_s, h2]</span></span><br><span class="line">    x = torch.matmul(weights, vh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># combine heads</span></span><br><span class="line">    <span class="comment"># output: [batch, length_s, hidden]</span></span><br><span class="line">    output = self.o_transform(self.combine_heads(x))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> output, k, v</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后直接返回output</span></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>说起来，这里面可能有几个不太好理解的地方。第一个是<code>MultiHeadAttentionBase.split_heads</code>和<code>MultiHeadAttentionBase.combine_heads</code>的具体实现方法：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_heads</span><span class="params">(x, heads)</span>:</span></span><br><span class="line">    batch = x.shape[<span class="number">0</span>]</span><br><span class="line">    length = x.shape[<span class="number">1</span>]</span><br><span class="line">    channels = x.shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    y = torch.reshape(x, [batch, length, heads, channels // heads])</span><br><span class="line">    <span class="keyword">return</span> torch.transpose(y, <span class="number">2</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>为什么要先reshape再transpose，而不是直接reshape呢？显然，我们需要修改<code>head</code>这一维度的位置。reshape成<code>[batch, length, heads, h2]</code>的维度之后，相当于每个token长度为<code>heads * h2</code>的表示向量被分成了<code>heads</code>个向量，每个向量的长度为<code>h2</code>。然而我们希望的是每个句子的表示分成<code>heads</code>个矩阵，每个矩阵的维度是<code>[length, h2]</code>。为了做到这一点，需要进行transpose，也就相当于把后三维旋转一下：</p>
<p><img src="transpose.png" alt="transpose的过程"></p>
<p>而如果直接reshape的话，实际效果是把<code>length</code>一维分到<code>heads</code>一维去了，这显然不太合理。</p>
<p>如果还是觉得不够形象的话，请参见这篇文章：<a href="https://lihan.me/2018/01/numpy-reshape-and-transpose/" target="_blank" rel="noopener">Numpy reshape and transpose</a></p>
<p>而<code>combine_heads</code>正好是把上述过程反过来：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combine_heads</span><span class="params">(x)</span>:</span></span><br><span class="line">    batch = x.shape[<span class="number">0</span>]</span><br><span class="line">    heads = x.shape[<span class="number">1</span>]</span><br><span class="line">    length = x.shape[<span class="number">2</span>]</span><br><span class="line">    channels = x.shape[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    y = torch.transpose(x, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.reshape(y, [batch, length, heads * channels])</span><br></pre></td></tr></table></figure>
<p>第二个地方是<code>torch.matmul</code>的broadcast。在上述过程中，一个维度为<code>[batch, heads, length_s, h2]</code>和一个维度为<code>[batch, heads, h2, length_s]</code>的矩阵相乘，实际效果是有<code>batch * heads</code>个维度为<code>[length_s, h2]</code>的矩阵和<code>[h2, length_s]</code>的矩阵相乘。这倒是很好理解，不过还是应该说一下<code>torch.matmul</code>的broadcast规则。在多维数据的情况下，<code>matmul</code>使用两个矩阵的后两个维度进行相乘，其他的维度都可以认为是batch维度。详情见<a href="https://pytorch.org/docs/stable/generated/torch.matmul.html" target="_blank" rel="noopener">TORCH.MATMUL</a></p>
<p>第三个地方是<code>bias</code>和<code>logits</code>加法的broadcast（好吧，怎么又是broadcast）：一个维度为<code>[batch, 1, 1, length_s]</code>的矩阵加到维度为<code>[batch, heads, length_s, length_s]</code>的矩阵上，如何broadcast呢？这里需要参照<a href="https://pytorch.org/docs/stable/notes/broadcasting.html" target="_blank" rel="noopener">BROADCASTING SEMANTICS</a>：</p>
<blockquote>
<p>Two tensors are “broadcastable” if the following rules hold:</p>
<ul>
<li>Each tensor has at least one dimension.</li>
<li>When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.</li>
</ul>
</blockquote>
<p>显然，<code>bias</code>矩阵的大小是符合这个条件的。最后的效果就是，对于batch里的每一个句子，它的每一个head对应的attention矩阵的每一行都加上了对应于mask的一个偏置，把padding部分的k和v都屏蔽掉了。</p>
<p>（Feed-forward部分忽略）</p>
<p>最后回到<code>Transformer.encode</code>函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, features, state)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    state[<span class="string">"encoder_output"</span>] = encoder_output</span><br><span class="line">    state[<span class="string">"enc_attn_bias"</span>] = enc_attn_bias</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> state</span><br></pre></td></tr></table></figure>
<p>此时的<code>state</code>是这样的（在encode的过程中<code>&quot;decoder&quot;</code>的部分根本就没动，只是加了几个属性）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"encoder_output"</span>: [batch, length_s, hidden]</span><br><span class="line">    <span class="string">"enc_attn_bias"</span>: [batch, <span class="number">1</span>, <span class="number">1</span>, length_s]</span><br><span class="line">    <span class="string">"decoder"</span>: ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2>decoder</h2>
<p>和之前一样，我们先来看看创建权重的过程。首先是<code>TransformerDecoder.__init__</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"decoder"</span>)</span>:</span></span><br><span class="line">    super(TransformerDecoder, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    self.normalization = params.normalization</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            TransformerDecoderLayer(params, name=<span class="string">"layer_%d"</span> % i)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(params.num_decoder_layers)])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">            self.layer_norm = modules.LayerNorm(params.hidden_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.layer_norm = <span class="keyword">None</span></span><br></pre></td></tr></table></figure>
<p>很显然，它创建了若干个（默认为6个）<code>TransformerDecoderLayer</code>。<code>TransformerDecoderLayer.__init__</code>的内容如下所示：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, name=<span class="string">"layer"</span>)</span>:</span></span><br><span class="line">    super(TransformerDecoderLayer, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> utils.scope(name):</span><br><span class="line">        self.self_attention = AttentionSubLayer(params,</span><br><span class="line">                                                name=<span class="string">"self_attention"</span>)</span><br><span class="line">        self.encdec_attention = AttentionSubLayer(params,</span><br><span class="line">                                                name=<span class="string">"encdec_attention"</span>)</span><br><span class="line">        self.feed_forward = FFNSubLayer(params)</span><br></pre></td></tr></table></figure>
<p>它创建了两个<code>AttentionSubLayer</code>（self-attention和enc-dec attention）和一个<code>FFNSublayer</code>，这些在encoder的部分都已经说过了，所以不再说了。</p>
<p>之后我们回到<code>Transformer.forward</code>函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, features, labels, mode=<span class="string">"train"</span>, level=<span class="string">"sentence"</span>)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    logits, _ = self.decode(features, state, mode=mode)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure>
<p>它调用了<code>Transformer.decode</code>函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, features, state, mode=<span class="string">"infer"</span>)</span>:</span></span><br><span class="line">    <span class="comment"># [batch, length_t]</span></span><br><span class="line">    tgt_seq = features[<span class="string">"target"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch, 1, 1, length_s]</span></span><br><span class="line">    enc_attn_bias = state[<span class="string">"enc_attn_bias"</span>]</span><br><span class="line">    <span class="comment"># [1, 1, length_t, length_t]</span></span><br><span class="line">    <span class="comment"># 是一个下三角部分为0（包括对角线），上三角部分为无穷小的矩阵</span></span><br><span class="line">    dec_attn_bias = self.causal_bias(tgt_seq.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对目标端进行embedding</span></span><br><span class="line">    <span class="comment"># targets: [batch, length_t, hidden]</span></span><br><span class="line">    targets = torch.nn.functional.embedding(tgt_seq, self.tgt_embedding)</span><br><span class="line">    targets = targets * (self.hidden_size ** <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 去掉tgt_seq开头的&lt;bos&gt;，把表示全都换成0</span></span><br><span class="line">    decoder_input = torch.cat(</span><br><span class="line">        [targets.new_zeros([targets.shape[<span class="number">0</span>], <span class="number">1</span>, targets.shape[<span class="number">-1</span>]]),</span><br><span class="line">            targets[:, <span class="number">1</span>:, :]], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 加入positional encoding，进行dropout</span></span><br><span class="line">    decoder_input = nn.functional.dropout(self.encoding(decoder_input),</span><br><span class="line">                                            self.dropout, self.training)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch, length_s, hidden]</span></span><br><span class="line">    encoder_output = state[<span class="string">"encoder_output"</span>]</span><br><span class="line">    <span class="comment"># 调整dec_attn_bias的dtype和device</span></span><br><span class="line">    dec_attn_bias = dec_attn_bias.to(targets)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># train阶段先不用管</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">"infer"</span>:</span><br><span class="line">        decoder_input = decoder_input[:, <span class="number">-1</span>:, :]</span><br><span class="line">        dec_attn_bias = dec_attn_bias[:, :, <span class="number">-1</span>:, :]</span><br><span class="line"></span><br><span class="line">    decoder_output = self.decoder(decoder_input, dec_attn_bias,</span><br><span class="line">                                    enc_attn_bias, encoder_output, state)</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure>
<p>为什么这里要把<code>decoder_input</code>每个batch的第一列都换成0呢？这和训练模式下我们到底是如何进行训练的有关。事实上，假设我们有一个句子是</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    <span class="string">""</span>,</span><br><span class="line">    <span class="string">"Ich"</span>,</span><br><span class="line">    <span class="string">"bin"</span>,</span><br><span class="line">    <span class="string">"ein"</span>,</span><br><span class="line">    <span class="string">"Student"</span>,</span><br><span class="line">    <span class="string">"."</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>那么我们在完成decoder的解码过程后，在每个位置期望得到的是：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    <span class="string">"Ich"</span> (The token after <span class="string">""</span>),</span><br><span class="line">    <span class="string">"bin"</span> (The token after <span class="string">"Ich"</span>),</span><br><span class="line">    <span class="string">"ein"</span> (The token after <span class="string">"Ich bin"</span>),</span><br><span class="line">    <span class="string">"Student"</span> (The token after <span class="string">"Ich bin ein"</span>),</span><br><span class="line">    <span class="string">"."</span> (The token after <span class="string">"Ich bin ein Student"</span>),</span><br><span class="line">    <span class="string">"&lt;eos&gt;"</span> (The token after <span class="string">"Ich bin ein Student ."</span>)</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>这种方法叫做Teacher Forcing：在训练阶段，我们不考虑每一步decoder实际输出的token，而是直接把正确的（一部分）句子输入到decoder中。</p>
<p>然后就拿着这些input、bias、output和state去调用<code>TransformerDecoder.forward</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x: [batch, length_t, hidden]</span></span><br><span class="line"><span class="comment"># attn_bias: [1, 1, length_t, length_t]，下三角为0（包括对角线，上三角为无穷小）</span></span><br><span class="line"><span class="comment"># enc_attn_bias: [batch, 1, 1, length_s]，实际句子部分为0，padding部分为无穷小</span></span><br><span class="line"><span class="comment"># memory: [batch, length_s, hidden]，就是encoder_output...</span></span><br><span class="line"><span class="comment"># state: 见encoder一节最后部分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, attn_bias, encdec_bias, memory, state=None)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(self.layers):</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="comment"># 从encoder节可以看出，这里的state["decoder"]["layer_%d" % i]是两个空的矩阵</span></span><br><span class="line">            <span class="comment"># 传过去只是为了记录</span></span><br><span class="line">            x = layer(x, attn_bias, encdec_bias, memory,</span><br><span class="line">                        state[<span class="string">"decoder"</span>][<span class="string">"layer_%d"</span> % i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = layer(x, attn_bias, encdec_bias, memory, <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">        x = self.layer_norm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><code>x</code>被逐层传给<code>TransformerDecoderLayer.__call__</code>函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x, attn_bias, encdec_bias, memory, state=None)</span>:</span></span><br><span class="line">    x = self.self_attention(x, attn_bias, state=state)</span><br><span class="line">    x = self.encdec_attention(x, encdec_bias, memory)</span><br><span class="line">    x = self.feed_forward(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这看起来非常简单，只是逐层调用了两个attention和一个ffn而已。</p>
<p>接下来首先看self_attention部分（下面会出现大量的重复attention代码，但是它们的功能和之前提到的会有一些区别）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x: [batch, length_t, hidden]，target端embed的结果或上一层的输出</span></span><br><span class="line"><span class="comment"># bias: [1, 1, length_t, length_t]，下三角为0（包括对角线），上三角为无穷小</span></span><br><span class="line"><span class="comment"># memory: None</span></span><br><span class="line"><span class="comment"># state: 见encoder一节最后部分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, bias, memory=None, state=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">        y = self.layer_norm(x)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y = x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 因为目前处于训练阶段，因此调用attention时传过去的state是None</span></span><br><span class="line">    <span class="keyword">if</span> self.training <span class="keyword">or</span> state <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        y = self.attention(y, bias, memory, <span class="keyword">None</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        kv = [state[<span class="string">"k"</span>], state[<span class="string">"v"</span>]]</span><br><span class="line">        y, k, v = self.attention(y, bias, memory, kv)</span><br><span class="line">        state[<span class="string">"k"</span>], state[<span class="string">"v"</span>] = k, v</span><br><span class="line"></span><br><span class="line">    y = nn.functional.dropout(y, self.dropout, self.training)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">        <span class="keyword">return</span> x + y</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(x + y)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># query: [batch, length_t, hidden]，target端embed的结果或上一层的输出</span></span><br><span class="line"><span class="comment"># bias: [batch, 1, 1, length_s]，下三角为0（包括对角线），上三角为无穷小</span></span><br><span class="line"><span class="comment"># memory: None</span></span><br><span class="line"><span class="comment"># kv: None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, bias, memory=None, kv=None)</span>:</span></span><br><span class="line">    <span class="comment"># 用query计算出q</span></span><br><span class="line">    <span class="comment"># q: [batch, length_t, hidden]</span></span><br><span class="line">    q = self.q_transform(query)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            k, v = kv</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            k, v = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># encoder-decoder attention</span></span><br><span class="line">        k = k <span class="keyword">or</span> self.k_transform(memory)</span><br><span class="line">        v = v <span class="keyword">or</span> self.v_transform(memory)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># memory=None，因此是self-attention，看这部分</span></span><br><span class="line">        <span class="comment"># self-attention</span></span><br><span class="line">        <span class="comment"># 用query直接计算出k和v</span></span><br><span class="line">        <span class="comment"># 我 query 我 自 己</span></span><br><span class="line">        <span class="comment"># k, v: [batch, length_t, hidden]</span></span><br><span class="line">        k = self.k_transform(query)</span><br><span class="line">        v = self.v_transform(query)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># kv=None，因此不用管这段</span></span><br><span class="line">        <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            k = torch.cat([kv[<span class="number">0</span>], k], dim=<span class="number">1</span>)</span><br><span class="line">            v = torch.cat([kv[<span class="number">1</span>], v], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># split heads</span></span><br><span class="line">    <span class="comment"># qh, kh, vh: [batch, heads, length_t, h2]</span></span><br><span class="line">    qh = self.split_heads(q, self.num_heads)</span><br><span class="line">    kh = self.split_heads(k, self.num_heads)</span><br><span class="line">    vh = self.split_heads(v, self.num_heads)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># scale query</span></span><br><span class="line">    qh = qh * (self.hidden_size // self.num_heads) ** <span class="number">-0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># dot-product attention</span></span><br><span class="line">    <span class="comment"># kh: [batch, heads, h2, length_t]</span></span><br><span class="line">    kh = torch.transpose(kh, <span class="number">-2</span>, <span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># logits: [batch, heads, length_t, length_t]</span></span><br><span class="line">    logits = torch.matmul(qh, kh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加入bias</span></span><br><span class="line">    <span class="comment"># bias: [1, 1, length_t, length_t]</span></span><br><span class="line">    <span class="comment"># 这里的bias不是mask bias，而是causal bias，也就是在每个attention矩阵中，</span></span><br><span class="line">    <span class="comment"># token不能attend到未来的token</span></span><br><span class="line">    <span class="comment"># 最后得到的attention是一个下三角矩阵</span></span><br><span class="line">    <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        logits = logits + bias</span><br><span class="line">    <span class="comment"># logits: [batch, heads, length_t, length_t]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对最后一维做softmax</span></span><br><span class="line">    <span class="comment"># weights: [batch, heads, length_t, length_t]</span></span><br><span class="line">    weights = torch.nn.functional.dropout(torch.softmax(logits, dim=<span class="number">-1</span>),</span><br><span class="line">                                            p=self.dropout,</span><br><span class="line">                                            training=self.training)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x: [batch, heads, length_t, h2]</span></span><br><span class="line">    x = torch.matmul(weights, vh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># combine heads</span></span><br><span class="line">    <span class="comment"># output: [batch, length_t, hidden]</span></span><br><span class="line">    output = self.o_transform(self.combine_heads(x))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># kv=None，所以不用管</span></span><br><span class="line">    <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> output, k, v</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>那么问题来了。这里到底为什么要做一个带causal bias的attention呢？</p>
<p>这是因为，对每一个位置来说，它实际能够attend到的部分是已经解码出的部分，不能attend到那些对它来说还没出现的部分。</p>
<p>然后是enc-dec attention：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x: [batch, length_t, hidden]，本层self-attention的输出</span></span><br><span class="line"><span class="comment"># bias: [batch, 1, 1, lenth_s]，source端的masking bias</span></span><br><span class="line"><span class="comment"># memory: [batch, length_s, hidden]，encoder端的输出</span></span><br><span class="line"><span class="comment"># state: None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, bias, memory=None, state=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">        y = self.layer_norm(x)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y = x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.training <span class="keyword">or</span> state <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="comment"># 因为training=True，因此还是调用这里</span></span><br><span class="line">        y = self.attention(y, bias, memory, <span class="keyword">None</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        kv = [state[<span class="string">"k"</span>], state[<span class="string">"v"</span>]]</span><br><span class="line">        y, k, v = self.attention(y, bias, memory, kv)</span><br><span class="line">        state[<span class="string">"k"</span>], state[<span class="string">"v"</span>] = k, v</span><br><span class="line"></span><br><span class="line">    y = nn.functional.dropout(y, self.dropout, self.training)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.normalization == <span class="string">"before"</span>:</span><br><span class="line">        <span class="keyword">return</span> x + y</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(x + y)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># query: [batch, length_t, hidden]，本层self-attention的输出</span></span><br><span class="line"><span class="comment"># bias: [batch, 1, 1, lenth_s]，source端的masking bias</span></span><br><span class="line"><span class="comment"># memory: [batch, length_s, hidden]，encoder端的输出</span></span><br><span class="line"><span class="comment"># kv: None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, bias, memory=None, kv=None)</span>:</span></span><br><span class="line">    <span class="comment"># q: [batch, length_t, hidden]</span></span><br><span class="line">    <span class="comment"># query来自decoder端</span></span><br><span class="line">    q = self.q_transform(query)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            k, v = kv</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            k, v = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># encoder-decoder attention</span></span><br><span class="line">        <span class="comment"># k, v: [batch, length_s, hidden]</span></span><br><span class="line">        <span class="comment"># kv来自encoder端</span></span><br><span class="line">        k = k <span class="keyword">or</span> self.k_transform(memory)</span><br><span class="line">        v = v <span class="keyword">or</span> self.v_transform(memory)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># self-attention</span></span><br><span class="line">        k = self.k_transform(query)</span><br><span class="line">        v = self.v_transform(query)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            k = torch.cat([kv[<span class="number">0</span>], k], dim=<span class="number">1</span>)</span><br><span class="line">            v = torch.cat([kv[<span class="number">1</span>], v], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># split heads</span></span><br><span class="line">    <span class="comment"># qh: [batch, heads, length_t, h2]</span></span><br><span class="line">    <span class="comment"># kh, vh: [batch, heads, length_s, h2]</span></span><br><span class="line">    qh = self.split_heads(q, self.num_heads)</span><br><span class="line">    kh = self.split_heads(k, self.num_heads)</span><br><span class="line">    vh = self.split_heads(v, self.num_heads)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># scale query</span></span><br><span class="line">    qh = qh * (self.hidden_size // self.num_heads) ** <span class="number">-0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># dot-product attention</span></span><br><span class="line">    <span class="comment"># kh: [batch, heads, h2, length_s]</span></span><br><span class="line">    kh = torch.transpose(kh, <span class="number">-2</span>, <span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># logits: [batch, heads, length_t, length_s]</span></span><br><span class="line">    logits = torch.matmul(qh, kh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每行加上source端的masking bias</span></span><br><span class="line">    <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        logits = logits + bias</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对最后一维做softmax</span></span><br><span class="line">    <span class="comment"># weights: [batch, heads, length_t, length_s]</span></span><br><span class="line">    weights = torch.nn.functional.dropout(torch.softmax(logits, dim=<span class="number">-1</span>),</span><br><span class="line">                                            p=self.dropout,</span><br><span class="line">                                            training=self.training)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x: [batch, heads, length_t, h2]</span></span><br><span class="line">    x = torch.matmul(weights, vh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># combine heads</span></span><br><span class="line">    <span class="comment"># output: [batch, length_t, hidden]</span></span><br><span class="line">    output = self.o_transform(self.combine_heads(x))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> kv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> output, k, v</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>回到<code>Transformer.decode</code>的最后一部分：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, features, state, mode=<span class="string">"infer"</span>)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    <span class="comment"># decoder_output: [batch * length_t, hidden]</span></span><br><span class="line">    decoder_output = torch.reshape(decoder_output, [<span class="number">-1</span>, self.hidden_size])</span><br><span class="line">    <span class="comment"># decoder_output: [hidden, batch * length_t]</span></span><br><span class="line">    decoder_output = torch.transpose(decoder_output, <span class="number">-1</span>, <span class="number">-2</span>)</span><br><span class="line">    <span class="comment"># 把每个词的softmax embedding和每个表示做点积，得到每个位置词的概率分布</span></span><br><span class="line">    <span class="comment"># （虽然logits还没有做过处理，不是概率分布）</span></span><br><span class="line">    <span class="comment"># [tvoc_size, hidden] * [hidden, batch * length_t] = [tvoc_size, batch * length_t]</span></span><br><span class="line">    logits = torch.matmul(self.softmax_embedding, decoder_output)</span><br><span class="line">    <span class="comment"># logits: [batch * length_t, tvoc_size]</span></span><br><span class="line">    logits = torch.transpose(logits, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> logits, state</span><br></pre></td></tr></table></figure>
<p>再回到<code>Transformer.forward</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, features, labels, mode=<span class="string">"train"</span>, level=<span class="string">"sentence"</span>)</span>:</span></span><br><span class="line">    ......</span><br><span class="line">    <span class="comment"># 此处state被丢掉了，反正也没用</span></span><br><span class="line">    logits, _ = self.decode(features, state, mode=mode)</span><br><span class="line">    <span class="comment"># labels就是数据处理中target端句子的id，后面加了&lt;eos&gt;</span></span><br><span class="line">    <span class="comment"># logits: [batch * length_t, tvoc_size]</span></span><br><span class="line">    <span class="comment"># labels: [batch, length_t]</span></span><br><span class="line">    <span class="comment"># loss: [batch, length_t]</span></span><br><span class="line">    loss = self.criterion(logits, labels)</span><br><span class="line">    <span class="comment"># mask: [batch, length_t]</span></span><br><span class="line">    mask = mask.to(logits)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 不是eval，就先不管了</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">"eval"</span>:</span><br><span class="line">        <span class="keyword">if</span> level == <span class="string">"sentence"</span>:</span><br><span class="line">            <span class="keyword">return</span> -torch.sum(loss * mask, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>  torch.exp(-loss) * mask - (<span class="number">1</span> - mask)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把loss和mask逐元素相乘，得到mask后的结果，取平均值为loss</span></span><br><span class="line">    <span class="keyword">return</span> torch.sum(loss * mask) / torch.sum(mask)</span><br></pre></td></tr></table></figure>
<p>这里的<code>self.criterion</code>是<a href="https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/modules/losses.py" target="_blank" rel="noopener">losses.py</a>中实现的<code>SmoothedCrossEntropyLoss</code>，在实现的时候会把<code>labels</code>拉平，具体在这里就不展开了。</p>
<p>这之后的处理就先不说了，总之，训练阶段的模型和数据流讲完了。</p>

      </div>
        
          <section class='meta' id="footer-meta">
            <hr>
            <div class='new-meta-box'>
              
                <div class="new-meta-item date" itemprop="dateUpdated" datetime="2020-08-31T12:04:23+00:00">
                  <a class='notlink'>
                    <i class="fas fa-save" aria-hidden="true"></i>
                    2020-08-31
                  </a>
                </div>
              
              
                
                <div class="new-meta-item meta-tags"><a class="tag" href="/tags/THUMT/"><i class="fas fa-hashtag" aria-hidden="true"></i>&nbsp;THUMT</a></div>
              
              
            </div>
          </section>
        

        
            <div class="prev-next">
                
                    <section class="prev">
                        <span class="art-item-left">
                            <h6><i class="fas fa-chevron-left" aria-hidden="true"></i>&nbsp;上一页</h6>
                            <h4>
                                <a href="/post/thumt-code-summary-4/" rel="prev" title="THUMT代码详解（4）：infer阶段模型和数据流">
                                  
                                      THUMT代码详解（4）：infer阶段模型和数据流
                                  
                                </a>
                            </h4>
                            
                                
                                <h6 class="tags">
                                    <a class="tag" href="/tags/THUMT/"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>THUMT</a>
                                </h6>
                            
                        </span>
                    </section>
                
                
                    <section class="next">
                        <span class="art-item-right" aria-hidden="true">
                            <h6>下一页&nbsp;<i class="fas fa-chevron-right" aria-hidden="true"></i></h6>
                            <h4>
                                <a href="/post/thumt-code-summary-2/" rel="prev" title="THUMT代码详解（2）：数据处理">
                                    
                                        THUMT代码详解（2）：数据处理
                                    
                                </a>
                            </h4>
                            
                                
                                <h6 class="tags">
                                    <a class="tag" href="/tags/THUMT/"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>THUMT</a>
                                </h6>
                            
                        </span>
                    </section>
                
            </div>
        

    </section>

</article>

<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->


<br>

<!-- 显示推荐文章和评论 -->



  <article class="post white-box comments">
    <section class="article typo">
      <h4><i class="fas fa-comments fa-fw" aria-hidden="true"></i>&nbsp;评论</h4>
      
      
        <section id="comments">
          <div id="lv-container" data-id="city" data-uid="MTAyMC80MjgyNi8xOTM3Mw==">
            <noscript><div><i class='fas fa-exclamation-triangle'>&nbsp;无法加载Livere评论系统，请确保您的网络能够正常访问。</div></noscript>
          </div>
        </section>
      
      
    </section>
  </article>



<script>
    window.subData = {
        title: 'THUMT代码详解（3）：训练阶段模型和数据流',
        tools: true
    }
</script>


        </div>
        <aside class='l_side'>
            
  
  
    
      
      
        <section class='author'>
  <div class='content pure'>
    
    
    
      <div class="social-wrapper">
        
          
            <a href="mailto:zhanghuimeng1997@gmail.com" class="social flat-btn" target="_blank" rel="external"><i class="social fas fa-envelope" aria-hidden="true"></i></a>
          
        
          
            <a href="https://github.com/zhanghuimeng" class="social flat-btn" target="_blank" rel="external"><i class="social fab fa-github" aria-hidden="true"></i></a>
          
        
          
            <a href="https://music.163.com/#/user/home?id=261028414" class="social flat-btn" target="_blank" rel="external"><i class="social fas fa-music" aria-hidden="true"></i></a>
          
        
      </div>
    
  </div>
</section>

      
    
  
    
      
      
        
  <section class='toc-wrapper'>
    
<header class='pure'>
  <div><i class="fas fa-list fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;文章目录</div>
  
    <div class='wrapper'><a class="s-toc rightBtn" rel="external nofollow noopener noreferrer" href="javascript:void(0)"><i class="fas fa-thumbtack fa-fw"></i></a></div>
  
</header>

    <div class='content pure'>
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-text">模型入口</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-text">encoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-text">decoder</span></a></li></ol>
    </div>
  </section>


      
    
  
    
      
      
        
  <section class='category'>
    
<header class='pure'>
  <div><i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;所有分类</div>
  
</header>

    <div class='content pure'>
      <ul class="entry">
        
          <li><a class="flat-box" title="/categories/Blogging/" href="/categories/Blogging/"><div class='name'>Blogging</div><div class='badge'>(2)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Codeforces/" href="/categories/Codeforces/"><div class='name'>Codeforces</div><div class='badge'>(4)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Leetcode/" href="/categories/Leetcode/"><div class='name'>Leetcode</div><div class='badge'>(32)</div></a></li>
        
          <li><a class="flat-box" title="/categories/MLDS/" href="/categories/MLDS/"><div class='name'>MLDS</div><div class='badge'>(0)</div></a></li>
        
          <li><a class="flat-box" title="/categories/NLP/" href="/categories/NLP/"><div class='name'>NLP</div><div class='badge'>(5)</div></a></li>
        
          <li><a class="flat-box" title="/categories/USACO/" href="/categories/USACO/"><div class='name'>USACO</div><div class='badge'>(5)</div></a></li>
        
          <li><a class="flat-box" title="/categories/博客/" href="/categories/博客/"><div class='name'>博客</div><div class='badge'>(0)</div></a></li>
        
          <li><a class="flat-box" title="/categories/旧博客/" href="/categories/旧博客/"><div class='name'>旧博客</div><div class='badge'>(2)</div></a></li>
        
          <li><a class="flat-box" title="/categories/机器学习/" href="/categories/机器学习/"><div class='name'>机器学习</div><div class='badge'>(2)</div></a></li>
        
          <li><a class="flat-box" title="/categories/深度学习/" href="/categories/深度学习/"><div class='name'>深度学习</div><div class='badge'>(0)</div></a></li>
        
          <li><a class="flat-box" title="/categories/读书笔记/" href="/categories/读书笔记/"><div class='name'>读书笔记</div><div class='badge'>(16)</div></a></li>
        
          <li><a class="flat-box" title="/categories/随笔/" href="/categories/随笔/"><div class='name'>随笔</div><div class='badge'>(3)</div></a></li>
        
      </ul>
    </div>
  </section>


      
    
  
    
      
      
        
  <section class='tagcloud'>
    
<header class='pure'>
  <div><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;热门标签</div>
  
</header>

    <div class='content pure'>
      <a href="/tags/A-Munday/" style="font-size: 14px; color: #999">A.Munday</a> <a href="/tags/Blogging/" style="font-size: 14px; color: #999">Blogging</a> <a href="/tags/C-Marlowe/" style="font-size: 14px; color: #999">C.Marlowe</a> <a href="/tags/CSP/" style="font-size: 15.07px; color: #929292">CSP</a> <a href="/tags/Codeforces/" style="font-size: 19.36px; color: #757575">Codeforces</a> <a href="/tags/Codeforces-Contest/" style="font-size: 19px; color: #777">Codeforces Contest</a> <a href="/tags/Counseling/" style="font-size: 14px; color: #999">Counseling</a> <a href="/tags/Cryptography/" style="font-size: 14px; color: #999">Cryptography</a> <a href="/tags/D-Drayton/" style="font-size: 14px; color: #999">D.Drayton</a> <a href="/tags/Deep-Learning/" style="font-size: 15.07px; color: #929292">Deep Learning</a> <a href="/tags/Depth-first-Search/" style="font-size: 14px; color: #999">Depth-first Search</a> <a href="/tags/Deutsch/" style="font-size: 14px; color: #999">Deutsch</a> <a href="/tags/DigitCircuit/" style="font-size: 14px; color: #999">DigitCircuit</a> <a href="/tags/E-Vere/" style="font-size: 14px; color: #999">E. Vere</a> <a href="/tags/E-Spencer/" style="font-size: 14px; color: #999">E.Spencer</a> <a href="/tags/Essay/" style="font-size: 14.36px; color: #979797">Essay</a> <a href="/tags/Flask/" style="font-size: 14px; color: #999">Flask</a> <a href="/tags/Github/" style="font-size: 14.71px; color: #949494">Github</a> <a href="/tags/GoldenTreasury/" style="font-size: 23.29px; color: #5a5a5a">GoldenTreasury</a> <a href="/tags/Google-Analytics/" style="font-size: 14px; color: #999">Google Analytics</a> <a href="/tags/H-Constable/" style="font-size: 14px; color: #999">H.Constable</a> <a href="/tags/Hexo/" style="font-size: 14px; color: #999">Hexo</a> <a href="/tags/J-Donne/" style="font-size: 14px; color: #999">J.Donne</a> <a href="/tags/J-Lyly/" style="font-size: 14px; color: #999">J.Lyly</a> <a href="/tags/J-Sylvester/" style="font-size: 14px; color: #999">J.Sylvester</a> <a href="/tags/J-Webster/" style="font-size: 14px; color: #999">J.Webster</a> <a href="/tags/Leetcode/" style="font-size: 24px; color: #555">Leetcode</a> <a href="/tags/Leetcode-Contest/" style="font-size: 23.64px; color: #575757">Leetcode Contest</a> <a href="/tags/Lyric/" style="font-size: 17.21px; color: #838383">Lyric</a> <a href="/tags/Machine-Learning/" style="font-size: 19.36px; color: #757575">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 16.5px; color: #888">Machine Translation</a> <a href="/tags/Maths/" style="font-size: 14px; color: #999">Maths</a> <a href="/tags/NLP/" style="font-size: 14px; color: #999">NLP</a> <a href="/tags/Natural-Language-Processing/" style="font-size: 17.21px; color: #838383">Natural Language Processing</a> <a href="/tags/OS/" style="font-size: 21.14px; color: #686868">OS</a> <a href="/tags/OSTEP/" style="font-size: 17.93px; color: #7e7e7e">OSTEP</a> <a href="/tags/Old-Blog/" style="font-size: 14px; color: #999">Old Blog</a> <a href="/tags/OldBlog/" style="font-size: 14.71px; color: #949494">OldBlog</a> <a href="/tags/P-Sidney/" style="font-size: 14px; color: #999">P.Sidney</a> <a href="/tags/PRML/" style="font-size: 18.29px; color: #7c7c7c">PRML</a> <a href="/tags/Paper/" style="font-size: 16.5px; color: #888">Paper</a> <a href="/tags/Paul-Simon/" style="font-size: 14px; color: #999">Paul Simon</a> <a href="/tags/PhysicsExperiment/" style="font-size: 14px; color: #999">PhysicsExperiment</a> <a href="/tags/Psychology/" style="font-size: 14px; color: #999">Psychology</a> <a href="/tags/PyCharm/" style="font-size: 14px; color: #999">PyCharm</a> <a href="/tags/Quality-Estimation/" style="font-size: 15.43px; color: #8f8f8f">Quality Estimation</a> <a href="/tags/R-Barnfield/" style="font-size: 14px; color: #999">R.Barnfield</a> <a href="/tags/Raspberry-Pi/" style="font-size: 14px; color: #999">Raspberry Pi</a> <a href="/tags/Reading-Report/" style="font-size: 17.57px; color: #818181">Reading Report</a> <a href="/tags/S-Daniel/" style="font-size: 14px; color: #999">S.Daniel</a> <a href="/tags/SGU/" style="font-size: 14.36px; color: #979797">SGU</a> <a href="/tags/Sonnet/" style="font-size: 19.71px; color: #727272">Sonnet</a> <a href="/tags/Spokes/" style="font-size: 14.71px; color: #949494">Spokes</a> <a href="/tags/SystemAnalysis-Control/" style="font-size: 14px; color: #999">SystemAnalysis&Control</a> <a href="/tags/T-Dekker/" style="font-size: 14px; color: #999">T.Dekker</a> <a href="/tags/T-Heywood/" style="font-size: 14px; color: #999">T.Heywood</a> <a href="/tags/T-Lodge/" style="font-size: 14px; color: #999">T.Lodge</a> <a href="/tags/T-Nashe/" style="font-size: 14px; color: #999">T.Nashe</a> <a href="/tags/T-Wyatt/" style="font-size: 14px; color: #999">T.Wyatt</a> <a href="/tags/THUMT/" style="font-size: 15.79px; color: #8d8d8d">THUMT</a> <a href="/tags/TensorFlow/" style="font-size: 15.07px; color: #929292">TensorFlow</a> <a href="/tags/Translation/" style="font-size: 18.64px; color: #797979">Translation</a> <a href="/tags/Tree/" style="font-size: 14px; color: #999">Tree</a> <a href="/tags/USACO/" style="font-size: 22.21px; color: #616161">USACO</a> <a href="/tags/W-Alexander/" style="font-size: 14px; color: #999">W.Alexander</a> <a href="/tags/W-Drummond/" style="font-size: 15.07px; color: #929292">W.Drummond</a> <a href="/tags/W-Shakespeare/" style="font-size: 21.5px; color: #666">W.Shakespeare</a> <a href="/tags/WebStorm/" style="font-size: 14px; color: #999">WebStorm</a> <a href="/tags/object-Object/" style="font-size: 14px; color: #999">[object Object]</a> <a href="/tags/alg-Ad-Hoc/" style="font-size: 14.36px; color: #979797">alg:Ad-Hoc</a> <a href="/tags/alg-Aho–Corasick-Algorithm/" style="font-size: 14px; color: #999">alg:Aho–Corasick Algorithm</a> <a href="/tags/alg-Array/" style="font-size: 20.79px; color: #6b6b6b">alg:Array</a> <a href="/tags/alg-Automata/" style="font-size: 14px; color: #999">alg:Automata</a> <a href="/tags/alg-Backtracking/" style="font-size: 15.79px; color: #8d8d8d">alg:Backtracking</a> <a href="/tags/alg-Binary-Indexed-Tree/" style="font-size: 14px; color: #999">alg:Binary Indexed Tree</a> <a href="/tags/alg-Binary-Search/" style="font-size: 16.5px; color: #888">alg:Binary Search</a> <a href="/tags/alg-Binary-Search-Tree/" style="font-size: 16.86px; color: #868686">alg:Binary Search Tree</a> <a href="/tags/alg-Binary-Tree/" style="font-size: 14px; color: #999">alg:Binary Tree</a> <a href="/tags/alg-Binray-Search/" style="font-size: 14px; color: #999">alg:Binray Search</a> <a href="/tags/alg-Bit-Manipulation/" style="font-size: 15.43px; color: #8f8f8f">alg:Bit Manipulation</a> <a href="/tags/alg-Bitmasks/" style="font-size: 14px; color: #999">alg:Bitmasks</a> <a href="/tags/alg-Breadth-First-Search/" style="font-size: 14px; color: #999">alg:Breadth-First Search</a> <a href="/tags/alg-Breadth-first-Search/" style="font-size: 18.29px; color: #7c7c7c">alg:Breadth-first Search</a> <a href="/tags/alg-Breadth-firth-Search/" style="font-size: 14.36px; color: #979797">alg:Breadth-firth Search</a> <a href="/tags/alg-Brute-Force/" style="font-size: 17.21px; color: #838383">alg:Brute Force</a> <a href="/tags/alg-Centroid-Decomposition/" style="font-size: 14px; color: #999">alg:Centroid Decomposition</a> <a href="/tags/alg-Depth-first-Search/" style="font-size: 20.07px; color: #707070">alg:Depth-first Search</a> <a href="/tags/alg-Divide-and-Conquer/" style="font-size: 14px; color: #999">alg:Divide and Conquer</a> <a href="/tags/alg-Dynamic-Porgramming/" style="font-size: 14px; color: #999">alg:Dynamic Porgramming</a> <a href="/tags/alg-Dynamic-Programming/" style="font-size: 22.57px; color: #5f5f5f">alg:Dynamic Programming</a> <a href="/tags/alg-Games/" style="font-size: 14px; color: #999">alg:Games</a> <a href="/tags/alg-Geometry/" style="font-size: 14px; color: #999">alg:Geometry</a> <a href="/tags/alg-Graph/" style="font-size: 15.43px; color: #8f8f8f">alg:Graph</a> <a href="/tags/alg-Greedy/" style="font-size: 21.86px; color: #646464">alg:Greedy</a> <a href="/tags/alg-Hash-Table/" style="font-size: 19.71px; color: #727272">alg:Hash Table</a> <a href="/tags/alg-Heap/" style="font-size: 15.43px; color: #8f8f8f">alg:Heap</a> <a href="/tags/alg-In-Order-Traversal/" style="font-size: 14.36px; color: #979797">alg:In-Order Traversal</a> <a href="/tags/alg-Index-Search-Array/" style="font-size: 14px; color: #999">alg:Index Search Array</a> <a href="/tags/alg-Linked-List/" style="font-size: 15.79px; color: #8d8d8d">alg:Linked List</a> <a href="/tags/alg-Map/" style="font-size: 14px; color: #999">alg:Map</a> <a href="/tags/alg-Math/" style="font-size: 22.93px; color: #5c5c5c">alg:Math</a> <a href="/tags/alg-Matrix/" style="font-size: 14px; color: #999">alg:Matrix</a> <a href="/tags/alg-Meet-in-the-Middle/" style="font-size: 14.36px; color: #979797">alg:Meet in the Middle</a> <a href="/tags/alg-Minimax/" style="font-size: 14.36px; color: #979797">alg:Minimax</a> <a href="/tags/alg-Minmax/" style="font-size: 14px; color: #999">alg:Minmax</a> <a href="/tags/alg-Monotonic-Stack/" style="font-size: 16.14px; color: #8a8a8a">alg:Monotonic Stack</a> <a href="/tags/alg-Network-Flow/" style="font-size: 14px; color: #999">alg:Network Flow</a> <a href="/tags/alg-Priority-Queue/" style="font-size: 14px; color: #999">alg:Priority Queue</a> <a href="/tags/alg-Queue/" style="font-size: 14.71px; color: #949494">alg:Queue</a> <a href="/tags/alg-Rabin-Karp/" style="font-size: 14px; color: #999">alg:Rabin-Karp</a> <a href="/tags/alg-Random/" style="font-size: 14.71px; color: #949494">alg:Random</a> <a href="/tags/alg-Rank-Tree/" style="font-size: 14px; color: #999">alg:Rank Tree</a> <a href="/tags/alg-Recursion/" style="font-size: 15.43px; color: #8f8f8f">alg:Recursion</a> <a href="/tags/alg-Recursive/" style="font-size: 14.36px; color: #979797">alg:Recursive</a> <a href="/tags/alg-Rejection-Sampling/" style="font-size: 14px; color: #999">alg:Rejection Sampling</a> <a href="/tags/alg-Reservoir-Sampling/" style="font-size: 14px; color: #999">alg:Reservoir Sampling</a> <a href="/tags/alg-Segmentation-Tree/" style="font-size: 14px; color: #999">alg:Segmentation Tree</a> <a href="/tags/alg-Set/" style="font-size: 14px; color: #999">alg:Set</a> <a href="/tags/alg-Sliding-Window/" style="font-size: 14px; color: #999">alg:Sliding Window</a> <a href="/tags/alg-Sort/" style="font-size: 15.07px; color: #929292">alg:Sort</a> <a href="/tags/alg-Stack/" style="font-size: 19px; color: #777">alg:Stack</a> <a href="/tags/alg-String/" style="font-size: 19px; color: #777">alg:String</a> <a href="/tags/alg-Suffix-Array/" style="font-size: 14px; color: #999">alg:Suffix Array</a> <a href="/tags/alg-Suffix-Tree/" style="font-size: 14px; color: #999">alg:Suffix Tree</a> <a href="/tags/alg-Ternary-Search/" style="font-size: 14px; color: #999">alg:Ternary Search</a> <a href="/tags/alg-Topological-Sort/" style="font-size: 14px; color: #999">alg:Topological Sort</a> <a href="/tags/alg-Treap/" style="font-size: 14px; color: #999">alg:Treap</a> <a href="/tags/alg-Tree/" style="font-size: 20.43px; color: #6d6d6d">alg:Tree</a> <a href="/tags/alg-Trie/" style="font-size: 14.36px; color: #979797">alg:Trie</a> <a href="/tags/alg-Two-Pointers/" style="font-size: 17.93px; color: #7e7e7e">alg:Two Pointers</a> <a href="/tags/alg-Union-find-Forest/" style="font-size: 15.43px; color: #8f8f8f">alg:Union-find Forest</a> <a href="/tags/artist-Ceremony/" style="font-size: 14px; color: #999">artist:Ceremony</a> <a href="/tags/artist-Cruel-Hand/" style="font-size: 14.36px; color: #979797">artist:Cruel Hand</a> <a href="/tags/artist-Have-Heart/" style="font-size: 14px; color: #999">artist:Have Heart</a> <a href="/tags/artist-Johnny-Cash/" style="font-size: 14px; color: #999">artist:Johnny Cash</a> <a href="/tags/artist-Touche-Amore/" style="font-size: 14px; color: #999">artist:Touche Amore</a> <a href="/tags/artist-Wir-Sind-Helden/" style="font-size: 14.71px; color: #949494">artist:Wir Sind Helden</a> <a href="/tags/translation/" style="font-size: 14.36px; color: #979797">translation</a> <a href="/tags/ucore/" style="font-size: 14px; color: #999">ucore</a> <a href="/tags/付勇林/" style="font-size: 15.79px; color: #8d8d8d">付勇林</a> <a href="/tags/卞之琳/" style="font-size: 14px; color: #999">卞之琳</a> <a href="/tags/屠岸/" style="font-size: 16.14px; color: #8a8a8a">屠岸</a> <a href="/tags/戴镏龄/" style="font-size: 15.79px; color: #8d8d8d">戴镏龄</a> <a href="/tags/曹明伦/" style="font-size: 15.43px; color: #8f8f8f">曹明伦</a> <a href="/tags/朱生豪/" style="font-size: 17.57px; color: #818181">朱生豪</a> <a href="/tags/李霁野/" style="font-size: 15.07px; color: #929292">李霁野</a> <a href="/tags/杨熙龄/" style="font-size: 14px; color: #999">杨熙龄</a> <a href="/tags/林天斗/" style="font-size: 14px; color: #999">林天斗</a> <a href="/tags/梁宗岱/" style="font-size: 16.86px; color: #868686">梁宗岱</a> <a href="/tags/梁葆成/" style="font-size: 14px; color: #999">梁葆成</a> <a href="/tags/袁广达/" style="font-size: 14px; color: #999">袁广达</a> <a href="/tags/郭沫若/" style="font-size: 14px; color: #999">郭沫若</a> <a href="/tags/黄新渠/" style="font-size: 14px; color: #999">黄新渠</a>
    </div>
  </section>


      
    
  
    
      
      
        <section class='list'>
  
<header class='pure'>
  <div><i class="fas fa-link fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;特别链接</div>
  
</header>

  <div class='content pure'>
    <ul class="entry">
      
        <li><a class="flat-box" title="https://wenj.github.io/" href="https://wenj.github.io/">
          <div class='name'>
            
              <i class="fas fa-comment-dots fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;wenj
          </div>
          
        </a></li>
      
        <li><a class="flat-box" title="http://bellasong.site/" href="http://bellasong.site/">
          <div class='name'>
            
              <i class="fas fa-comment-dots fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;ssh
          </div>
          
        </a></li>
      
    </ul>
  </div>
</section>

      
    
  


        </aside>
        <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
    </div>
    <footer id="footer" class="clearfix">
  
  
    <div class="social-wrapper">
      
        
          <a href="mailto:zhanghuimeng1997@gmail.com" class="social fas fa-envelope flat-btn" target="_blank" rel="external"></a>
        
      
        
          <a href="https://github.com/zhanghuimeng" class="social fab fa-github flat-btn" target="_blank" rel="external"></a>
        
      
        
          <a href="https://music.163.com/#/user/home?id=261028414" class="social fas fa-music flat-btn" target="_blank" rel="external"></a>
        
      
    </div>
  
  <br>
  <div><p>博客内容遵循 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</a></p>
</div>
  <div>本站使用 <a href="https://xaoxuu.com/wiki/material-x/" target="_blank" class="codename">Material X</a> 作为主题，总访问量为 <span id="busuanzi_value_site_pv"><i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span> 次。
  </div>
</footer>

    <script>setLoadingBarProgress(80);</script>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js"></script>

  <script>
    var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
    var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
    var ALGOLIA_API_KEY = "";
    var ALGOLIA_APP_ID = "";
    var ALGOLIA_INDEX_NAME = "";
    var AZURE_SERVICE_NAME = "";
    var AZURE_INDEX_NAME = "";
    var AZURE_QUERY_KEY = "";
    var BAIDU_API_ID = "";
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/"||"/";
    if(!ROOT.endsWith('/'))ROOT += '/';
  </script>



  <script async src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.5/dist/scrollreveal.min.js"></script>
  <script type="text/javascript">
    $(function() {
      const $reveal = $('.reveal');
      if ($reveal.length === 0) return;
      const sr = ScrollReveal({ distance: 0 });
      sr.reveal('.reveal');
    });
  </script>


  <script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>
  <script type="text/javascript">
    $(function() {
      Waves.attach('.flat-btn', ['waves-button']);
      Waves.attach('.float-btn', ['waves-button', 'waves-float']);
      Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
      Waves.attach('.flat-box', ['waves-block']);
      Waves.attach('.float-box', ['waves-block', 'waves-float']);
      Waves.attach('.waves-image');
      Waves.init();
    });
  </script>


  <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>







  <script type="text/javascript">
    (function(d, s) {
      var j, e = d.getElementsByTagName(s)[0];
      if (typeof LivereTower === 'function') { return; }
      j = d.createElement(s);
      j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
      j.async = true;
      e.parentNode.insertBefore(j, e);
    })(document, 'script');
  </script>





  <script src="/js/app.js"></script>
<script src="/js/search.js"></script>





<!-- 复制 -->
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  let COPY_SUCCESS = "复制成功";
  let COPY_FAILURE = "复制失败";
  /*页面载入完成后，创建复制按钮*/
  !function (e, t, a) {
    /* code */
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '  <i class="fa fa-copy"></i><span>Copy</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });

      clipboard.on('success', function(e) {
        //您可以加入成功提示
        console.info('Action:', e.action);
        console.info('Text:', e.text);
        console.info('Trigger:', e.trigger);
        success_prompt(COPY_SUCCESS);
        e.clearSelection();
      });
      clipboard.on('error', function(e) {
        //您可以加入失败提示
        console.error('Action:', e.action);
        console.error('Trigger:', e.trigger);
        fail_prompt(COPY_FAILURE);
      });
    }
    initCopyCode();

  }(window, document);

  /**
   * 弹出式提示框，默认1.5秒自动消失
   * @param message 提示信息
   * @param style 提示样式，有alert-success、alert-danger、alert-warning、alert-info
   * @param time 消失时间
   */
  var prompt = function (message, style, time)
  {
      style = (style === undefined) ? 'alert-success' : style;
      time = (time === undefined) ? 1500 : time*1000;
      $('<div>')
          .appendTo('body')
          .addClass('alert ' + style)
          .html(message)
          .show()
          .delay(time)
          .fadeOut();
  };

  // 成功提示
  var success_prompt = function(message, time)
  {
      prompt(message, 'alert-success', time);
  };

  // 失败提示
  var fail_prompt = function(message, time)
  {
      prompt(message, 'alert-danger', time);
  };

  // 提醒
  var warning_prompt = function(message, time)
  {
      prompt(message, 'alert-warning', time);
  };

  // 信息提示
  var info_prompt = function(message, time)
  {
      prompt(message, 'alert-info', time);
  };

</script>


<!-- fancybox -->
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script>
  let LAZY_LOAD_IMAGE = "";
  $(".article-entry").find("img").each(function () {
      var element = document.createElement("a");
      $(element).attr("data-fancybox", "gallery");
      $(element).attr("href", $(this).attr("src"));
      /* 图片采用懒加载处理时,
       * 一般图片标签内会有个属性名来存放图片的真实地址，比如 data-original,
       * 那么此处将原本的属性名src替换为对应属性名data-original,
       * 修改如下
       */
       if (LAZY_LOAD_IMAGE) {
         $(element).attr("href", $(this).attr("data-original"));
       }
      $(this).wrap(element);
  });
</script>





    <script>setLoadingBarProgress(100);</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
