---
title: 多语言预训练相关工作
urlname: 多语言预训练相关工作
toc: true
date: 2019-04-20 15:00:03
updated: 2019-04-20 15:00:03
tags:
categories:
---

目前的评价指标：

* XNLI（[Xnli: Evaluating cross-lingual sentence representations (Conneau et al. 2018)](https://arxiv.org/abs/1809.05053)）：Translate-Train，Translate-Test，Zero-Shot
* 低资源语言模型的perplexity
* 无监督生成word embedding的距离（相同的词在不同语言中的相似度）

## [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al. 2018)](http://arxiv.org/abs/1810.04805)

BERT的原文中并没有谈到多语言的问题，但是后来他们出了一个[多语言模型](https://github.com/google-research/bert/blob/master/multilingual.md)，好像只是单纯地想办法把所有语言都分词，然后用同一个模型进行预训练了。

## [Cross-lingual Language Model Pretraining (Lample et al. 2019)](http://arxiv.org/abs/1901.07291)

Lample等人一直在做这个方面的工作。

### 问题

希望将generative pretraining扩展到多语言预训练上。

### 相关工作

* 预训练
* Transformer模型
* BERT
* 无监督翻译（[Phrase-Based & Neural Unsupervised Machine Translation (Lample et al. 2018)](http://arxiv.org/abs/1804.07755)）

### 方法

这篇文章有两个主要贡献：

1. 改进了在多语言分类任务下训练BERT的方法
2. 将预训练BERT作为无监督翻译模型的初始encoder/decoder，用来提高表现

[Multilingual BERT](https://github.com/google-research/bert/blob/master/multilingual.md)有一个明显的弱点：它的词汇表完全没有优化。这篇文章改为用BPE对词汇表进行预处理。

作者提出了一个使用平行语料的新训练objective，称为TLM（Translation Language Modeling），是在MLM基础上的一个改进：两个输入句子在不同语言下是平行的，仍然随机遮盖任意token；这样模型就可以同时学到预测和不同语言的对齐了。

![TLM和MLM](tlm.png)

然后作者在多语言分类任务下加入了TLM objective，在无监督翻译模型中对整个encoder和decoder（用MLM objective）进行预训练（而不是仅仅用预训练word embedding初始化lookup table）。

### 实验结果

在多语言分类、无监督翻译和有监督翻译任务上取得了SOTA。

多语言分类（无论是否进行fine-tuning）和无监督翻译任务的提升都非常显著，但有监督翻译只写了在罗马尼亚语上有提升，想必在其他语种上没有。。。与此同时，多语言预训练可以增强低资源语言模型的表现和word embedding的性能。
