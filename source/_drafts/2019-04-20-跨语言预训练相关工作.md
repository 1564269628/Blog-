---
title: 多语言预训练相关工作
urlname: multi-lingual-pre-training-works
toc: true
mathjax: true
date: 2019-04-20 15:00:03
updated: 2019-04-20 15:00:03
tags: [NLP]
categories: NLP
description: 缓慢学习中……
---

目前的评价指标：

* XNLI（[Xnli: Evaluating cross-lingual sentence representations (Conneau et al. 2018)](https://arxiv.org/abs/1809.05053)）：Translate-Train，Translate-Test，Zero-Shot
* 语言模型的perplexity
* 无监督生成word embedding的距离（相同的词在不同语言中的相似度）
* GLUE：测试系统在多个任务上的表现（用以测试鲁棒性）

## [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al. 2018)](http://arxiv.org/abs/1810.04805)

代码：[google-research/bert/multilingual.md](https://github.com/google-research/bert/blob/master/multilingual.md)

BERT的原文中并没有谈到多语言的问题，但是后来他们出了一个[多语言模型](https://github.com/google-research/bert/blob/master/multilingual.md)，好像只是单纯地想办法把所有语言都分词，然后用同一个模型进行预训练了。MLM训练目标仍然是针对单独的语言的。

## [Language Models are Unsupervised Multitask Learners (Radford et al. 2018)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

代码：[openai/gpt-2](https://github.com/openai/gpt-2)

### 问题

自然语言处理任务通常需要标注数据，这很麻烦；作者希望模型训练完之后能够直接执行各种任务，不需要再进行fine-tuning。

### 相关工作

* 多任务学习
* 在大规模语料上训练语言模型
* 生成模型学习到的特性
* 如何通过网页创建大规模数据集
* 预训练

### 方法

训练一个足够大的模型，对$p(output|input, task)$进行建模，认为supervised objective和unsupervised objective本质上是一样的，因此只需最小化unsupervised objective。

作者首先爬了Reddit上所有的链接（大约45M个），去掉维基百科（因为这是测试集），处理后得到了大约40G的文本，将这个数据集称为WebText。然后直接用真·经过了一定修正的·byte版本的BPE进行预处理和训练，这使得模型可以输入任何Unicode串。

值得注意的是，作者筛去了非英语的网页。[^trans]

[^trans]: 但是fr-en无监督翻译居然还有11.5 BLEU，真是见了鬼了。。。

模型结构和GPT基本是类似的，但有一定修改，如下图：

![GPT和GPT2的对比](opengpt_structure.png)

### 实验结果

简单来说，在很多任务上的zero-shot测试结果都达到了SOTA。

* language modeling（算PPL之类的[^ppl]）：SOTA 7/8（有8项任务）
* Children’s Book Test（猜词）：SOTA
* LAMBADA（猜词）：SOTA
* Winograd Schema Challenge（推理）：SOTA
* CoQA（阅读理解）：和BERT相差甚远
* Summarization：和baseline相当
* Translation（无监督）：远不如SOTA
* Natural Questions（QA）：巨差

[^ppl]: PPL是我最熟悉的评价指标，不过作者还report了别的，比如enwik8报告的是BPB（bits per byte），text8报告的是BPC（bits per character）。虽然它们听起来像是压缩率，但实际上并不是，它们大概只是不同granularity的PPL。它们之间是可以互相转换的。详情见[Character-Level Language Modeling with Deeper Self-Attention](https://arxiv.org/pdf/1808.04444.pdf)和[How to compute bits per character (BPC)?](https://stats.stackexchange.com/questions/211858/how-to-compute-bits-per-character-bpc)

总的来说，GPT-2的zero-shot表现距离投入使用还有相当的距离。

## [Cross-lingual Language Model Pretraining (Lample et al. 2019)](http://arxiv.org/abs/1901.07291)

Lample等人一直在做这个方面的工作。

[XLM — Enhancing BERT for Cross-lingual Language Model](https://towardsdatascience.com/xlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b)这篇文章讲得很清楚。

### 问题

希望将generative pretraining扩展到多语言预训练上。

### 相关工作

* 预训练
* Transformer模型
* BERT
* 无监督翻译（[Phrase-Based & Neural Unsupervised Machine Translation (Lample et al. 2018)](http://arxiv.org/abs/1804.07755)）

### 方法

这篇文章有两个主要贡献：

1. 改进了在多语言分类任务下训练BERT的方法
2. 将预训练BERT作为无监督翻译模型的初始encoder/decoder，用来提高表现

[Multilingual BERT](https://github.com/google-research/bert/blob/master/multilingual.md)有一个明显的弱点：它的词汇表完全没有优化。这篇文章改为用BPE对词汇表进行预处理。

作者提出了一个使用平行语料的新训练objective，称为TLM（Translation Language Modeling），是在MLM基础上的一个改进：两个输入句子在不同语言下是平行的，仍然随机遮盖任意token；这样模型就可以同时学到预测和不同语言的对齐了。

![TLM和MLM](tlm.png)

然后作者在多语言分类任务下加入了TLM objective，在无监督翻译模型中对整个encoder和decoder（用MLM objective）进行预训练（而不是仅仅用预训练word embedding初始化lookup table）。

### 实验结果

在多语言分类、无监督翻译和有监督翻译任务上取得了SOTA。

多语言分类（无论是否进行fine-tuning）和无监督翻译任务的提升都非常显著，但有监督翻译只写了在罗马尼亚语上有提升，想必在其他语种上没有。。。与此同时，多语言预训练可以增强低资源语言模型的表现和word embedding的性能。
