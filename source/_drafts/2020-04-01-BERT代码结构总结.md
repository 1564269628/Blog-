---
title: BERT代码结构总结
urlname: bert-code-style
toc: true
date: 2020-04-01 02:25:51
updated: 2020-04-01 02:25:51
tags: [BERT, NLP]
categories: 深度学习
---

本文主要包括以下内容：

<!--more-->

* BERT的代码文件功能
* BERT如何处理训练数据
* BERT如何建模
* BERT如何进行预训练
* BERT如何进行精调

## BERT中的代码文件

BERT中共有以下几个主要的代码文件：

* [tokenization.py](https://github.com/google-research/bert/blob/master/tokenization.py)：进行tokenization
* [create_pretraining_data.py](https://github.com/google-research/bert/blob/master/create_pretraining_data.py)：创建MLM/NSP训练目标的TF-Example文件
* [modeling.py](https://github.com/google-research/bert/blob/master/modeling.py)：模型
* [optimization.py](https://github.com/google-research/bert/blob/master/optimization.py)：优化相关
* [run_pretraining.py](https://github.com/google-research/bert/blob/master/run_pretraining.py)：进行预训练
* [extract_features.py](https://github.com/google-research/bert/blob/master/extract_features.py)：从预训练模型中提取feature
* [run_classifier.py](https://github.com/google-research/bert/blob/master/run_classifier.py)：在分类任务上进行fine-tune
* [run_squad.py](https://github.com/google-research/bert/blob/master/run_squad.py)：在SQUAD任务上进行fine-tune

代码文件之间的调用关系如下图所示：

![BERT代码文件之间的调用关系](bert-files.png)

## BERT对训练数据的处理

### BERT Tokenizer

BERT的tokenizer在[tokenization.py](https://github.com/google-research/bert/blob/master/tokenization.py)中。

// TODO

### 训练数据预处理

## BERT的模型

BERT的模型主要定义在[modeling.py](https://github.com/google-research/bert/blob/master/modeling.py)中，其中包含`BertConfig`和`BertModel`两个主要的类，以及一大堆辅助函数。

从`BertModel`的`__init__`函数可以看出，BERT模型是由这3个主要部分组成的：

* embedding：将输入的token序列转换成embedding
* encoder：将embedding输入到一个Transformer encoder中
* pooler：从encoder的输出中取出每个句子的表示

如下图所示：

![BERT模型整体结构](bert-structure-all.png)

### BertConfig类

`BertConfig`类中包含了BERT的主要参数：

* `vocab_size`：词表大小
* `hidden_size=768`：隐藏层的宽度
* `num_hidden_layers=12`：Transformer的层数
* `num_attention_heads=12`：Transformer中每一层attention的head数量
* `intermediate_size=3072`：Transformer中feed-forward层的宽度
* `hidden_act="gelu"`：encoder和pooler中使用的非线性激活函数
* `hidden_dropout_prob=0.1`：embedding、encoder和pooler中所有全连接层的dropout
* `attention_probs_dropout_prob=0.1`：attention的dropout
* `max_position_embeddings=512`：模型能输入的最大序列长度，之后用于positional embedding的计算
* `type_vocab_size=16`：`BertModel`输入中的`token_type_ids`的词表大小，即token最多被分成几类
* `initializer_range=0.02`：所有权重矩阵的初始值的stddev

### BertModel类

BERT模型的主要结构位于`BertModel`的`__init__`函数中。

#### 输入

// TODO: 训练数据预处理还没写

// TODO：我找不到输入参数的reference了。。。

`BertModel.__init__`有3个主要的模型输入参数，从训练数据的预处理中可以得知：

* `input_ids`：经过padding的一个batch的输入序列，形状为`[batch_size, seq_length]`
* `input_mask=None`：形状与`input_ids`相同，真实token的位置为1，padding的位置为0；默认为全1
* `token_type_ids=None`：形状与`input_ids`相同，表示token的类型，通常用于NSP训练中标记前后句，前句标为0，后句标为1；默认为全0

如下图所示：

![BERT的输入](bert-input.png)

其余是一些设置参数：

* `config`：输入一个`BertConfig`实例，用于读取参数
* `is_training`：现在是否为训练状态；控制是否使用dropout
* `use_one_hot_embeddings=False`：是否使用one-hot embedding，默认使用`tf.gather`，节省空间
* `scope=None`：tensorflow参数域的名称，默认为bert（一般没什么用）

#### embedding

embedding分为两步：第一步调用`embedding_lookup`函数，将`input_ids`转换为embedding，也就是一般所说的embedding；第二步调用`embedding_postprocessor`函数，添加positional embedding和token type embedding，最后进行layer norm和dropout。

embedding的结构如下图所示：

![embedding](bert-embeddings.png)

`embedding_lookup`函数假定输入的形状为`[batch_size, seq_length, num_inputs]`（实际输入形状为`[batch_size, seq_length]`，被reshape成了`[batch_size, seq_length, 1]`），然后：

* 创建变量`word_embeddings`（初始化函数为`tf.truncated_normal_initializer`，形状为`[vocab_size, embedding_size]`）
* 将输入reshape为长度为`batch_size*seq_length`的一维向量
* 调用`tf.gather`，得到形状为`[batch_size*seq_length, embedding_size]`的embedding结果
* 将结果重新reshape为`[batch_size, seq_length, embedding_size]`的形状

`embedding_postprocessor`函数输入embedding结果和`token_type_ids`，然后：

* 计算`token_type_embeddings`
  * 创建变量`token_type_table`（初始化函数为`tf.truncated_normal_initializer`，形状为`[token_type_vocab_size, embedding_size]`）
  * 将`token_type_ids` reshape为长度为`batch_size*seq_length`的一维向量，调用`tf.gather`，reshape，得到`token_type_embeddings`，形状为`[batch_size, seq_length, embedding_size]`
* 计算`position_embeddings`
  * 验证`seq_length <= max_position_embeddings`
  * 创建变量`full_position_embeddings`（初始化函数为`tf.truncated_normal_initializer`，形状为`[max_position_embeddings, embedding_size]`），它的大小是固定的，第一维长度为最长的可能序列长度；对于长度比它小的序列，只需取前若干列即可
  * 取出`full_position_embeddings`的前`seq_length`列，记为`position_embeddings`，形状为`[seq_length, embedding_size]`
* 求和：将embedding结果和`token_type_embeddings`相加，对于batch中的每个句子，都加上`position_embeddings`
* layer norm：调用`tf.contrib.layers.layer_norm`实现
* dropout：调用`tf.nn.dropout`实现

最后输出`embedding_output`。

#### encoder

encoder主要分为两个部分：首先创建`attention_mask`，然后将`embedding_output`和`attention_mask`一起输入到`transformer_model`中，最后得到`sequence_output`。

encoder的主要结构如下图所示：

![BERT encoder主要结构](bert-transformer.png)

`create_attention_mask_from_input_mask`函数输入`input_ids`和`input_mask`，输出`attention_mask`：

* 将`input_mask` reshape成`[batch_size, 1, seq_length]`的形状
* 创建全1的tensor，形状为`[batch_size, seq_length, 1]`
* 将两者相乘，得到形状为`[batch_size, seq_length, seq_length]`的`attention_mask`

`attention_mask[b][i][j]`意思是第`b`个句子的第`i`和第`j`个token能否attend，能则为1，不能则为0。最后会得到一些相同的行，这是因为我们并不在意padding token是否会和padding token进行attend。

`transformer_model`函数输入`embedding_output`和`attention_mask`和一些必要的参数：

* 计算`attention_head_size = hidden_size / num_attention_heads`
* 保证`hidden_size = embedding_size`（因为每一层都要进行residual connection）
* 将`embedding_output` reshape成`[batch_size*seq_length, embedding_size]`的形状
* 对于每一个隐藏层（共有`num_hidden_layer`层）
  * 令`layer_input`为上一层的输出（或初始输入）
  * attention部分
    * 调用`attention_layer`函数，得到`attention_output`
    * 将`attention_output`输入到一个`tf.layers.dense`中，宽度为`hidden_size`，初始化函数为`tf.truncated_normal_initializer`
    * 对输出结果进行dropout、residual connection（`layer_input`）和layer norm，输出结果仍记为`attention_output`
  * intermediate部分
    * 将`attention_output`输入到一个`tf.layers.dense`中，宽度为`hidden_size`，初始化函数为`tf.truncated_normal_initializer`，输出`intermediate_output`
  * output部分
    * 将`intermediate_output`输入到一个`tf.layers.dense`中，宽度为`hidden_size`，初始化函数为`tf.truncated_normal_initializer`，输出`layer_output`
    * 对`layer_output`进行dropout、layer norm（`attention_output`）和layer norm
    * 将输出结果记入`all_layer_outputs`中
* 将`all_layer_outputs`中的最后一层或所有层reshape回`[batch_size, seq_length, embedding_size]`的形状，返回

可以看出上述函数的核心部分是`attention_layer`函数。它是对Transformer attention的一个标准实现：



## BERT的预训练
