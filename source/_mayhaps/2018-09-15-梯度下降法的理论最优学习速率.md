---
title: 梯度下降法的理论最优学习速率
urlname: optimal-learning Rate in Gradient Descent
toc: true
date: 2018-09-15 14:27:53
updated: 2018-09-15 14:27:53
tags: [Machine Learning]
---

我很久没学过微积分了，估计这篇文章里会出现一些莫名其妙的说法。不过总的结论是这样的：对于梯度下降法

$$\mathbf{x}_{k+1} = \mathbf{x}_{k} - \alpha \nabla f(\mathbf{x}_{k})$$

最优学习速度的表达式为

$$\alpha^{opt} = \frac{||\nabla f||^2}{\nabla f^T H \nabla f}$$

在一维情形下可以简化为

$$\alpha^{opt} = \frac{1}{f''(x)}$$

## 一维情形下的推导方法

先以$f(x) = ax^2 + bx + c (a < 0)$为例。此时，$f'(x) = 2ax + b$，$f''(x) = 2a$，最小值点位于$x_m = -\frac{b}{2a}$。假设起始点为$x_0$，则通过梯度下降法得到的下一个点为

$$x_1 = x_0 - \alpha f'(x_0)$$

在最优的情况下，从$x_0$一步即可到达$x_min$，因此令上式中的$x_1 = x_m$：

$$x_m = -\frac{b}{2a} = x_0 - \alpha f'(x_0) = x_0 - \alpha (2ax_0 + b)$$

从上式可得，此时

$$\alpha = \frac{1}{2a} = \frac{1}{f'(x_0)}$$

---

在更一般的情况下，将$f(x)$在$x_0$点进行泰勒展开：

$$f(x) = f(x_0) + f'(x_0) (x - x_0) + \frac{1}{2} f''(x_0) (x - x_0)^2 + o((x - x_0)^2)$$

设该凸函数有唯一极小值点（虽然我不知道这个条件是否合适）$x_m$满足$f'(x_m) = 0$。则

$$f(x_1) = f(x_0 - \alpha f'(x_0)) = f(x_0) + f'(x_0) (- \alpha f'(x_0)) + \frac{1}{2} f''(x_0) (- \alpha f'(x_0))^2$$

两侧
