---
title: 读书笔记：《深度学习》第2章：线性代数
urlname: reading-report-deep-learning-chapter-2-linear-algebra
toc: true
mathjax: true
date: 2018-09-28 01:01:56
updated: 2018-09-28 01:01:56
tags: [Reading Report, Deep Learning]
---

书：[Deep Learning](https://www.deeplearningbook.org/)

---

这一章主要就是线性代数复习。我发现里面的东西我大部分都学过。不过最后的推导好像有点难。

## 标量，矢量，矩阵和张量

几个数学概念：

* 标量（scalar）：一个数，通常用斜体小写字母作为变量名称，如$s \in \mathbb{R}$
* 向量（vector）：有顺序的一组数字，通常用粗斜体小写字母作为变量名称；矢量中的元素用斜体小写字母加下标表示。$\boldsymbol{x} \in \mathbb{R}^n$，$\boldsymbol{x} = [x_1, x_2, ..., x_n]^T$
* 矩阵（matrix）：一个二维数组，通常用粗斜体大写字母作为变量名称，如$\boldsymbol{A} \in \mathbb{R}^{m \times n}$。用$A_{1,1}$表示$\boldsymbol{A}$的元素，用$\boldsymbol{A}\_{i,:}$表示$\boldsymbol{A}$的第$i$行，$\boldsymbol{A}\_{:,i}$表示$\boldsymbol{A}$的第$i$列。
* 张量（tensor）：维数大于2的数组，通常用粗体大写字母作为变量名称，如$\mathbb{A}$。用$A_{1, 2, 3}$表示$\mathbb{A}$的元素。

矩阵的转置：$(\boldsymbol{A}^T)\_{i, j} = A_{j, i}$

向量可以被认为是只有一列的矩阵，所以向量的转置是只有一行的矩阵，可以用这种方法书写向量。

维度相同的矩阵可以相加：

$$\boldsymbol{C} = \boldsymbol{A} + \boldsymbol{B} \Rightarrow C_{i,j} = A_{i, j} + B_{i, j}$$

矩阵和标量可以相加或相乘：

$$\boldsymbol{D} = d \cdot \boldsymbol{B} + c \Rightarrow D_{i, j} = d \cdot B_{i, j} + c$$

矩阵和向量也可以相加，即把向量加到矩阵的每一行中，这一方法称为广播（broadcasting）：

$$\boldsymbol{C} = \boldsymbol{A} + b \Rightarrow C_{i, j} = A_{i, j} + b_j$$

## 矩阵乘法和向量乘法

矩阵乘法：

$$\boldsymbol{C} = \boldsymbol{A} \boldsymbol{B} \Rightarrow C_{i, j} = \sum_k A_{i,k} B{k,j}$$

矩阵的元素对应乘积（element-wise product，又称Hadamard product）：$\boldsymbol{A} \odot \boldsymbol{B}$

向量的点积（dot product）：$\boldsymbol{x}^T \boldsymbol{y}$

可以认为矩阵乘法$\boldsymbol{C} = \boldsymbol{A} \boldsymbol{B}$是计算$\boldsymbol{A}$的第$i$行和$\boldsymbol{B}$的第$j$行的点积。

矩阵乘法的一些性质：

* 分配律：$\boldsymbol{A} (\boldsymbol{B} + \boldsymbol{C}) = \boldsymbol{AB} + \boldsymbol{AC}$
* 结合律：$\boldsymbol{A}(\boldsymbol{BC}) = (\boldsymbol{AB})\boldsymbol{C}$
* 点积的交换律：$\boldsymbol{x}^T \boldsymbol{y} = \boldsymbol{y}^T \boldsymbol{x}$
* 乘法和转置：$(\boldsymbol{AB})^T = \boldsymbol{B}^T \boldsymbol{A}^T$

此时我们可以写出一个线性方程组了：

$$\boldsymbol{Ax} = \boldsymbol{b}$$

这个方程组事实上相当于$m$个包含$n$个变量的方程组：

$$\boldsymbol{A}_{1,:} \boldsymbol{x} = b_1$$

$$\boldsymbol{A}_{2,:} \boldsymbol{x} = b_2$$

$$\cdots$$

$$\boldsymbol{A}_{m,:} \boldsymbol{x} = b_m$$

可以更明确地写成

$$A_{1,1}x_1 + A_{1, 2}x_2 + \cdots + A_{1, n} x_n = b_1$$

$$A_{2,1}x_1 + A_{2, 2}x_2 + \cdots + A_{2, n} x_n = b_2$$

$$\cdots$$

$$A_{m,1}x_1 + A_{m, 2}x_2 + \cdots + A_{m, n} x_n = b_m$$

## 单位矩阵和逆矩阵

单位矩阵（identity matrix）：和任何向量都相乘都不改变它的矩阵。$\boldsymbol{I}_n \in \mathrm{R}^{n \ctime n}, \forall \boldsymbol{x} \in \mathrm{R}^n, \boldsymbol{I}_n \boldsymbol{x} = \boldsymbol{x}$

单位矩阵的对角线元素均为1，其余元素均为0。

矩阵$\boldsymbol{A}$的逆矩阵（matrix inverse）：记为$\boldsymbol{A}^{-1}$，定义为满足$\boldsymbol{A}^{-1} \boldsymbol{A} = \boldsymbol{I}_n$的矩阵。

可以通过逆矩阵解线性方程组（当然前提是逆存在）：

$$\boldsymbol{Ax} = \boldsymbol{b}$$

$$\boldsymbol{A}^{-1} \boldsymbol{Ax} = \boldsymbol{A}^{-1} \boldsymbol{b}$$

$$\boldsymbol{I}_n \boldsymbol{x} = \boldsymbol{A}^{-1} \boldsymbol{b}$$

$$\boldsymbol{x} = \boldsymbol{A}^{-1} \boldsymbol{b}$$

## 线性相关和子空间

如果$\boldsymbol{A}^{-1}$存在，则$\boldsymbol{Ax} = \boldsymbol{b}$对于每个$\boldsymbol{b}$都恰好存在一个解。但是有时候，对于有些$\boldsymbol{b}$的值，方程组无解或有无穷多解。

可以通过反证法证明，不存在这样的$\boldsymbol{b}$，使得方程只有大于1个但仍然有限多的解。假设这些解中的两个分别是$\boldsymbol{x}$和$\boldsymbol{y}$，则$\boldsymbol{z} = \alpha \boldsymbol{x} + (1 - \alpha) \boldsymbol{y}$也是方程的解。

不妨把$\boldsymbol{A}$的各**列**看成是从原点出发的向量，则$\boldsymbol{x}$的各个元素表明我们应该在该向量的对应方向上走多远。求解方程组的过程就是确定有多少种方法可以走到$\boldsymbol{b}$：（这个解释很形象生动）

$$\boldsymbol{Ax} = \sum_i x_i \boldsymbol{A}_{:,i}$$

这种操作一般被称为线性组合（linear combination）。

线性组合：对于一组向量${\boldsymbol{v}^{(1)}, \cdots, \boldsymbol{v}^{(n)}}$，它们的线性组合为$\sum_i c_i \boldsymbol{v}^{(i)}$。

一组向量的生成子空间（span）：该组向量的线性组合所能生成的全部点的集合。
