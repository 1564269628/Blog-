---
title: 在Spokes中实现弹性（resilience）
urlname: building-resilience-in-spokes-translation
toc: true
date: 2018-07-05 23:31:15
updated: 2018-07-06 09:51:00
tags: [Github, Spokes, Translation]
---

这篇文章翻译自[Building resilience in Spokes](https://githubengineering.com/building-resilience-in-spokes/)，我加入了一些自己的注解。

---

Spokes是我们的文件服务器的复制系统，我们在里面存储了超过3800万个Git仓库和超过3600万个gists。它至少存储了每个仓库和每个gist的三个副本，这样，即使服务器和网络出现故障，我们也可以提供持久且高可用的内容访问。Spokes使用Git和rsync<a href="#note1" id="note1ref"><sup>1</sup></a>的组合来对存储库进行复制，修复和重新平衡。

## Spokes是什么？
在我们进入这一主题——如何实现弹性——之前，我们需要声明一个新的名字：DGit现在改名为Spokes了。

今年早些时候，我们[宣布了](https://githubengineering.com/introducing-dgit/)我们的应用级Git复制系统，名为“DGit”（“Distributed Git”）。我们得到的反馈表明，“DGit”这个名字的区分度不高，可能会导致与Git项目本身混淆。所以我们决定重命名这个系统为*Spokes*。

## “弹性”的定义
在任何系统或服务中，有两种衡量弹性的关键方法：可用性（availability）和持久性（durability）。系统的可用性指的是系统提供它应当提供的服务所需的运行时间。它可以提供内容吗？它能接受写操作吗？可用性可能是部分的，完整的或退化的：每个仓库都可用吗？是否有一些仓库——或者整个服务器——的访问很缓慢？

系统的持久性指的是它对永久性数据丢失的抵抗能力。一旦系统接受了一个写操作——推送，合并，通过网站进行的编辑，创建新仓库等——它就应该永远不会破坏或回退该内容到之前的状态。这里的关键问题出现在系统接受写入时：需要存储多少副本，以及在哪里存储？显然，必须存储足够数量的副本，才能保证写操作不丢失的可能性足够高。

系统可以是持久但不可用的。例如，如果系统能够为当前写操作制造的副本数量不能超过最低要求，则系统可能会拒绝接受写操作。这样的系统对于写操作是暂时不可用的，不过它同时能够保证不会丢失数据。当然，系统也可以是不持久但可用的。例如，接收任何写入，无论它们是否可以安全地提交，。

读者可能会意识到这与[CAP定理](https://en.wikipedia.org/wiki/CAP_theorem)<a href="#note2" id="note2ref"><sup>2</sup></a>有关。简而言之，系统最多可以满足以下三个特性中的两个：

* 一致性（consistency）：所有节点都读到相同的数据
* 可用性（availability）：系统可以满足读写请求
* 分区容错性（partition tolerance）：即使节点关闭或无法通信，系统也能正常工作

Spokes将一致性和分区容错性放在首位。在最坏的情况下，它将拒绝接受一些写入，对于这些写入，它不能同步提交至至少两个副本。

## 可用性

Spokes的可用性取决于底层服务器和网络的可用性，以及我们检测和绕过服务器和网络问题的能力。

单个服务器经常会变得不可用。自从今年春天开始试用Spokes以来，由于内核死锁和RAM芯片故障，我们的一些服务器崩溃了。有时，由于较轻的硬件故障或较高的系统负载，服务器能够提供的服务退化了。在所有这些情况下，Spokes都必须快速检测出问题并绕过它。每个存储库都复制在三台服务器上，因此即使一台服务器处于脱机状态，也基本总会有一个最新的可用副本可以访问。不过，Spokes可不只是它的每个单独的容错部分的总和。<a href="#note3" id="note3ref"><sup>3</sup></a>

快速检测问题只是第一步。Spokes同时使用心跳服务（heartbeat）<a href="#note4" id="note4ref"><sup>4</sup></a>和实际应用程序流量的组合来确定文件服务器何时停止工作。使用实际应用流量很关键，原因如下。首先，心跳服务的学习和反应速度很慢。我们的每个文件服务器每秒需要处理超过100个请求。如果心跳每秒发生一次，则只有在一百个请求都已经失败后才能发现故障。其次，心跳测试只能覆盖服务器功能的一个子集：例如，服务器是否可以接受TCP连接并响应无操作请求。但是如果失败的情形更微妙呢？如果Git二进制文件已损坏怎么办？如果磁盘访问停止了怎么办？如果所有经过身份验证的操作都失败怎么办？当真正的流量失败时，有时无操作服务仍然能够成功。

因此，Spokes会在处理实际应用程序流量时监视失败情况，如果有太多请求失败，它会将节点标记为脱机。当然，实际请求在正常情况下有时也会失败。例如，有人会尝试读取已经删除的分支，或尝试推送到他们无权访问的分支。因此，Spoke仅仅在三个请求连续失败时才将节点标记为脱机。这有时会导致完全健康的节点脱机——在正常情况下，三个请求也可能会连续失败——但这种情况很少发生，并且导致的代价并不大。

Spokes也使用心跳服务，但不是作为主要的故障检测机制。相反的是，心跳有两个目的：轮询系统负载，并在节点被标记为脱机后提供全清信号（all-clear signal）<a href="#note5" id="note5ref"><sup>5</sup></a>。一旦心跳成功，该节点将再次标记为在线。如果在服务器出现问题的情况下心跳成功（检索系统负载几乎是无操作），则在三次失败的请求之后，节点将再次标记为脱机。

因此，Spokes在节点大约发生三次失败之后就会检测到故障。但是连续三次失败的操作仍然太多了！对于干净的故障——连接被拒绝或超时——所有操作都知道如何尝试下一个主机。请记住，Spokes对于每个仓库都维护了三个或更多的副本。对仓库的路由查询不只返回一个服务器，而是返回按优先顺序排序的三个（大约）最新副本的列表。如果对首选副本上的操作失败，则通常还有至少两个个副本可以尝试。

从一个服务器故障转移到另一个服务器的操作图（此处为远程过程调用（Remote Procedure Call，RPC）<a href="#note6" id="note6ref"><sup>6</sup></a>）清楚地显示了服务器何时脱机。在下图中，有一个服务器在约1.5小时的时间内不可用；在此期间，数千个RPC操作被重定向到其他服务器。这样的图表是Spokes团队发现出错服务器的最佳检测方法。

![一台服务器停机](one-server-down.png)

Spokes的节点离线检测只是建议性的——这只是一种优化。连续三次失败的节点只会被移动到所有读操作的优先顺序列表的末尾，而不是直接从副本列表中移除。尝试一个可能离线的副本还是比根本不进行尝试要好的。

这个故障检测器对服务器故障很有效：当服务器过载或脱机时，对它的操作将失败。Spokes检测到这些故障，并暂时停止将流量定向到故障服务器，直到心跳成功为止。但是，网络和应用程序（Rails）服务器的故障更加混乱。给定的文件服务器可能只对应用程序服务器的一个子集处于脱机状态，而一个出错的应用程序服务器可能会看到每个文件服务器都处于脱机状态的假象。因此，Spokess的故障检测实际上是MxN<a href="#note7" id="note7ref"><sup>7</sup></a>的：每个应用程序服务器都保留自己的脱机文件服务器列表。如果我们发现许多应用程序服务器都将某个文件服务器标记为脱机，那么它可能确实脱机了。而如果我们发现某一个应用程序服务器将许多文件服务器标记为脱机，则发现了一个应用程序服务器的错误。

下图说明了故障检测的MxN特性，并以红色显示，如果文件服务器`dfs4`处于脱机状态，哪些故障检测器会发现错误。<a href="#note8" id="note8ref"><sup>8</sup></a>

![MxN故障检测](mxn.png)

在最近的一次事件中，开发环境中的一个前端应用程序服务器发生了无法解析文件服务器的DNS名称的错误。因为它无法到达文件服务器以向它们发送RPC操作或心跳，所以它得出结论，每个文件服务器都处于脱机状态。但是，只有那台应用程序服务器发生了这些错误；所有其他应用服务器都在正常工作。因此，这台坏掉的应用程序服务器在RPC故障转移图中变得非常明显，并没有没有生产流量因此受到影响。<a href="#note9" id="note9ref"><sup>9</sup></a>

## 持久性
有时，服务器会失效。磁盘可能会失效；RAID控制器可能会失败；甚至整个服务器或整个机架上的全部机器都可能出现故障。即使面对这种逆境，Spokes也为仓库数据提供了持久性。

就像可用性那样，持久性的的实现基础是复制。Spokes至少保留了每个存储库，wiki和gist的三个副本，且这些副本位于不同的机架中。除非严格多数个副本可以应用更改并获得相同的结果，否则不接受对仓库的更新——推送，重命名，编辑维基等。

Spokes只需要一个额外的副本即可避免单节点故障。那么，为什么需要严格多数呢？存储库很可能在大致相同的时间被多次写入，这种情况是很常见的。这些写入可能会发生冲突：例如，一个用户可能会删除一个分支，而另一个用户将新的提交推送到同一个分支。冲突的写入必须被序列化——也就是说，必须在每个副本上以相同的顺序应用（或拒绝）这些写入，这样每个副本才能得到相同的结果。Spokes将写入序列化的方式是确保每次写入都获得对大多数副本的独占锁定。两个写入不可能同时获得多数锁定，因此Spokes通过完全消除并发写入来避免冲突。

如果一个仓库恰好有三个副本上，则在两个副本上的成功写入既保证了持久性，也保证了多数。如果仓库有四个或五个副本，则成功写入需要三个副本。

在很多其他的复制和共识协议（consensus protocols）中，写入到主副本的顺序是官方顺序，所有其他副本都必须按该顺序进行写入。主副本通常需要手动指定，或使用选举协议（election protocol）自动指定。Spokes简单地跳过这一步骤，并将每次写操作都视为一次选举——选择出胜出的顺序之后，可以直接得到写入结果，而不是得到一个能够指示写入顺序的获胜服务器。

无法在多数副本上以相同方式应用的任何写操作都会被Spokes从它被应用的副本上回退。实质上，每个写入操作都需要经过一个投票协议，投票失败方的任何副本都被标记为不健康——不可读取或写入——直到它们被修复。维修是自动和快速的。由于需要多数副本同意接受或回滚更新，在修复不健康的副本时，仍然至少有两个副本可以继续接受读取和写入。

需要明确的一点是，分歧和修复发生的概率是很小的。GitHub每天接受数百万次仓库写入操作。在典型的一天里，几十次写入才会导致一次非一致投票，通常是因为一个副本特别繁忙，到它的连接超时了，而其他副本在没有它的情况下投票成功，继续前进。落后的副本几乎总是在一两分钟内恢复，不会对仓库的可用性造成用户可见的影响。

整个磁盘和整个服务器的故障更罕见，但它们确实会发生。当我们必须移除整个服务器时，突然有数十万个仓库只剩下有两个副本了，而不是三个。这一状况也是可修复的。Spokes会定期检查每个仓库是否具有所需数量的副本；如果没有，则创建更多副本。可以在任何地方创建新副本，并且可以通过每个仓库剩余的两个副本中的任何一个进行复制。因此，服务器故障后的修复是N对N的。文件服务器的集群越大，从单节点故障中恢复的速度就越快。

## 正常关机

如上所述，Spokes可以快速透明地处理服务器脱机和永久失效。那么，我们可以将这一方法直接用于需要计划维护时对服务器的重启或移除吗？是，也不是。

我们的确可以通过`sudo reboot`重新启动服务器，也可以通过直接把服务器拔掉来移除它。但这样做有一些微妙的缺点，因此我们需要设计一种更谨慎的机制，重用一些用于应对崩溃和故障的相同逻辑。

简单地重新启动服务器不会影响未来的读写操作，这些操作将被透明地指向其他副本。它也不影响正在进行的写入操作，因为这些操作发生在所有副本上，而其他两个副本可以直接投票成功并继续写入，不需要我们正在重新启动的服务器上的副本。但重启确实会打断正在进行的读取操作。大多数读取操作——例如，获取README以显示在仓库的主页上——速度都很快，能够在服务器正常关闭之前完成。但有些读取，特别是大型仓库的克隆，取决于最终用户的网速，需要几分钟或几小时才能完成。直接打断这些操作是非常粗鲁的。可以在另一个副本上重新启动这些操作，但到目前为止的所有进度都将丢失。

因此，在Spokes中，为了主动重启一台服务器，我们需要先将它置于静默期（quiescing period）。当服务器处于静默状态时，它对于新的读取操作被标记为脱机，但允许现有的读取操作（包括克隆）完成。静默期可能会持续几秒到几个小时，具体取决于被重启的服务器上哪些读取操作处于活动状态。

可能会令人惊讶的是，写操作像往常一样被发送到服务器，即使它们静默也是如此。这是因为写操作需要在所有副本上运行，因此单个副本可以随时丢弃，不会发生用户可见的影响。此外，如果副本在静默时没有接收到任何写入操作，那么该副本将大大落后于其他副本，当它最终完全重新上线时，时会产生大量的追赶负载（catch-up load）。

我们不在Spokes文件服务器上执行“混乱猴子”（chaos monkey）测试<a href="#note10" id="note10ref"><sup>10</sup></a>，原因与我们在重新启动它们之前要将它们置为静默状态的原因相同：避免中断需要长时间运行的读取操作。也就是说，我们不会仅仅为了确认突发的单节点故障仍然（在大多数情况下）是无害的而随机重启文件服务器。

虽然我们不执行“混乱猴子”测试，我们仍然会按需要对服务器轮流进行重启，这实现了大致相同的测试目标。当我们需要进行一些需要重启的更改时——比如更改内核或文件系统参数，或更改BIOS设置——我们会将这些服务器置于静默状态并重启它们。我们将机架作为可用性区域<a href="#note11" id="note11ref"><sup>11</sup></a>，因此我们一次将整个机架置于静默状态。当给定机架中的服务器结束静默状态——即完成所有未完成的读取操作——我们分批重启这些服务器，每次最多五个。整个机架重启结束后，我们继续前进到下一个机架。

下图显示了在轮流重启期间失败的RPC操作。用不同的颜色标记每个服务器。值是堆叠的，因此在最高的峰值表示的时刻中，八个服务器在同时重启。浅红色块表示一台服务器未能正常重启，因此离线了大约两个小时。

![滚动重启](rolling-reboots.png)

用直接插拔的方法移除服务器的弊端与计划外重启的弊端类似。除了会中断任何正在进行的读取操作外，这种行为还会为在这台服务器上托管的所有仓库带来几个小时的额外风险。当一台服务器突然消失时，之前存储在里面的所有仓库现在都只剩下两个副本了。两个副本足够执行任何读取或写入操作，但两个副本无法承受额外的故障。换句话说，在没有警告的情况下就删除服务器，这样会增加在同一天晚些时候写入操作被拒绝的概率。我们的目标是将这种可能性保持在最低水平。

因此，在准备移除一台服务器之前，我们不再把它存储的仓库副本作为任何仓库的活跃副本。Spokes仍然可以使用该服务器进行读写操作。但当它询问是否所有仓库都有足够的副本时，其中一些仓库——有副本位于将被移除的服务器上的那些 ——将声称不够，然后创建更多的副本。修复这一问题的过程类似于服务器直接消失后的修复过程，不过，区别在于，现在服务器仍然可用，以防其他服务器出现故障。

## 结论
可用性是很重要的，而持久性甚至更为重要。可用性衡量的是服务响应请求的时间长度<a href="#note12" id="note12ref"><sup>12</sup></a>。持久性衡量的是，服务能够可信地存储输入数据中的多少。

为了提供可用性和持久性，Spokes为每个仓库至少保留了三个副本。三个副本意味着，即使一个服务器失效，也不会对用户产生可见的影响。如果两个服务器都发生了故障，Spokes仍然可以为大部分仓库提供完全的访问权限，并为那些恰好有两个副本存储在这两个故障服务器上的存储库提供只读访问。

Spokes只在大多数副本——一般至少为两个——能够提交写入并得到相同的仓库状态时才接受对仓库的写入，这一要求通过确保所有副本上的写入顺序相同提供了一致性。通过在至少两个位置存储每个已提交的写入，它也可以在单个​​服务器发生故障时提供持久性。

Spokes的故障检测器通过监视实时应用程序流量，确定服务器何时脱机并绕过该问题。最后，Spokes具有自动修复功能，可在磁盘或服务器发生永久性故障时快速恢复。

## 注解

<a id="note1" href="#note1ref"><sup>1</sup></a>关于[rsync](https://zh.wikipedia.org/wiki/Rsync)：
>rsync是Unix下的一款应用软件，它能同步更新两处计算机的文件与目录，并适当利用差分编码以减少数据传输量。rsync中的一项同类软件不常见的重要特性是每个目标的镜像只需发送一次。rsync可以拷贝／显示目录内容，以及拷贝文件，并可选压缩以及递归拷贝。
在常驻模式（daemon mode）下，rsync默认监听TCP端口873，以原生rsync传输协议或者通过远程shell如RSH或者SSH提供文件。SSH模式下，rsync客户端运行程序必须同时在本地和远程机器上安装。
rsync是以GNU通用公共许可证发行的自由软件。

<a id="note2" href="#note2ref"><sup>2</sup></a>[CAP定理](https://zh.wikipedia.org/wiki/CAP%E5%AE%9A%E7%90%86)：
>在理论计算机科学中，**CAP定理**（CAP theorem），又被称作**布鲁尔定理**（Brewer's theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：
>
* 一致性（**C**onsistence） （等同于所有节点访问同一份最新的数据副本）
* 可用性（**A**vailability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据）
* 分区容错性（**P** artition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。）
根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项。理解CAP理论的最简单方式是想象两个节点分处分区两侧。允许至少一个节点更新状态会导致数据不一致，即丧失了C性质。如果为了保证数据一致性，将分区一侧的节点设置为不可用，那么又丧失了A性质。除非两个节点可以互相通信，才能既保证C又保证A，这又会导致丧失P性质。

<a id="note3" href="#note3ref"><sup>3</sup></a>不知道这个地方在说什么。是说Spokes能提供的功能远多于冗余性吗？

<a id="note4" href="#note4ref"><sup>4</sup></a>heartbeat的定义（[http://blog.51cto.com/hoolee/1408615](http://blog.51cto.com/hoolee/1408615)）：
>Heartbeat 项目是Linux-HA工程的一个组成部分，它实现了一个高可用集群系统。心跳服务和集群通信是高可用集群的两个关键组件，在 Heartbeat 项目里，由 heartbeat 模块实现了这两个功能。
heartbeat（Linux-HA）的工作原理：heartbeat最核心的包括两个部分，心跳监测部分和资源接管部分，心跳监测可以通过网络链路和串口进行，而且支持冗 余链路，它们之间相互发送报文来告诉对方自己当前的状态，如果在指定的时间内未收到对方发送的报文，那么就认为对方失效，这时需启动资源接管模块来接管运行在对方主机上的资源或者服务。

<a id="note5" href="#note5ref"><sup>5</sup></a>所谓“[All clear](https://en.wikipedia.org/wiki/All_clear)”是一种防空警报，它的含义是空袭已经结束，民众可以离开防空洞。

<a id="note6" href="#note6ref"><sup>6</sup></a>[远程过程调用](https://zh.wikipedia.org/wiki/%E9%81%A0%E7%A8%8B%E9%81%8E%E7%A8%8B%E8%AA%BF%E7%94%A8)：
>远程过程调用（英语：Remote Procedure Call，缩写为 RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用，例：Java RMI。

<a id="note7" href="#note7ref"><sup>7</sup></a>我没有完全理解M*N是什么意思。不过我猜测，这指的是错误是可以双向检测的。

<a id="note8" href="#note8ref"><sup>8</sup></a>这张图大概显示了3个fetch请求，一个远端git worker请求和一个web api请求。

<a id="note9" href="#note9ref"><sup>9</sup></a>此处令人思考的是，这一故障检测是如何实现的。我们需要了解请求是否会失败，以及一些能够进行请求的非文件服务器记录下的请求失败情况。通过其他服务器的请求情况，实际上，我们可以确定这些服务器的实际运行状况。而RPC图大概是需要通过收集所有请求的实际状况来绘制的。绘制完成之后，Spokes系统可能会根据某种策略自动进行结点状况判断，也可以绘制成实时状态图，让人类判断里面发生错误的结点。

<a id="note10" href="#note10ref"><sup>10</sup></a>[Chaos monkey](https://github.com/Netflix/SimianArmy/wiki/Chaos-Monkey)：
>Chaos Monkey是一种识别系统组并随机终止组中某个系统的服务。该服务在受控时间（不在周末和假日运行）和间隔（仅在工作时间运行）运行。在大多数情况下，我们将应用程序设计为在对等设备脱机时继续工作，但在这些特殊情况下，我们希望确保周围有人来解决和学习任何问题。考虑到这一点，Chaos Monkey只在工作时间运行，其目的是让工程师保持警觉并能够做出响应。

（其实就是在系统里自动造成随机失败，增加工程师的警觉性）

<a id="note11" href="#note11ref"><sup>11</sup></a>（原注）将机架作为可用性区域处理意味着我们放置仓库副本的时候需要保证同一机架中不会存储同一存储库的两个副本。这样就可以保证，即使丢失了整个服务器机架，也不会影响托管在里面的任何仓库的可用性或持久性。我们选择机架作为可用性区域，是因为几种重要的故障模式（failure mode），特别是与电源和网络相关的故障模式，可能会同时影响整个服务器机架。

>“可用性区域”（Availability Zone）由独立的数据中心构成，每个地区都是一个数据中心集群，这些数据中心之间距离足够近，这样可以保证数据库等应用的延迟足够低，但又足够远，这样可以防止出现意外时同时宕机，例如亚马逊在日本的数据中心就分布在不同的地震区。（[香港：亚马逊云计算全球化布局下一站？](https://ctocio.com/ccnews/5685.html)）

<a id="note12" href="#note12ref"><sup>12</sup></a>所以这个是单次请求所需的时间长度，和服务器自己的运行时间没有关系？那么，看来前面搞错了……
