---
title: PRML读书笔记：5.1 前馈神经网络
urlname: prml-chap-5-1-feed-forward-network-functions
toc: true
mathjax: true
date: 2020-02-18 11:19:46
updated: 2020-02-18 11:19:46
tags: [PRML, Machine Learning]
categories: 读书笔记
---

### 神经网络的基本结构

基本的神经网络结构：由两层组成

<!--more-->

![神经网络结构图](network-structure.png)

输入层的表达式为

$$z_j = h(a_j) = h(\sum_{i=0}^D w_{ji}^{(1)}x_i)$$

其中上标(1)表示对应的参数是第一层的参数，$w_{ji}^{(1)}$为权重（weight），$w_{j0}^{(1)}$为偏置（bias），$x_0=1$（这是整合偏置参数的方法），$h$是可微的非线性激活函数（activation function），通常选用logistic sigmoid函数或者双曲正切函数。

输出层的表达式为

$$y_k = h'(a_k) = h'(\sum_{j=0}^M w_{kj}^{(2)}z_j)$$

其中$h'$为激活函数，由数据和目标变量的假定的分布确定，如：

* 回归问题：使用恒等函数
* 多二元分类问题：使用logistic sigmoid函数，即$\sigma(a) = \frac{1}{1+\exp{(-a)}}$
* 多分类问题：使用softmax函数，即$p(C_k|x) = \frac{\exp{(a_k)}}{\sum_j \exp{(a_j)}}$

整体网络函数为

$$
y_{k}(\boldsymbol{x}, \boldsymbol{w})=\sigma\left(\sum_{j=0}^{M} w_{k j}^{(2)} h\left(\sum_{i=0}^{D} w_{j i}^{(1)} x_{i}\right)\right)
$$

其中$\boldsymbol{w}$是所有权重和偏置的集合。

神经网络模型可以看做是一个从输入变量$\{x_i\}$到输出变量$\{y_k\}$的非线性函数，并且由可调节参数向量$\boldsymbol{w}$控制。

### 神经网络的性质和扩展

* 神经网络和感知器的区别和联系：神经网络的每一层都类似于感知器，区别是神经网络的激活函数使用**可微的**连续函数，而感知器的激活函数使用阶梯函数
* 激活函数全都取线性函数的网络相当于线性变换，没有意义
* 神经网络的扩展：只要是前馈结构即可
  * 增加更多层数（形似第二层）
  * 引入跨层连接
  * 稀疏网络（如CNN）

### 神经网络的近似能力

神经网络具有通用的近似性。通用近似定理（Universal approximation theorem）表明，只要隐藏单元数量足够多，对于很多激活函数（多项式函数除外），一个带有线性输入的两层网络可以在任意精度下近似任何输入变量较少的函数。

**例：用神经网络对4个不同函数进行近似**

![图5.3：用多层感知机近似不同函数](figure-5-3.png)

[程序代码](https://github.com/zhanghuimeng/prml-code/blob/master/chp_05/05-01_01_approximator.py)

![程序运行结果](05-01_01_approximator.png)

**例：用神经网络进行二分类**

![图5.4：简单的二分类问题](figure-5-4.png)

// TODO

## 5.1.1 权重空间的近似性

对于不同的权重向量$\boldsymbol{w}$，网络可能产生同样的从输入到输出的映射函数。这一性质通常没有实际用处。
