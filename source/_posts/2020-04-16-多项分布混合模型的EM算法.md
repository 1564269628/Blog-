---
title: 多项分布混合模型的EM算法
urlname: em-for-mixture-of-multinomials
toc: true
mathjax: true
date: 2020-04-16 20:31:25
updated: 2020-04-16 20:31:25
tags:
categories: 机器学习
---

本文将介绍如何用EM算法求解多项分布混合模型（Mixture of Multinomials）。

<!--more-->

## EM算法的基本步骤

给定观测变量$\boldsymbol{X}$和隐变量$\boldsymbol{Z}$上的一个联合概率分布$p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})$，由参数$\boldsymbol{\theta}$控制，目标是关于$\boldsymbol{\theta}$最大化似然函数$p(\boldsymbol{X} | \boldsymbol{\theta})$：

* 初始化参数$\boldsymbol{\theta}$，记为$\boldsymbol{\theta}^{\text{old}}$
* E步骤：计算$\boldsymbol{Z}$在$\boldsymbol{\theta}^{\text{old}}$下的后验分布$p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\theta}^{\text{old}})$
* M步骤：计算$\boldsymbol{\theta}^{\text{new}}$
  * $\mathcal{Q}\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}}\right)=\sum_{\boldsymbol{Z}} p\left(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\theta}^{\text{old}}\right) \ln p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})$（对数似然的期望）
  * $\boldsymbol{\theta}^{\text{new}}=\underset{\boldsymbol{\theta}}{\arg \max } \mathcal{Q}\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}}\right)$（最大化期望）
* 检查对数似然函数或者参数值的收敛性。如果不满足收敛准则，则令$\boldsymbol{\theta}^{\text{old}} = \boldsymbol{\theta}^{\text{new}}$，回到E步骤

## 多项混合模型

假定我们共有$T$个文档，词表大小为$W$，$T_{dw}$表示文档$d$中词$w$出现的次数。每个文档都有一个主题$c_d$，主题共有$K$种，满足分布

$$
P(c_d = k) = \pi_k, \, k = 1, 2, ..., K
$$

每个主题下的词汇都满足多项分布$Mult(\boldsymbol{\mu} _ k)$，其中$\boldsymbol{\mu} _ k = (\mu_{k1}, \cdots, \mu _ {kW})$。因此文档$d$的主题为$k$时，词汇分布的概率为

$$
P(d | c_d = k) = \frac{n_d!}{\prod_{w=1}^W T_{dw}!} \prod_{w=1}^W \mu_{kw}^{T_{dw}}, \, n_d = \sum_{w=1}^W T_{dw}
$$

因此文档$d$总的概率分布为

{% raw %}
$$
\begin{aligned}
P(d) &= \sum_{k=1}^K P(d|c_d=k)P(c_d=k) \\
&= \frac{n_d!}{\prod_{w=1}^W T_{dw}!} \sum_{k=1}^K \pi_k \prod_{w=1}^W \mu_{kw}^{T_{dw}}
\end{aligned}
$$
{% endraw %}

## 用EM求解多项混合模型

### E步骤

此时$d$相当于观测变量，$c_d$相当于隐变量。首先通过贝叶斯公式求解$P(c_d | d)$。

{% raw %}
$$
\begin{aligned}
P(d, c_d = k) &= P(d | c_d = k) P(c_d = k) \\
&= \frac{n_d!}{\prod_{w=1}^W T_{dw}!} \pi_k \prod_{w=1}^W \mu_{kw}^{T_{dw}}
\end{aligned}
$$

$$
\begin{aligned}
P(c_d = k | d) &= \frac{P(d, c_d = k)}{P(d)} \\
&= \frac{\frac{n_d!}{\prod_{w=1}^W T_{dw}!} \pi_k \prod_{w=1}^W \mu_{kw}^{T_{dw}}}{\frac{n_d!}{\prod_{w=1}^W T_{dw}!} \sum_{j=1}^K \pi_j \prod_{w=1}^W \mu_{jw}^{T_{dw}}} \\
&= \frac{\pi_k \prod_{w=1}^W \mu_{kw}^{T_{dw}}}{\sum_{j=1}^K \pi_j \prod_{w=1}^W \mu_{jw}^{T_{dw}}}
\end{aligned}
$$
{% endraw %}

记$\gamma_{dk} = P(c_d = k | d)$，在接下来的M步骤里，这一项是固定的。

### M步骤

接下来求解对数似然的期望。首先，单个文档的对数似然为（省略常数项，令$\boldsymbol{D}$表示文档集合，$\boldsymbol{C}$表示主题集合）：

{% raw %}
$$
\log{P(d, c_d)} = \log{\pi_{c_d}} + \sum_{w=1}^W T_{dw} \log{\mu_{c_d w}}
$$
$$
\log{P(\boldsymbol{D}, \boldsymbol{C})} = \log{\prod_{d=1}^D} P(d, c_d) = \sum_{d=1}^D \log{P(d, c_d)}
$$
{% endraw %}

对数似然的期望为

{% raw %}
$$
\begin{aligned}
\mathbb{E}_{\boldsymbol{C}} [ \log{P(\boldsymbol{D}, \boldsymbol{C})} ] &= \sum_{d=1}^D \mathbb{E}_{\boldsymbol{C}} [\log{P(d, c_d)}] \\
&= \sum_{d=1}^D \sum_{k=1}^K \log{P(d, c_d=k)} P(c_d = k | d) \\
&= \sum_{d=1}^D \sum_{k=1}^K \gamma_{dk} \left[\log{\pi_{k}} + \sum_{w=1}^W T_{dw} \log{\mu_{kw}}\right]
\end{aligned}
$$
{% endraw %}

接下来需要对$\boldsymbol{\pi}$和$\boldsymbol{\mu}$最大化$\mathbb{E}_{\boldsymbol{C}} [ \log{P(\boldsymbol{D}, \boldsymbol{C})} ]$。由于有限制条件

{% raw %}
$$
\begin{aligned}
\sum_{k=1}^K \pi_k &= 1 \\
\sum_{w=1}^W \mu_{kw} &= 1, \, k=1, \cdots, K
\end{aligned}
$$
{% endraw %}

因此需要用拉格朗日乘数法进行优化。令

{% raw %}
$$
\begin{aligned}
L &= \mathbb{E}_{\boldsymbol{C}} [ \log{P(\boldsymbol{D}, \boldsymbol{C})} ] - \lambda_0 \left(\sum_{k=1}^K \pi_k - 1\right) - \sum_{k=1}^K \lambda_k \left(\sum_{w=1}^W \mu_{kw} - 1\right)
\end{aligned}
$$
{% endraw %}

由于

{% raw %}
$$
\begin{aligned}
\frac{\partial L}{\partial \pi_k} = \frac{\sum_{d=1}^D \gamma_{dk}}{\pi_k} - \lambda_0 &= 0 \\
\sum_{k=1}^K \pi_k &= 1
\end{aligned}
$$
{% endraw %}

可以求得

{% raw %}
$$
\begin{aligned}
\lambda_0 &= \sum_{d=1}^D \sum_{k=1}^K \gamma_{dk} \\
\pi_k &= \frac{\sum_{d=1}^D \gamma_{dk}}{\sum_{d=1}^D \sum_{k=1}^K \gamma_{dk}}
\end{aligned}
$$
{% endraw %}

由于

{% raw %}
$$
\begin{aligned}
\frac{\partial L}{\partial \mu_{kw}} = \frac{\sum_{d=1}^D \gamma_{dk} T_{dw}}{\mu_{kw}} - \lambda_k &= 0 \\
\sum_{w=1}^W \mu_{kw} &= 1
\end{aligned}
$$
{% endraw %}

可以求得

{% raw %}
$$
\begin{aligned}
\lambda_k &= \sum_{d=1}^D \sum_{w=1}^W \gamma_{dk} T_{dw} \\
\mu_{kw} &= \frac{\sum_{d=1}^D \gamma_{dk} T_{dw}}{\sum_{d=1}^D \sum_{w=1}^W \gamma_{dk} T_{dw}}
\end{aligned}
$$
{% endraw %}

综上，M步骤更新参数的方式为

{% raw %}
$$
\begin{aligned}
\pi_k &= \frac{\sum_{d=1}^D \gamma_{dk}}{\sum_{d=1}^D \sum_{k=1}^K \gamma_{dk}} \\
\mu_{kw} &= \frac{\sum_{d=1}^D \gamma_{dk} T_{dw}}{\sum_{d=1}^D \sum_{w=1}^W \gamma_{dk} T_{dw}}
\end{aligned}
$$
{% endraw %}

## 代码

用EM算法在[20 Newsgroups dataset](http://ml.cs.tsinghua.edu.cn/~shuyu/sml/20news.zip)上求解。

算法链接：[multinomial-mixture-em](https://github.com/zhanghuimeng/multinomial-mixture-em)

代码中有一些需要注意的地方：

* 由于不完整数据集的最大似然函数难以计算，所以就没有去求它，但如果求了，应该可以看出它的值在不断增大
* 在求解过程中，$\gamma_{dk} = \frac{\pi_k \prod_{w=1}^W \mu_{kw}^{T_{dw}}}{\sum_{j=1}^K \pi_j \prod_{w=1}^W \mu_{jw}^{T_{dw}}}$可能会下溢，可以先对分子取log，同时减去最大值之后再做exp，最后再对整个$\gamma_{d, \cdot}$取平均值
* 在求解过程中，$\mu_{kw}$可能会下溢到0，可以添加一个很小的数来规避

## 参考文献

* PRML第9章
* [Mixture Models and Expectation-Maximization](https://www.cs.princeton.edu/courses/archive/spring12/cos424/pdf/em-mixtures.pdf)
