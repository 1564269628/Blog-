---
title: THUMT代码详解（2）：数据处理
urlname: thumt-code-summary-2
toc: true
date: 2020-08-31 11:47:29
updated: 2020-08-31 11:47:29
tags: THUMT
categories: NLP
---

[简介篇地址](/post/thumt-code-summary-1)

<!--more-->

这篇分为以下三个部分：训练集数据输入、验证集数据输入、测试集数据输入。

## 训练集数据输入

[trainer.py](https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/bin/trainer.py)中，`dataset = data.get_dataset(params.input, "train", params)`一行创建了训练用的数据集。具体的创建方式是：

```py
def get_dataset(filenames, mode, params):
    input_fn = build_input_fn(filenames, mode, params)

    with tf.device("/cpu:0"):
        dataset = input_fn()

    return dataset
```

以下列数据为例，说明`train_input_fn`对输入数据的处理：

```txt
file 0: I am a student .
file 1: Ich bin ein Student .
```

```py
def train_input_fn():
    src_dataset = tf.data.TextLineDataset(filenames[0])
    tgt_dataset = tf.data.TextLineDataset(filenames[1])
    # src_dataset: ["I am a student .", ...]
    # tgt_dataset: ["Ich bin ein Student .", ...]
    # 注：请把最外层的中括号理解成dataset结构

    dataset = tf.data.Dataset.zip((src_dataset, tgt_dataset))
    dataset = dataset.shard(torch.distributed.get_world_size(),
                            torch.distributed.get_rank())
    dataset = dataset.prefetch(params.buffer_size)
    dataset = dataset.shuffle(params.buffer_size)
    # dataset: [("I am a student .", "Ich bin ein Student ."), ...]

    # Split string
    dataset = dataset.map(
        lambda x, y: (tf.strings.split([x]).values,
                        tf.strings.split([y]).values),
        num_parallel_calls=tf.data.experimental.AUTOTUNE)
    # dataset: [(["I", "am", "a", "student", "."], ["Ich". "bin", "ein", "Student", "."]), ...]

    # Append BOS and EOS
    dataset = dataset.map(
        lambda x, y: (
            (tf.concat([x, [tf.constant(params.eos)]], axis=0),
                tf.concat([[tf.constant(params.bos)], y], axis=0)),
            tf.concat([y, [tf.constant(params.eos)]], axis=0)),
        num_parallel_calls=tf.data.experimental.AUTOTUNE)
    # dataset: [((["I", "am", "a", "student", ".", "<eos>"], ["<bos>", "Ich". "bin", "ein", "Student", "."]),
    #           ["Ich". "bin", "ein", "Student", ".", "<eos>"]), ...]

    dataset = dataset.map(
        lambda x, y: ({
            "source": x[0],
            "source_length": tf.shape(x[0])[0],
            "target": x[1],
            "target_length": tf.shape(x[1])[0]
        }, y),
        num_parallel_calls=tf.data.experimental.AUTOTUNE)
    # dataset = [({
    #     "source": ["I", "am", "a", "student", ".", "<eos>"],
    #     "source_length": 6,
    #     "target": ["<bos>", "Ich". "bin", "ein", "Student", "."],
    #     "target_length": 6,
    #     }, ["Ich". "bin", "ein", "Student", ".", "<eos>"]), ...]

    def bucket_boundaries(max_length, min_length=8, step=8):
        x = min_length
        boundaries = []

        while x <= max_length:
            boundaries.append(x + 1)
            x += step

        return boundaries

    batch_size = params.batch_size
    max_length = (params.max_length // 8) * 8
    min_length = params.min_length
    boundaries = bucket_boundaries(max_length)
    batch_sizes = [max(1, batch_size // (x - 1))
                    if not params.fixed_batch_size else batch_size
                    for x in boundaries] + [1]

    def element_length_func(x, y):
        return tf.maximum(x["source_length"], x["target_length"])

    def valid_size(x, y):
        size = element_length_func(x, y)
        return tf.logical_and(size >= min_length, size <= max_length)

    transformation_fn = tf.data.experimental.bucket_by_sequence_length(
        element_length_func,
        boundaries,
        batch_sizes,
        padded_shapes=({
            "source": tf.TensorShape([None]),
            "source_length": tf.TensorShape([]),
            "target": tf.TensorShape([None]),
            "target_length": tf.TensorShape([])
            }, tf.TensorShape([None])),
        padding_values=({
            "source": params.pad,
            "source_length": 0,
            "target": params.pad,
            "target_length": 0
            }, params.pad),
        pad_to_bucket_boundary=False)

    dataset = dataset.filter(valid_size)
    # 分为batch，进行padding
    dataset = dataset.apply(transformation_fn)
    # 注：下面只写形状，不写具体的值了
    # dataset = [({
    #     "source": [batch, length],
    #     "source_length": 6,
    #     "target": [batch, length],
    #     "target_length": 6,
    #     }, [batch, length]), ...]

    dataset = dataset.map(
        lambda x, y: ({
            "source": x["source"],
            "source_mask": tf.sequence_mask(x["source_length"],
                                            tf.shape(x["source"])[1],
                                            tf.float32),
            "target": x["target"],
            "target_mask": tf.sequence_mask(x["target_length"],
                                            tf.shape(x["target"])[1],
                                            tf.float32)
        }, y),
        num_parallel_calls=tf.data.experimental.AUTOTUNE)

    return dataset
```

具体的过程代码里已经说得很清楚了，只有几点需要注意的：

* 在调用[tf.data.experimental.bucket_by_sequence_length](https://www.tensorflow.org/api_docs/python/tf/data/experimental/bucket_by_sequence_length)函数时，`source`和`target` pad后的长度是相同的
* `target`前面是`<bos>`，label后面是`<eos>`
* `source_mask`和`target_mask`的作用是在attention的时候帮助创建bias，后面会提到

之后，在具体的训练过程中，我们显然还需要把token转换成id，参见[trainer.py](https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/bin/trainer.py)中的

```python
features = data.lookup(features, "train", params)
```

这一行，调用了[vocab.py](https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/data/vocab.py)中的`lookup`函数：

```python
def lookup(inputs, mode, params):
    if mode != "infer":
        features, labels = inputs
        source, target = features["source"], features["target"]
        # 把tf.Tensor转换成np.ndarray
        source = source.numpy()
        target = target.numpy()
        labels = labels.numpy()
        src_mask = torch.FloatTensor(features["source_mask"].numpy()).cuda()
        tgt_mask = torch.FloatTensor(features["target_mask"].numpy()).cuda()

        # 把token转换成id，再把id转换成torch.LongTensor
        source = _lookup(source, params.lookup["source"])
        target = _lookup(target, params.lookup["target"])
        labels = _lookup(labels, params.lookup["target"])

        features = {
            "source": source,
            "source_mask": src_mask,
            "target": target,
            "target_mask": tgt_mask
        }

        return features, labels
    # 在训练阶段后面都不用看了
    else:
        source = inputs["source"].numpy()
        source = _lookup(source, params.lookup["source"])
        src_mask = torch.FloatTensor(inputs["source_mask"].numpy()).cuda()

        features = {
            "source": source,
            "source_mask": src_mask
        }

        return features
```

`lookup`函数之前的数据处理是用TensorFlow做的，大概是因为PyTorch版是用TensorFlow改过来的，数据处理就沿用了tf版本的做法。

最后features和label就可以输到模型里了：

```python
loss = train_fn(features)
```

```python
def train_fn(inputs):
    features, labels = inputs
    loss = model(features, labels)
    return loss
```

## 验证集数据输入

在[trainer.py](https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/bin/trainer.py)里，如果验证集存在，那么模型会加载一堆东西：

```python
if params.validation:
    sorted_key, eval_dataset = data.get_dataset(
        params.validation, "infer", params)
    references = load_references(params.references)
```

下面我们来逐个看一下。

首先是`data.get_dataset`。注意到这里调用`data.get_dataset`时使用的参数是`"infer"`，也就是说会使用[dataset.py](https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/data/dataset.py)里的那个`infer_input_fn()`。

```python
def infer_input_fn():
    sorted_key, sorted_data = sort_input_file(filenames)
```

`infer_input_fn`首先调用了`sort_input_file`函数：

```python
def sort_input_file(filename, reverse=True):
    # 实际上只读入了source端
    with open(filename, "rb") as fd:
        inputs = [line.strip() for line in fd]

    input_lens = [
        (i, len(line.split())) for i, line in enumerate(inputs)]

    # 对source端的句子按长度从大到小进行排序
    sorted_input_lens = sorted(input_lens, key=lambda x: x[1],
                               reverse=reverse)
    sorted_keys = {}
    sorted_inputs = []

    for i, (idx, _) in enumerate(sorted_input_lens):
        sorted_inputs.append(inputs[idx])
        sorted_keys[idx] = i

    return sorted_keys, sorted_inputs
```

然后对数据继续进行处理：

```python
def infer_input_fn():
    ......
    dataset = tf.data.Dataset.from_tensor_slices(
        tf.constant(sorted_data))
    dataset = dataset.shard(torch.distributed.get_world_size(),
                            torch.distributed.get_rank())

    # 将source端句子进行分词，在后面加上<eos>
    dataset = dataset.map(
        lambda x: tf.strings.split([x]).values,
        num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.map(
        lambda x: tf.concat([x, [tf.constant(params.eos)]], axis=0),
        num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.map(
        lambda x: {
            "source": x,
            "source_length": tf.shape(x)[0]
        },
        num_parallel_calls=tf.data.experimental.AUTOTUNE)

    # 进行padding
    dataset = dataset.padded_batch(
        params.decode_batch_size,
        padded_shapes={
            "source": tf.TensorShape([None]),
            "source_length": tf.TensorShape([])
        },
        padding_values={
            "source": tf.constant(params.pad),
            "source_length": 0
        })

    # 添加source_mask
    dataset = dataset.map(
        lambda x: {
            "source": x["source"],
            "source_mask": tf.sequence_mask(x["source_length"],
                                            tf.shape(x["source"])[1],
                                            tf.float32),
        },
        num_parallel_calls=tf.data.experimental.AUTOTUNE)

    return sorted_key, dataset
```

其中`sorted_key`的主要作用是在翻译完之后把译文按照原来的次序重新排列。那么为啥要把source端排序呢，这主要是为了分batch的方便，在evaluation阶段每个batch包含的句子数量是相同的，和train阶段不同。（不过为什么要这样设计呢……？）很明显的是，此处把target端相关的东西全都拿走了，因为实际上执行的是一个翻译的过程，没有参考的target。

接下来就是[trainer.py](https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/bin/trainer.py)中的`load_reference`函数：

```py
def load_references(pattern):
    if not pattern:
        return None

    files = glob.glob(pattern)
    references = []

    for name in files:
        ref = []
        with open(name, "rb") as fd:
            for line in fd:
                items = line.strip().split()
                ref.append(items)
        references.append(ref)

    return list(zip(*references))
```

可以看出这个函数的主要功能就是把所有的reference都加载进来，然后分词，准备在翻译完后与译文进行比较。

## 测试集数据输入

测试集数据输入的入口在[translator.py](https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/bin/translator.py)，和验证集一样，也调用了`get_dataset`，只不过这次没有reference：

```py
if len(args.input) == 1:
    mode = "infer"
    sorted_key, dataset = data.get_dataset(
        args.input[0], mode, params)
else:
    # Teacher-forcing
    mode = "eval"
    dataset = data.get_dataset(args.input, mode, params)
    sorted_key = None
```

接下来的部分和验证集用的是同一个函数，就不细讲了：

```py
def infer_input_fn():
    sorted_key, sorted_data = sort_input_file(filenames)
    dataset = tf.data.Dataset.from_tensor_slices(
        tf.constant(sorted_data))
    dataset = dataset.shard(torch.distributed.get_world_size(),
                            torch.distributed.get_rank())

    dataset = dataset.map(
        lambda x: tf.strings.split([x]).values,
        num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.map(
        lambda x: tf.concat([x, [tf.constant(params.eos)]], axis=0),
        num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.map(
        lambda x: {
            "source": x,
            "source_length": tf.shape(x)[0]
        },
        num_parallel_calls=tf.data.experimental.AUTOTUNE)

    dataset = dataset.padded_batch(
        params.decode_batch_size,
        padded_shapes={
            "source": tf.TensorShape([None]),
            "source_length": tf.TensorShape([])
        },
        padding_values={
            "source": tf.constant(params.pad),
            "source_length": 0
        })

    dataset = dataset.map(
        lambda x: {
            "source": x["source"],
            "source_mask": tf.sequence_mask(x["source_length"],
                                            tf.shape(x["source"])[1],
                                            tf.float32),
        },
        num_parallel_calls=tf.data.experimental.AUTOTUNE)

    return sorted_key, dataset
```
