---
title: THUMT代码详解（3）：训练阶段模型和数据流
urlname: thumt-code-summary-3
toc: true
date: 2020-08-31 12:04:23
updated: 2020-08-31 12:04:23
tags: THUMT
categories: NLP
---

[简介篇地址](/post/thumt-code-summary-1)

<!--more-->

接下来是训练阶段的模型和数据流。入口在[trainer.py](https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/bin/trainer.py)：

```py
def main(args):
    model_cls = models.get_model(args.model)
    ......
    model = model_cls(params).cuda()
    ......
    def train_fn(inputs):
        features, labels = inputs
        loss = model(features, labels)
        return loss
    ......
    loss = train_fn(features)
    ......
```

THUMT-PyTorch版本中只实现了Transformer这个模型，因此`models.get_model`必然会返回它。

之后就是具体的模型和数据流了，分成模型入口、encoder和decoder来讲。

下文中会用以下符号标出变量的维度：

* `batch`：batch size
* `length_s`：batch中源端句子的长度
* `length_t`：batch中目标端句子的长度
* `hidden`：表示的长度，默认为512
* `h2`: `= hidden // heads`

## 模型入口

模型初始化时首先执行`Transformer.__init__`函数：

```py
def __init__(self, params, name="transformer"):
    super(Transformer, self).__init__(name=name)
    self.params = params

    with utils.scope(name):
        # 创建embedding，默认不共享
        self.build_embedding(params)
        # 创建positional embedding，详见Transformer论文
        self.encoding = modules.PositionalEmbedding()
        # 创建encoder及其参数，默认为6层
        self.encoder = TransformerEncoder(params)
        # 创建decoder及其参数，默认为6层
        self.decoder = TransformerDecoder(params)

    # 计算翻译结果和标签之间的交叉熵，label_smooting默认0.1
    self.criterion = modules.SmoothedCrossEntropyLoss(
        params.label_smoothing)
    # dropout，默认0.1
    self.dropout = params.residual_dropout
    # hidden_size，默认512
    self.hidden_size = params.hidden_size
    # encoder层数，默认6层
    self.num_encoder_layers = params.num_encoder_layers
    # decoder层数，默认6层
    self.num_decoder_layers = params.num_decoder_layers
    # 初始化embedding
    self.reset_parameters()
```

其中调用的`Transformer.build_embedding`函数是这样的：

```py
def build_embedding(self, params):
    svoc_size = len(params.vocabulary["source"])
    tvoc_size = len(params.vocabulary["target"])

    if params.shared_source_target_embedding and svoc_size != tvoc_size:
        raise ValueError("Cannot share source and target embedding.")

    if not params.shared_embedding_and_softmax_weights:
        self.softmax_weights = torch.nn.Parameter(
            torch.empty([tvoc_size, params.hidden_size]))
        self.add_name(self.softmax_weights, "softmax_weights")

    if not params.shared_source_target_embedding:
        self.source_embedding = torch.nn.Parameter(
            torch.empty([svoc_size, params.hidden_size]))
        self.target_embedding = torch.nn.Parameter(
            torch.empty([tvoc_size, params.hidden_size]))
        self.add_name(self.source_embedding, "source_embedding")
        self.add_name(self.target_embedding, "target_embedding")
    else:
        self.weights = torch.nn.Parameter(
            torch.empty([svoc_size, params.hidden_size]))
        self.add_name(self.weights, "weights")

    self.bias = torch.nn.Parameter(torch.zeros([params.hidden_size]))
    self.add_name(self.bias, "bias")
```

还是很简单易懂的。

`Transformer.__init__`函数中分别创建了一个`TransformerEncoder`和一个`TransformerDecoder`：

```py
def __init__(self, params, name="encoder"):
    super(TransformerEncoder, self).__init__(name=name)

    self.normalization = params.normalization

    with utils.scope(name):
        self.layers = nn.ModuleList([
            TransformerEncoderLayer(params, name="layer_%d" % i)
            for i in range(params.num_encoder_layers)])
        if self.normalization == "before":
            self.layer_norm = modules.LayerNorm(params.hidden_size)
        else:
            self.layer_norm = None
```

```py
def __init__(self, params, name="decoder"):
    super(TransformerDecoder, self).__init__(name=name)

    self.normalization = params.normalization

    with utils.scope(name):
        self.layers = nn.ModuleList([
            TransformerDecoderLayer(params, name="layer_%d" % i)
            for i in range(params.num_decoder_layers)])

        if self.normalization == "before":
            self.layer_norm = modules.LayerNorm(params.hidden_size)
        else:
            self.layer_norm = None
```

具体内容放到encoder和decoder部分再去说。

`Transformer.__init__`函数最后调用的`Transformer.reset_parameters`函数也很简单：

```py
def reset_parameters(self):
    nn.init.normal_(self.src_embedding, mean=0.0,
                    std=self.params.hidden_size ** -0.5)
    nn.init.normal_(self.tgt_embedding, mean=0.0,
                    std=self.params.hidden_size ** -0.5)

    if not self.params.shared_embedding_and_softmax_weights:
        nn.init.normal_(self.softmax_weights, mean=0.0,
                        std=self.params.hidden_size ** -0.5)
```

值得注意的是这里的`src_embedding`、`tgt_embedding`和`softmax_weights`都是使用`@property`装饰器的属性，因为它们是可以共享权重的。

然后把features和labels输入到模型中，调用的是`Transformer.forward`函数：

```py
def forward(self, features, labels, mode="train", level="sentence"):
    # mask: [batch, length_s]
    mask = features["target_mask"]

    state = self.empty_state(features["target"].shape[0],
                                labels.device)
    state = self.encode(features, state)
    ......
```

其中调用的`Transformer.empty_state`函数是这样的：

```py
def empty_state(self, batch_size, device):
    state = {
        "decoder": {
            "layer_%d" % i: {
                "k": torch.zeros([batch_size, 0, self.hidden_size],
                                    device=device),
                "v": torch.zeros([batch_size, 0, self.hidden_size],
                                    device=device)
            } for i in range(self.num_decoder_layers)
        }
    }

    return state
```

大概就是创建了decoder每层的key和value的一个位置。

之后就是encoder部分了。

## encoder

首先是`encode`函数：

```py
def encode(self, features, state):
    # src_seq: [batch, length_s]
    src_seq = features["source"]
    # src_mask: [batch, length_s]
    src_mask = features["source_mask"]
    enc_attn_bias = self.masking_bias(src_mask)
    ......
```

`encode`函数调用了`masking_bias`，这个函数的主要功能是把`src_mask`中为0的部分变成无穷大，在attention的时候用来去掉padding部分的attention：

```py
# mask: [batch, length_s]
def masking_bias(mask, inf=-1e9):
    # 把mask中非0部分变成无穷小
    ret = (1.0 - mask) * inf
    # unsqueeze后的形状：[batch, 1, 1, length]
    return torch.unsqueeze(torch.unsqueeze(ret, 1), 1)
```

然后回到`Transformer.encode`函数：

```py
def encode(self, features, state):
    ......
    # 实际进行embedding
    # input: [batch, length_s, hidden]
    inputs = torch.nn.functional.embedding(src_seq, self.src_embedding)
    # 保证inputs的方差和均值都是1
    inputs = inputs * (self.hidden_size ** 0.5)
    inputs = inputs + self.bias
    # 在dropout之前先把positional embedding加上
    inputs = nn.functional.dropout(self.encoding(inputs), self.dropout,
                                    self.training)

    # 将enc_attn_bias的dtype的device修改为与inputs匹配
    enc_attn_bias = enc_attn_bias.to(inputs)
    # 调用TransformerEncoder.forward
    encoder_output = self.encoder(inputs, enc_attn_bias)
    ......
```

前面我们已经看到了，`Transformer`是先把对应的Module和权重都创建出来，再在运行过程中进行计算的。因此我们首先来看一下创建的过程，首先是`TransformerEncoder.__init__`：

```py
def __init__(self, params, name="encoder"):
    super(TransformerEncoder, self).__init__(name=name)

    self.normalization = params.normalization

    with utils.scope(name):
        # 创建了若干个TransformerEncoderLayer，默认数量为6
        self.layers = nn.ModuleList([
            TransformerEncoderLayer(params, name="layer_%d" % i)
            for i in range(params.num_encoder_layers)])
        if self.normalization == "before":
            self.layer_norm = modules.LayerNorm(params.hidden_size)
        else:
            self.layer_norm = None
```

然后是`TransformerEncoderLayer.__init__`，每层包括两个sub layer：

```py
def __init__(self, params, name="layer"):
    super(TransformerEncoderLayer, self).__init__(name=name)

    with utils.scope(name):
        self.self_attention = AttentionSubLayer(params)
        self.feed_forward = FFNSubLayer(params)
```

然后是`AttentionSubLayer.__init__`和`FFNSubLayer.__init__`：

```py
def __init__(self, params, name="attention"):
    super(AttentionSubLayer, self).__init__(name=name)

    self.dropout = params.residual_dropout
    self.normalization = params.normalization

    with utils.scope(name):
        self.attention = modules.MultiHeadAttention(
            params.hidden_size, params.num_heads, params.attention_dropout)
        self.layer_norm = modules.LayerNorm(params.hidden_size)
```

```py
def __init__(self, params, dtype=None, name="ffn_layer"):
    super(FFNSubLayer, self).__init__(name=name)

    self.dropout = params.residual_dropout
    self.normalization = params.normalization

    with utils.scope(name):
        self.ffn_layer = modules.FeedForward(params.hidden_size,
                                                params.filter_size,
                                                dropout=params.relu_dropout)
        self.layer_norm = modules.LayerNorm(params.hidden_size)
```

`AttentionSubLayer.__init__`还调用了`MultiHeadAttention.__init__`（`FFNSubLayer.__init__`也调用了`FeedForward.__init__`，但很简单，就不说了）：

```py
def __init__(self, hidden_size, num_heads, dropout=0.0,
                name="multihead_attention"):
    super(MultiHeadAttention, self).__init__(name=name)

    self.num_heads = num_heads
    self.hidden_size = hidden_size
    self.dropout = dropout

    with utils.scope(name):
        # attention中的q、k、v
        self.q_transform = Affine(hidden_size, hidden_size,
                                    name="q_transform")
        self.k_transform = Affine(hidden_size, hidden_size,
                                    name="k_transform")
        self.v_transform = Affine(hidden_size, hidden_size,
                                    name="v_transform")
        # combine heads之后最后再做一个变换
        self.o_transform = Affine(hidden_size, hidden_size,
                                    name="o_transform")

    self.reset_parameters()
```

然后再回到`Transformer.encode`函数：

```py
def encode(self, features, state):
    encoder_output = self.encoder(inputs, enc_attn_bias)
    ......
```

首先调用`TransformerEncoder.forward`函数（因为这些类都是继承`nn.Module`的）：

```py
def forward(self, x, bias):
    # x: [batch, length_s]
    # bias: [batch, 1, 1, length_s]
    for layer in self.layers:
        x = layer(x, bias)

    if self.normalization == "before":
        x = self.layer_norm(x)

    return x
```

直接调用每一层的`TransformerEncoderLayer.forward`，把`x`和`bias`传过去：

```py
def forward(self, x, bias):
    x = self.self_attention(x, bias)
    x = self.feed_forward(x)
    return x
```

只是简单地把`x`和`bias`传给了`MultiHeadAttention.forward`函数，接下来的部分比较重要：

```py
# 在encoder self-attention的情况下，memory和kv都是None
def forward(self, query, bias, memory=None, kv=None):
    # q: [batch, length_s, hidden]
    # bias: [batch, 1, 1, length_s]
    q = self.q_transform(query)

    # 因为是self-attention，所以if部分的内容没用
    if memory is not None:
        if kv is not None:
            k, v = kv
        else:
            k, v = None, None

        # encoder-decoder attention
        k = k or self.k_transform(memory)
        v = v or self.v_transform(memory)
    else:
        # self-attention
        # k: [batch, length_s, hidden]
        k = self.k_transform(query)
        # v: [batch, length_s, hidden]
        v = self.v_transform(query)

        # kv is None，不用管
        if kv is not None:
            k = torch.cat([kv[0], k], dim=1)
            v = torch.cat([kv[1], v], dim=1)

    # split heads
    # 主要功能是把维度为[batch, length_s, hidden]的矩阵变成
    # [batch, heads, length_s, h2]
    # 其中h2 = hidden // heads
    qh = self.split_heads(q, self.num_heads)
    kh = self.split_heads(k, self.num_heads)
    vh = self.split_heads(v, self.num_heads)

    # scale query
    qh = qh * (self.hidden_size // self.num_heads) ** -0.5

    # dot-product attention
    # kh: [batch, heads, h2, length_s]
    kh = torch.transpose(kh, -2, -1)
    # 在matmul时，前两维不变，后两维相乘，得到维度为
    # logits: [batch, heads, length_s, length_s]
    # 相当于每一个句子有若干个head，每个head都有一个length_s*length_s的矩阵，代表q到k的attention
    logits = torch.matmul(qh, kh)

    # bias: [batch, 1, 1, length_s]
    # 在每一行加上对应的bias，使padding部分的attention被屏蔽
    # logits维度不变: [batch, heads, length_s, length_s]
    if bias is not None:
        logits = logits + bias

    # 对最后一维（每行）做softmax，然后做dropout，得到
    # weight: [batch, heads, length_s, length_s]
    weights = torch.nn.functional.dropout(torch.softmax(logits, dim=-1),
                                            p=self.dropout,
                                            training=self.training)

    # x: [batch, heads, length_s, h2]
    x = torch.matmul(weights, vh)

    # combine heads
    # output: [batch, length_s, hidden]
    output = self.o_transform(self.combine_heads(x))

    if kv is not None:
        return output, k, v

    # 最后直接返回output
    return output
```

说起来，这里面可能有几个不太好理解的地方。第一个是`MultiHeadAttentionBase.split_heads`和`MultiHeadAttentionBase.combine_heads`的具体实现方法：

```py
@staticmethod
def split_heads(x, heads):
    batch = x.shape[0]
    length = x.shape[1]
    channels = x.shape[2]

    y = torch.reshape(x, [batch, length, heads, channels // heads])
    return torch.transpose(y, 2, 1)
```

为什么要先reshape再transpose，而不是直接reshape呢？显然，我们需要修改`head`这一维度的位置。reshape成`[batch, length, heads, h2]`的维度之后，相当于每个token长度为`heads * h2`的表示向量被分成了`heads`个向量，每个向量的长度为`h2`。然而我们希望的是每个句子的表示分成`heads`个矩阵，每个矩阵的维度是`[length, h2]`。为了做到这一点，需要进行transpose，也就相当于把后三维旋转一下：

![transpose的过程](transpose.png)

而如果直接reshape的话，实际效果是把`length`一维分到`heads`一维去了，这显然不太合理。

如果还是觉得不够形象的话，请参见这篇文章：[Numpy reshape and transpose](https://lihan.me/2018/01/numpy-reshape-and-transpose/)

而`combine_heads`正好是把上述过程反过来：

```py
@staticmethod
def combine_heads(x):
    batch = x.shape[0]
    heads = x.shape[1]
    length = x.shape[2]
    channels = x.shape[3]

    y = torch.transpose(x, 2, 1)

    return torch.reshape(y, [batch, length, heads * channels])
```

第二个地方是`torch.matmul`的broadcast。在上述过程中，一个维度为`[batch, heads, length_s, h2]`和一个维度为`[batch, heads, h2, length_s]`的矩阵相乘，实际效果是有`batch * heads`个维度为`[length_s, h2]`的矩阵和`[h2, length_s]`的矩阵相乘。这倒是很好理解，不过还是应该说一下`torch.matmul`的broadcast规则。在多维数据的情况下，`matmul`使用两个矩阵的后两个维度进行相乘，其他的维度都可以认为是batch维度。详情见[TORCH.MATMUL](https://pytorch.org/docs/stable/generated/torch.matmul.html)

第三个地方是`bias`和`logits`加法的broadcast（好吧，怎么又是broadcast）：一个维度为`[batch, 1, 1, length_s]`的矩阵加到维度为`[batch, heads, length_s, length_s]`的矩阵上，如何broadcast呢？这里需要参照[BROADCASTING SEMANTICS](https://pytorch.org/docs/stable/notes/broadcasting.html)：

>Two tensors are “broadcastable” if the following rules hold:
>
>* Each tensor has at least one dimension.
>* When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.

显然，`bias`矩阵的大小是符合这个条件的。最后的效果就是，对于batch里的每一个句子，它的每一个head对应的attention矩阵的每一行都加上了对应于mask的一个偏置，把padding部分的k和v都屏蔽掉了。

（Feed-forward部分忽略）

最后回到`Transformer.encode`函数：

```py
def encode(self, features, state):
    ......
    state["encoder_output"] = encoder_output
    state["enc_attn_bias"] = enc_attn_bias

    return state
```

此时的`state`是这样的（在encode的过程中`"decoder"`的部分根本就没动，只是加了几个属性）：

```py
{
    "encoder_output": [batch, length_s, hidden]
    "enc_attn_bias": [batch, 1, 1, length_s]
    "decoder": ...
}
```

## decoder

和之前一样，我们先来看看创建权重的过程。首先是`TransformerDecoder.__init__`：

```py
def __init__(self, params, name="decoder"):
    super(TransformerDecoder, self).__init__(name=name)

    self.normalization = params.normalization

    with utils.scope(name):
        self.layers = nn.ModuleList([
            TransformerDecoderLayer(params, name="layer_%d" % i)
            for i in range(params.num_decoder_layers)])

        if self.normalization == "before":
            self.layer_norm = modules.LayerNorm(params.hidden_size)
        else:
            self.layer_norm = None
```

很显然，它创建了若干个（默认为6个）`TransformerDecoderLayer`。`TransformerDecoderLayer.__init__`的内容如下所示：

```py
def __init__(self, params, name="layer"):
    super(TransformerDecoderLayer, self).__init__(name=name)

    with utils.scope(name):
        self.self_attention = AttentionSubLayer(params,
                                                name="self_attention")
        self.encdec_attention = AttentionSubLayer(params,
                                                name="encdec_attention")
        self.feed_forward = FFNSubLayer(params)
```

它创建了两个`AttentionSubLayer`（self-attention和enc-dec attention）和一个`FFNSublayer`，这些在encoder的部分都已经说过了，所以不再说了。

之后我们回到`Transformer.forward`函数：

```py
def forward(self, features, labels, mode="train", level="sentence"):
    ......
    logits, _ = self.decode(features, state, mode=mode)
    ......
```

它调用了`Transformer.decode`函数：

```py
def decode(self, features, state, mode="infer"):
    # [batch, length_t]
    tgt_seq = features["target"]

    # [batch, 1, 1, length_s]
    enc_attn_bias = state["enc_attn_bias"]
    # [1, 1, length_t, length_t]
    # 是一个下三角部分为0（包括对角线），上三角部分为无穷小的矩阵
    dec_attn_bias = self.causal_bias(tgt_seq.shape[1])

    # 对目标端进行embedding
    # targets: [batch, length_t, hidden]
    targets = torch.nn.functional.embedding(tgt_seq, self.tgt_embedding)
    targets = targets * (self.hidden_size ** 0.5)

    # 去掉tgt_seq开头的<bos>，把表示全都换成0
    decoder_input = torch.cat(
        [targets.new_zeros([targets.shape[0], 1, targets.shape[-1]]),
            targets[:, 1:, :]], dim=1)
    # 加入positional encoding，进行dropout
    decoder_input = nn.functional.dropout(self.encoding(decoder_input),
                                            self.dropout, self.training)

    # [batch, length_s, hidden]
    encoder_output = state["encoder_output"]
    # 调整dec_attn_bias的dtype和device
    dec_attn_bias = dec_attn_bias.to(targets)

    # train阶段先不用管
    if mode == "infer":
        decoder_input = decoder_input[:, -1:, :]
        dec_attn_bias = dec_attn_bias[:, :, -1:, :]

    decoder_output = self.decoder(decoder_input, dec_attn_bias,
                                    enc_attn_bias, encoder_output, state)
    ......
```

为什么这里要把`decoder_input`每个batch的第一列都换成0呢？这和训练模式下我们到底是如何进行训练的有关。事实上，假设我们有一个句子是

```py
[
    "",
    "Ich",
    "bin",
    "ein",
    "Student",
    "."
]
```

那么我们在完成decoder的解码过程后，在每个位置期望得到的是：

```py
[
    "Ich" (The token after ""),
    "bin" (The token after "Ich"),
    "ein" (The token after "Ich bin"),
    "Student" (The token after "Ich bin ein"),
    "." (The token after "Ich bin ein Student"),
    "<eos>" (The token after "Ich bin ein Student .")
]
```

这种方法叫做Teacher Forcing：在训练阶段，我们不考虑每一步decoder实际输出的token，而是直接把正确的（一部分）句子输入到decoder中。

然后就拿着这些input、bias、output和state去调用`TransformerDecoder.forward`：

```py
# x: [batch, length_t, hidden]
# attn_bias: [1, 1, length_t, length_t]，下三角为0（包括对角线，上三角为无穷小）
# enc_attn_bias: [batch, 1, 1, length_s]，实际句子部分为0，padding部分为无穷小
# memory: [batch, length_s, hidden]，就是encoder_output...
# state: 见encoder一节最后部分
def forward(self, x, attn_bias, encdec_bias, memory, state=None):
    for i, layer in enumerate(self.layers):
        if state is not None:
            # 从encoder节可以看出，这里的state["decoder"]["layer_%d" % i]是两个空的矩阵
            # 传过去只是为了记录
            x = layer(x, attn_bias, encdec_bias, memory,
                        state["decoder"]["layer_%d" % i])
        else:
            x = layer(x, attn_bias, encdec_bias, memory, None)

    if self.normalization == "before":
        x = self.layer_norm(x)

    return x
```

`x`被逐层传给`TransformerDecoderLayer.__call__`函数：

```py
def __call__(self, x, attn_bias, encdec_bias, memory, state=None):
    x = self.self_attention(x, attn_bias, state=state)
    x = self.encdec_attention(x, encdec_bias, memory)
    x = self.feed_forward(x)
    return x
```

这看起来非常简单，只是逐层调用了两个attention和一个ffn而已。

接下来首先看self_attention部分（下面会出现大量的重复attention代码，但是它们的功能和之前提到的会有一些区别）：

```py
# x: [batch, length_t, hidden]，target端embed的结果或上一层的输出
# bias: [1, 1, length_t, length_t]，下三角为0（包括对角线），上三角为无穷小
# memory: None
# state: 见encoder一节最后部分
def forward(self, x, bias, memory=None, state=None):
    if self.normalization == "before":
        y = self.layer_norm(x)
    else:
        y = x

    # 因为目前处于训练阶段，因此调用attention时传过去的state是None
    if self.training or state is None:
        y = self.attention(y, bias, memory, None)
    else:
        kv = [state["k"], state["v"]]
        y, k, v = self.attention(y, bias, memory, kv)
        state["k"], state["v"] = k, v

    y = nn.functional.dropout(y, self.dropout, self.training)

    if self.normalization == "before":
        return x + y
    else:
        return self.layer_norm(x + y)
```

```py
# query: [batch, length_t, hidden]，target端embed的结果或上一层的输出
# bias: [batch, 1, 1, length_s]，下三角为0（包括对角线），上三角为无穷小
# memory: None
# kv: None
def forward(self, query, bias, memory=None, kv=None):
    # 用query计算出q
    # q: [batch, length_t, hidden]
    q = self.q_transform(query)

    if memory is not None:
        if kv is not None:
            k, v = kv
        else:
            k, v = None, None

        # encoder-decoder attention
        k = k or self.k_transform(memory)
        v = v or self.v_transform(memory)
    else:
        # memory=None，因此是self-attention，看这部分
        # self-attention
        # 用query直接计算出k和v
        # 我 query 我 自 己
        # k, v: [batch, length_t, hidden]
        k = self.k_transform(query)
        v = self.v_transform(query)

        # kv=None，因此不用管这段
        if kv is not None:
            k = torch.cat([kv[0], k], dim=1)
            v = torch.cat([kv[1], v], dim=1)

    # split heads
    # qh, kh, vh: [batch, heads, length_t, h2]
    qh = self.split_heads(q, self.num_heads)
    kh = self.split_heads(k, self.num_heads)
    vh = self.split_heads(v, self.num_heads)

    # scale query
    qh = qh * (self.hidden_size // self.num_heads) ** -0.5

    # dot-product attention
    # kh: [batch, heads, h2, length_t]
    kh = torch.transpose(kh, -2, -1)
    # logits: [batch, heads, length_t, length_t]
    logits = torch.matmul(qh, kh)

    # 加入bias
    # bias: [1, 1, length_t, length_t]
    # 这里的bias不是mask bias，而是causal bias，也就是在每个attention矩阵中，
    # token不能attend到未来的token
    # 最后得到的attention是一个下三角矩阵
    if bias is not None:
        logits = logits + bias
    # logits: [batch, heads, length_t, length_t]

    # 对最后一维做softmax
    # weights: [batch, heads, length_t, length_t]
    weights = torch.nn.functional.dropout(torch.softmax(logits, dim=-1),
                                            p=self.dropout,
                                            training=self.training)

    # x: [batch, heads, length_t, h2]
    x = torch.matmul(weights, vh)

    # combine heads
    # output: [batch, length_t, hidden]
    output = self.o_transform(self.combine_heads(x))

    # kv=None，所以不用管
    if kv is not None:
        return output, k, v

    return output
```

那么问题来了。这里到底为什么要做一个带causal bias的attention呢？

这是因为，对每一个位置来说，它实际能够attend到的部分是已经解码出的部分，不能attend到那些对它来说还没出现的部分。

然后是enc-dec attention：

```py
# x: [batch, length_t, hidden]，本层self-attention的输出
# bias: [batch, 1, 1, lenth_s]，source端的masking bias
# memory: [batch, length_s, hidden]，encoder端的输出
# state: None
def forward(self, x, bias, memory=None, state=None):
    if self.normalization == "before":
        y = self.layer_norm(x)
    else:
        y = x

    if self.training or state is None:
        # 因为training=True，因此还是调用这里
        y = self.attention(y, bias, memory, None)
    else:
        kv = [state["k"], state["v"]]
        y, k, v = self.attention(y, bias, memory, kv)
        state["k"], state["v"] = k, v

    y = nn.functional.dropout(y, self.dropout, self.training)

    if self.normalization == "before":
        return x + y
    else:
        return self.layer_norm(x + y)
```

```py
# query: [batch, length_t, hidden]，本层self-attention的输出
# bias: [batch, 1, 1, lenth_s]，source端的masking bias
# memory: [batch, length_s, hidden]，encoder端的输出
# kv: None
def forward(self, query, bias, memory=None, kv=None):
    # q: [batch, length_t, hidden]
    # query来自decoder端
    q = self.q_transform(query)

    if memory is not None:
        if kv is not None:
            k, v = kv
        else:
            k, v = None, None

        # encoder-decoder attention
        # k, v: [batch, length_s, hidden]
        # kv来自encoder端
        k = k or self.k_transform(memory)
        v = v or self.v_transform(memory)
    else:
        # self-attention
        k = self.k_transform(query)
        v = self.v_transform(query)

        if kv is not None:
            k = torch.cat([kv[0], k], dim=1)
            v = torch.cat([kv[1], v], dim=1)

    # split heads
    # qh: [batch, heads, length_t, h2]
    # kh, vh: [batch, heads, length_s, h2]
    qh = self.split_heads(q, self.num_heads)
    kh = self.split_heads(k, self.num_heads)
    vh = self.split_heads(v, self.num_heads)

    # scale query
    qh = qh * (self.hidden_size // self.num_heads) ** -0.5

    # dot-product attention
    # kh: [batch, heads, h2, length_s]
    kh = torch.transpose(kh, -2, -1)
    # logits: [batch, heads, length_t, length_s]
    logits = torch.matmul(qh, kh)

    # 每行加上source端的masking bias
    if bias is not None:
        logits = logits + bias

    # 对最后一维做softmax
    # weights: [batch, heads, length_t, length_s]
    weights = torch.nn.functional.dropout(torch.softmax(logits, dim=-1),
                                            p=self.dropout,
                                            training=self.training)

    # x: [batch, heads, length_t, h2]
    x = torch.matmul(weights, vh)

    # combine heads
    # output: [batch, length_t, hidden]
    output = self.o_transform(self.combine_heads(x))

    if kv is not None:
        return output, k, v

    return output
```

回到`Transformer.decode`的最后一部分：

```py
def decode(self, features, state, mode="infer"):
    ......
    # decoder_output: [batch * length_t, hidden]
    decoder_output = torch.reshape(decoder_output, [-1, self.hidden_size])
    # decoder_output: [hidden, batch * length_t]
    decoder_output = torch.transpose(decoder_output, -1, -2)
    # 把每个词的softmax embedding和每个表示做点积，得到每个位置词的概率分布
    # （虽然logits还没有做过处理，不是概率分布）
    # [tvoc_size, hidden] * [hidden, batch * length_t] = [tvoc_size, batch * length_t]
    logits = torch.matmul(self.softmax_embedding, decoder_output)
    # logits: [batch * length_t, tvoc_size]
    logits = torch.transpose(logits, 0, 1)

    return logits, state
```

再回到`Transformer.forward`：

```py
def forward(self, features, labels, mode="train", level="sentence"):
    ......
    # 此处state被丢掉了，反正也没用
    logits, _ = self.decode(features, state, mode=mode)
    # labels就是数据处理中target端句子的id，后面加了<eos>
    # logits: [batch * length_t, tvoc_size]
    # labels: [batch, length_t]
    # loss: [batch, length_t]
    loss = self.criterion(logits, labels)
    # mask: [batch, length_t]
    mask = mask.to(logits)

    # 不是eval，就先不管了
    if mode == "eval":
        if level == "sentence":
            return -torch.sum(loss * mask, 1)
        else:
            return  torch.exp(-loss) * mask - (1 - mask)

    # 把loss和mask逐元素相乘，得到mask后的结果，取平均值为loss
    return torch.sum(loss * mask) / torch.sum(mask)
```

这里的`self.criterion`是[losses.py](https://github.com/THUNLP-MT/THUMT/blob/pytorch/thumt/modules/losses.py)中实现的`SmoothedCrossEntropyLoss`，在实现的时候会把`labels`拉平，具体在这里就不展开了。

这之后的处理就先不说了，总之，训练阶段的模型和数据流讲完了。
